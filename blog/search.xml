<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>windows和mac上安装Tower破解版</title>
      <link href="/blog/b5410c9.html"/>
      <url>/blog/b5410c9.html</url>
      
        <content type="html"><![CDATA[<p>Tower确实蛮好用的，才试用了几天就爱上了它。在<a href="https://www.git-tower.com" target="_blank" rel="noopener">Tower官网</a>这里有关它的介绍，以及有专门的<a href="https://www.git-tower.com/learn/" target="_blank" rel="noopener">电子书和视频</a>来让你快速上手Tower和git。</p><p><img src="https://ws1.sinaimg.cn/large/005EsThygy1fzs24oey0aj31z413ztdg.jpg" alt=""></p><a id="more"></a><p>#<kbd>Tower for windows</kbd><br>版本号截图<br><img src="https://longshilin.com/images/20190507100004.jpg" alt=""></p><p>Tower内部界面<br><img src="https://longshilin.com/images/20190507100014.jpg" alt=""></p><blockquote><p>下载地址：<a href="https://pan.baidu.com/s/1uFCJ3m18RUA1vhPfs27Zvw" target="_blank" rel="noopener">https://pan.baidu.com/s/1uFCJ3m18RUA1vhPfs27Zvw</a> 密码：r1ys</p></blockquote><p>#<kbd>Tower for Mac</kbd><br>版本号截图<br><img src="https://longshilin.com/images/20190507100042.jpg" alt=""></p><p>Tower内部界面<br><img src="https://longshilin.com/images/20190507100050.jpg" alt=""></p><blockquote><p>下载链接：<a href="https://pan.baidu.com/s/1mIQQXm-bEZcbnz06G-FJ2w" target="_blank" rel="noopener">https://pan.baidu.com/s/1mIQQXm-bEZcbnz06G-FJ2w</a> 密码：6fbn</p></blockquote><blockquote><p>有任何问题，请在留言区留言给我~</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tower </tag>
            
            <tag> git </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>更新你的highlightjs版本和支持语言</title>
      <link href="/blog/61ee7b96.html"/>
      <url>/blog/61ee7b96.html</url>
      
        <content type="html"><![CDATA[<p>HighlightJS<br>发布状态：稳定<br>履行解析器功能<br>描述“highlight.js”客户端语法高亮显示的包装器<br>作者（S）Aran Dunkley（Nad）</p><a id="more"></a><p><kbd>Getting highlight.js</kbd><br>在<a href="https://highlightjs.org/download/" target="_blank" rel="noopener">https://highlightjs.org/download/</a>下载自定义的语法js文件，并替换hexo博客中原始的版本。注意博客中要用自己的js，而不是cdn中的。<br><img src="https://ws1.sinaimg.cn/large/005EsThygy1fzrwcicv8ej30mf1280vk.jpg" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> highlightjs </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>常用写作格式</title>
      <link href="/blog/7eae6d59.html"/>
      <url>/blog/7eae6d59.html</url>
      
        <content type="html"><![CDATA[<div class="bs-callout bs-callout-primary"><p>c如何写的漂亮… 这是一个好问题</p></div><p>下面会以这样的格式展示每一个模块<br>Markdown:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ABC</span><br></pre></td></tr></table></figure><p>…outputs:<br>ABC</p><a id="more"></a><h3 id="表格">表格</h3><p>Markdown:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">| Table Header 1 | Table Header 2 | Table Header 3 |</span><br><span class="line">| - | - | - |</span><br><span class="line">| Division 1 | Division 2 | Division 3 |</span><br><span class="line">| Division 1 | Division 2 | Division 3 |</span><br><span class="line">| Division 1 | Division 2 | Division 3 |</span><br></pre></td></tr></table></figure><p>…outputs:</p><table><thead><tr><th>Table Header 1</th><th>Table Header 2</th><th>Table Header 3</th></tr></thead><tbody><tr><td>Division 1</td><td>Division 2</td><td>Division 3</td></tr><tr><td>Division 1</td><td>Division 2</td><td>Division 3</td></tr><tr><td>Division 1</td><td>Division 2</td><td>Division 3</td></tr></tbody></table><h3 id="bootstrap重点标注">Bootstrap重点标注</h3><p>这个Bootstrap标注可以给你的文章增添更多的醒目标记信息。可选项：default | primary | success | info | warning | danger<br>Markdown:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% callout info %&#125;</span><br><span class="line">#### &#123;% fa info-circle %&#125; Info</span><br><span class="line">This is some info content</span><br><span class="line">&#123;% endcallout %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% callout warning %&#125;</span><br><span class="line">#### &#123;% fa exclamation-triangle %&#125; Warning</span><br><span class="line">This is some warning content</span><br><span class="line">&#123;% endcallout %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% callout danger %&#125;</span><br><span class="line">#### &#123;% fa exclamation-triangle %&#125; Danger</span><br><span class="line">This is some danger content</span><br><span class="line">&#123;% endcallout %&#125;</span><br></pre></td></tr></table></figure><p>…outputs:</p><div class="bs-callout bs-callout-info"><h4 id="i-class-fa-fa-info-circle-i-info"><i class="fa fa-info-circle"></i> Info</h4><p>This is some info content</p></div><div class="bs-callout bs-callout-warning"><h4 id="i-class-fa-fa-exclamation-triangle-i-warning"><i class="fa fa-exclamation-triangle"></i> Warning</h4><p>This is some warning content</p></div><div class="bs-callout bs-callout-danger"><h4 id="i-class-fa-fa-exclamation-triangle-i-danger"><i class="fa fa-exclamation-triangle"></i> Danger</h4><p>This is some danger content</p></div><h3 id="fa图标">fa图标</h3><p>在 <a href="https://fontawesome.com/icons" target="_blank" rel="noopener">Font Awesome 资源</a>  可以选择你需要的任意图标。<br>Markdown：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% fa github %&#125;</span><br></pre></td></tr></table></figure><p>…outputs:</p><i class="fa fa-github"></i><h3 id="hexo-github-card">hexo-github-card</h3><p>在文章中引入某个github项目卡片，<a href="https://github.com/Gisonrg/hexo-github-card" target="_blank" rel="noopener">参考地址</a><br>Markdown:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% githubCard user:your_user [repo:your_repo] [width:400] [theme:Default] [client_id:your_client_id] [client_secret:your_client_secret] [align:text-align_position] %&#125;</span><br><span class="line"></span><br><span class="line">如：</span><br></pre></td></tr></table></figure><p>…outputs:</p><div style="text-align:center">  <div class="github-card" data-user="longshilin" data-repo="" data-width="400" data-theme="default" data-target="" data-client-id="" data-client-secret=""></div></div><script src="/github-card-lib/githubcard.js"></script><div style="text-align:center">  <div class="github-card" data-user="longshilin" data-repo="" data-width="400" data-theme="default" data-target="" data-client-id="" data-client-secret=""></div></div><script src="/github-card-lib/githubcard.js"></script>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>写博客必备的工具合集</title>
      <link href="/blog/4b315c79.html"/>
      <url>/blog/4b315c79.html</url>
      
        <content type="html"><![CDATA[<p>这里的工具合集主要是我写博客时需要涉及到的工具，这些工具的使用能够提高我的创作效率，并且给我带来实质上的便利。下面基本上是一句话概括每一个工具，并且配上官方使用的网页介绍，以及对于功能介绍的<i class="fa fa-file-pdf"></i>离线PDF版本以及<i class="fa fa-box"></i>安装包等资源的<i class="fa fa-archive"></i>归档文件目录分享。</p><a id="more"></a><h2 id="电脑优化性能集合">电脑优化性能集合</h2><h3 id="1-与多个平台进行整合的图床工具">[1] 与多个平台进行整合的图床工具</h3><ul><li>名称：PicGo</li><li>官方地址：<a href="https://molunerfinn.com/PicGo/" target="_blank" rel="noopener">https://molunerfinn.com/PicGo/</a></li><li>介绍：很不错的图床工具，适配多个平台，如：GitHub、微博、七牛、腾讯云等等。</li><li>推荐指数：⭐️⭐️⭐️⭐️⭐️</li></ul><h3 id="2-资源归档工具">[2] 资源归档工具</h3><ul><li>介绍：百度云网盘</li><li>官方地址：<a href="https://pan.baidu.com" target="_blank" rel="noopener">https://pan.baidu.com</a></li><li>介绍：网盘资源存放</li><li>推荐指数：⭐️⭐️⭐️⭐️⭐️</li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Double365</title>
      <link href="/blog/3e68368d.html"/>
      <url>/blog/3e68368d.html</url>
      
        <content type="html"><![CDATA[<p>这一次是我们第一次一起跨年，下面是我宝精心制作的视频剪辑哦，看的出来是花费了很多的精力的啦，嘻嘻 💘 希望宝贝以后的Vlog越来越多我也会更加配合的，哈哈哈😈 最后祝我宝猪年大吉呀🐷</p><a id="more"></a><p><video src="http://repo.longshilin.com/video/double365.mov" poster="https://ws1.sinaimg.cn/large/005EsThygy1fyx9bjpedvj30940fymx6.jpg" preload="" controls style="max-width: 50%; display: block; margin-left: auto; margin-right: auto;"><br>your browser does not support the video tag<br></video></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>提升使用体验的工具合集</title>
      <link href="/blog/725d13af.html"/>
      <url>/blog/725d13af.html</url>
      
        <content type="html"><![CDATA[<p>这里的工具合集，基本上是一句话概括每一个工具，并且配上官方使用的网页介绍，以及对于功能介绍的<i class="fas fa-file-pdf"></i>离线PDF版本以及安装包等资源 <i class="fas fa-archive"></i>。</p><a id="more"></a><h2 id="电脑优化性能集合">电脑优化性能集合</h2><h3 id="1-轻量化去广告的内存清理工具">[1] 轻量化去广告的内存清理工具</h3><ul><li>名称：Mem Reduct</li><li>官方地址：<a href="https://www.henrypp.org/product/memreduct" target="_blank" rel="noopener">https://www.henrypp.org/product/memreduct</a></li><li>介绍：轻量化清理机器内存的利器，具体的看我的另外一篇文章介绍 <a href="https://longshilin.com/blog/70c155f1.html">轻量化去广告的内存清理工具</a> <i class="fas fa-archive"></i> <a href="https://pan.baidu.com/s/1FyGqexfCUFrEp-ZKxqcPuw" target="_blank" rel="noopener">详细地址</a></li><li>推荐指数：⭐️⭐️⭐️⭐️</li></ul>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>在mybatis中构建带有sql in查询的mapper语法</title>
      <link href="/blog/9880d52a.html"/>
      <url>/blog/9880d52a.html</url>
      
        <content type="html"><![CDATA[<p>在mapper中编写带有sql in语法的sql时，需要注意不能直接以字符串&quot;in(…)&quot;的形式将语句贴在sql中，需要以list集合的形式遍历出来，具体的表达方式如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;select id=&quot;selectAllServiceByServiceType&quot; resultMap=&quot;BaseResultMap2&quot; parameterType=&quot;java.util.List&quot;&gt;</span><br><span class="line"> SELECT DISTINCT SERVICE_TYPE, IMAGE_TAG</span><br><span class="line"> FROM `SERVICE_CONFIG`</span><br><span class="line"> WHERE STATUS = 1</span><br><span class="line"> AND SERVICE_TYPE IN</span><br><span class="line"> &lt;foreach collection=&quot;list&quot; item=&quot;item&quot; index=&quot;index&quot; open=&quot;(&quot; separator=&quot;,&quot; close=&quot;)&quot;&gt;</span><br><span class="line">  #&#123;item&#125;</span><br><span class="line"> &lt;/foreach&gt;</span><br><span class="line"> ORDER BY SERVICE_TYPE;</span><br><span class="line">&lt;/select&gt;</span><br></pre></td></tr></table></figure><a id="more"></a><p>上面得到的sql也就是：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SELECT DISTINCT SERVICE_TYPE, IMAGE_TAG</span><br><span class="line">FROM `SERVICE_CONFIG`</span><br><span class="line">WHERE STATUS = 1</span><br><span class="line">AND SERVICE_TYPE IN (1, 2, 3, 4, 5);</span><br></pre></td></tr></table></figure><p>这里需要注意的是，foreach中的collection字段的list值是固定表达方式，在对应的mapper方法中，传入参数名也应为list，具体表示如下：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">List&lt;ServiceConfig&gt; selectAllServiceByServiceType(List&lt;Integer&gt; list);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> mybatis </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mybatis </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Keep Running</title>
      <link href="/blog/f1cb4bde.html"/>
      <url>/blog/f1cb4bde.html</url>
      
        <content type="html"><![CDATA[<p>今晚的线上Keep跑~ 慢慢的跑而又满满的幸福</p><a id="more"></a><p><img src="https://ws1.sinaimg.cn/large/005EsThygy1fx8h12ek7fj30yi1pctpb.jpg" alt=""></p><p><img src="https://ws1.sinaimg.cn/large/005EsThygy1fx8h1a6fdmj30u01hcq5e.jpg" alt=""></p>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>AWS Certified SysOps Administrator – Associate 考试的建议准备路径</title>
      <link href="/blog/2e3c6403.html"/>
      <url>/blog/2e3c6403.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.imgur.com/8xNyw7N.png" alt=""> 本篇博文主要介绍AWS Certified SysOps Administrator – Associate 考试的建议准备路径的详细执行计划清单。参考<a href="https://amazonaws-china.com/cn/certification/certification-prep/" target="_blank" rel="noopener">准备认证</a></p><p><img src="https://i.imgur.com/Ria4nMk.png" alt="AWS学习路径"></p><a id="more"></a><div class="bs-callout bs-callout-primary"><p>下面介绍的每个环节都是需要重视的~</p></div><h2 id="第-1-步：参加-aws-培训课程">第 1 步：参加 AWS 培训课程</h2><p>培训可以帮助您提高技术技能，并了解 AWS 最佳使用实践。</p><p><a href="https://amazonaws-china.com/training/course-descriptions/sysops" target="_blank" rel="noopener">Systems Operations on AWS</a><br>课堂 | 3 天</p><blockquote><p>这里没有参加培训课程，改为自己自学了解，下面是在这个阶段会接触到的一些内容。</p></blockquote><h3 id="awsome-day-官方视频-英文版">AWSome Day 官方视频（英文版）</h3><p>下面推荐YouTube上AWS官方发布的三个视频，是 <kbd>AWSome Day 2018</kbd> 的官方视频。</p><div class="video-container"><iframe src="//www.youtube.com/embed/K3qbY8j_Qfs" frameborder="0" allowfullscreen></iframe></div><br><div class="video-container"><iframe src="//www.youtube.com/embed/xToY64aiCgc" frameborder="0" allowfullscreen></iframe></div><br><div class="video-container"><iframe src="//www.youtube.com/embed/2-F4bariCcs" frameborder="0" allowfullscreen></iframe></div><h3 id="awsome-day-2018-全国大学生巡回视频-中文版">AWSome Day 2018 全国大学生巡回视频（中文版）</h3><p><a href="http://www.xuetangx.com/courses/course-v1:AWS+awsomeday+2017_t1/courseware" target="_blank" rel="noopener">AWSome Day 2018 全国大学生巡回视频</a></p><p>接下来的进阶课程，可以在学堂在线的<a href="http://www.xuetangx.com/livecast/microdegree/introduce/3/" target="_blank" rel="noopener">云计算解决方案架构师</a>课程中获得。</p><h3 id="附加">附加</h3><p>接下来你可以点击<a href="https://www.aws.training/" target="_blank" rel="noopener">这个链接</a>了解AWS教学视频和自主进度实验室。</p><h2 id="第-2-步：查看考试指南和样题">第 2 步：查看考试指南和样题</h2><p>了解考试涉及的概念并整体了解需要学习哪些内容。查看 <a href="http://awstrainingandcertification.s3.amazonaws.com/production/AWS_certified_sysops_associate_blueprint.pdf" target="_blank" rel="noopener">AWS Certified SysOps Administrator – Associate 考试指南</a>。<br>学习提示：不要跳过样题。</p><p>考试<a href="https://d0.awsstatic-china.com/training-and-certification/docs/AWS_certified_sysops_associate_examsample.pdf" target="_blank" rel="noopener">样题</a>可以帮助您检查自己对知识的掌握情况，并指明需要进一步学习的概念和领域。</p><h2 id="第-3-步：阅读官方考试学习指南">第 3 步：阅读官方考试学习指南</h2><p>AWS Certified SysOps Administrator – Associate 考试：官方学习指南由 AWS 专家编写而成。该指南可以让您做好准备以便在考试中证明自己的网络技能，其中涵盖各项考试目标，并结合 AWS Certified SysOps Administrator 可能遇到的各种情况指导您完成动手实验。从部署、管理和操作到迁移、数据流、成本控制等等，本指南将帮助您掌握与 AWS 相关的流程和最佳实践。</p><p>这里可以参考我的另外一篇博文<a href="https://longshilin.com/blog/112641e9.html">有关AWS考证的思考</a></p><h2 id="第-4-步：研究-aws-白皮书和常见问题">第 4 步：研究 AWS 白皮书和常见问题</h2><p>通过 AWS 团队、独立分析师和 AWS 合作伙伴编制的白皮书加深您的技术理解。</p><p>学习提示：重点学习以下白皮书。</p><p>云架构设计：AWS 最佳实践 || AWS 安全最佳实践 || Amazon Web Services：安全流程概述 || AWS 架构完善的框架 || AWS 上的开发和测试 || 使用 AWS 的备份与恢复方法 || Amazon Virtual Private Cloud 连接选项 || AWS 的定价机制</p><p>查看所有白皮书</p><p>浏览常见问题列表，熟悉不同用户经常提出的问题。</p><p>学习提示：重点查看以下常见问题。</p><p>Amazon EC2 || Amazon S3 || Amazon VPC || Amazon Route 53 || Amazon RDS || Amazon SQS</p><p>查看所有常见问题</p><h2 id="第-5-步：参加备考培训">第 5 步：参加备考培训</h2><p>利用我们的备考：AWS Certified SysOps Administrator – Associate（AWS 认证系统操作管理员 – 助理级）课程，向经过认证的技术讲师学习备考策略。</p><p><img src="https://i.imgur.com/mPAj5wu.png" alt="AWS培训课程指南"></p><blockquote><p>这一步也没有参加备考培训，转为是自己的自学过程，会在下面贴出自己的学习路径</p></blockquote><h3 id="aws尖峰学堂系列">AWS尖峰学堂系列</h3><p>通过<kbd>AWS尖峰学堂系列</kbd>课程的学习，可以掌握各个主要服务的入门级介绍，下面是整个视频列表。</p><ul><li>01-IAM忠实的AWS守门员.mp4</li><li>02-AWS安全运营体系概览.mp4</li><li></li></ul><h3 id="快速上手实验室">快速上手实验室</h3><p><a href="https://amazon.qwiklabs.com/catalog?locale=zh" target="_blank" rel="noopener">动手实验</a></p><h2 id="第-6-步：参加模拟考试">第 6 步：参加模拟考试</h2><p>在规定时间内答题，测试自己对知识的掌握情况。前往 aws.training 注册。</p><blockquote><p>这一步也没有参加线上模拟考试，下面会记录自己的刷题路径</p></blockquote><h2 id="第-7-步：报考并获得认证">第 7 步：报考并获得认证</h2><p>前往 aws.training 在您附近的测试中心报考。下面是AWS中各个认证类型</p><p><img src="https://i.imgur.com/enpg9cz.png" alt="AWS认证类别"></p>]]></content>
      
      
      <categories>
          
          <category> 认证 </category>
          
          <category> AWS认证 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws-soa </tag>
            
            <tag> aws </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>如何优雅写作</title>
      <link href="/blog/52b07b5c.html"/>
      <url>/blog/52b07b5c.html</url>
      
        <content type="html"><![CDATA[<div class="bs-callout bs-callout-primary"><p>如何优雅的写作，这里会包含两个模块：一个是如何方便些，一个是如何写的漂亮。</p></div><a id="more"></a><h2 id="如何写的方便">如何写的方便</h2><p>下面首先从如何方便写开始，这里我推荐Atom这款由 <i class="fa fa-github"></i>开发的编辑器，其中包含丰富的插件系统，可以支撑你的MarkDown写作，并且这些个性化设置及插件可以跨平台同步，实现你在多台机器上可以快速搭建你最熟悉的写作环境。</p><p>对于我使用的插件，可以参考我的这篇文章 <a href="https://longshilin.com/wiki/editor/atom-practice.html">Atom 实战</a>，里面包含如何同步fork我的插件列表到你的Atom个人配置中~ 🎆</p><p>下面我简单罗列下我安装的插件，在<a href="https://atom.io/packages" target="_blank" rel="noopener">Atom Packages</a>可以查询到下面这些的插件，并自行安装各个插件。</p><ul><li>atom-terminal</li><li>autocomplete-emjis</li><li>language-markdown</li><li>markdown-preview-enhanced</li><li>markdown-table-editor</li><li>markdown-writer</li><li>sync-setting</li></ul><h2 id="如何写的漂亮">如何写的漂亮</h2><p>这里分享有关这个hexo 主题原作者分享的一些渲染方法 <a href="https://www.cgmartin.com/tags/hexo/" target="_blank" rel="noopener">Hexo Theme</a>，详细信息可以查看在线PDF <a href="https://github.com/longshilin/files-repo/blob/master/Getting%20Started%20with%20the%20Hexo%20Blogging%20Framework%20_%20Christopher%20Martin.pdf" target="_blank" rel="noopener">Getting Started with the Hexo Blogging Framework</a> 和 <a href="https://github.com/longshilin/files-repo/blob/master/Hexo%20Theme_%20Bootstrap%20Blog%20_%20Christopher%20Martin.pdf" target="_blank" rel="noopener">Hexo Theme_ Bootstrap Blog</a></p><p>下面会总结一下最佳实践方式：</p><h3 id="表格">表格</h3><p>Markdown:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">| Table Header 1 | Table Header 2 | Table Header 3 |</span><br><span class="line">| - | - | - |</span><br><span class="line">| Division 1 | Division 2 | Division 3 |</span><br><span class="line">| Division 1 | Division 2 | Division 3 |</span><br><span class="line">| Division 1 | Division 2 | Division 3 |</span><br></pre></td></tr></table></figure><p>…outputs:</p><table><thead><tr><th>Table Header 1</th><th>Table Header 2</th><th>Table Header 3</th></tr></thead><tbody><tr><td>Division 1</td><td>Division 2</td><td>Division 3</td></tr><tr><td>Division 1</td><td>Division 2</td><td>Division 3</td></tr><tr><td>Division 1</td><td>Division 2</td><td>Division 3</td></tr></tbody></table><h3 id="bootstrap标注">Bootstrap标注</h3><p>这个Bootstrap标注可以给你的文章增添更多的醒目标记信息。</p><p>In the Markdown:</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;% callout info %&#125;</span><br><span class="line">#### &#123;% fa info-circle %&#125; Info</span><br><span class="line">This is some info content</span><br><span class="line">&#123;% endcallout %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% callout warning %&#125;</span><br><span class="line">#### &#123;% fa exclamation-triangle %&#125; Warning</span><br><span class="line">This is some warning content</span><br><span class="line">&#123;% endcallout %&#125;</span><br><span class="line"></span><br><span class="line">&#123;% callout danger %&#125;</span><br><span class="line">#### &#123;% fa exclamation-triangle %&#125; Danger</span><br><span class="line">This is some danger content</span><br><span class="line">&#123;% endcallout %&#125;</span><br></pre></td></tr></table></figure><p>…outputs:</p><div class="bs-callout bs-callout-info"><h4 id="i-class-fa-fa-info-circle-i-info"><i class="fa fa-info-circle"></i> Info</h4><p>This is some info content</p></div><div class="bs-callout bs-callout-warning"><h4 id="i-class-fa-fa-exclamation-triangle-i-warning"><i class="fa fa-exclamation-triangle"></i> Warning</h4><p>This is some warning content</p></div><div class="bs-callout bs-callout-danger"><h4 id="i-class-fa-fa-exclamation-triangle-i-danger"><i class="fa fa-exclamation-triangle"></i> Danger</h4><p>This is some danger content</p></div><h3 id="fa图标">fa图标</h3><p>在 <a href="https://fontawesome.com/icons" target="_blank" rel="noopener">Font Awesome 资源</a>  可以选择你需要的任意图标。</p><blockquote><p>举个栗子： 对于<i class="fa fa-github"></i>在Font Awesome中是<code>&lt;i class=&quot;fa github&quot;&gt;&lt;/i&gt;</code>，于是在博客中引入这个图标时，写入<code><i class="fa fa-github"></i></code></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
          <category> 写博工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> write </tag>
            
            <tag> markdown </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>站点的自动化部署及发布（CI/CD）</title>
      <link href="/blog/ea0641b.html"/>
      <url>/blog/ea0641b.html</url>
      
        <content type="html"><![CDATA[<p>今天看了业界比较知名的个人博主左耳朵耗子-陈浩在极客时间专栏的博文深有感触，其中对于自己写博如何将文章写得“独一份”，这是你需要思考和追求的，与其你不断优化站点的SEO，不如让你的文章更加与众不同，观点是别人少有研究，或者你的观点更鲜明，你就能被别人搜索到。</p><p>基于这个理念，我觉得对这两个月折腾的个人建站最有价值的部分，有必要好好写写，分享给想通过自动化的方式部署及发布你的博文的朋友。</p><a id="more"></a><p>在这篇文章中，我主要分享如何借助外部工具实现你的博文自动部署及更新发布站点，其中涉及的主要技术有：</p><ul><li>docker</li><li>git</li></ul><p>借助的第三方平台时<a href="https://www.gitlab.com" target="_blank" rel="noopener">Gitlab</a>，可能很多朋友听说或是用过<a href="https://www.github.com" target="_blank" rel="noopener">Github</a>，但对前者熟悉度不高。其实我也是在最近搞自动化部署站点的时候，才了解这个工具的。它的优势在于，不仅同样提供Pages功能，而且能够支持将资源push到仓库后，自动触发Gitlab的<a href="https://about.gitlab.com/features/gitlab-ci-cd/" target="_blank" rel="noopener">CI/CD模块</a>，这样能够按照你设定的方式进行构建和部署发布。</p><p><img src="https://longshilin.com/images/20190507102157.png" alt="ci-cd-test-deploy-illustration"></p><p>就如同下面这张图中，你的资源更新和自动化部署是可以迭代更新的，运用到我们持续集成我们的文章，持续发布我们的站点，同样是可以的。</p><p><img src="https://longshilin.com/images/20190507102146.png" alt="ci-cd-devops-loop"></p><p>下面我详细说一下我们站点的自动化部署及发布需要进行的准备工作：</p><ol><li>首先你需要创建一个gitlab的项目仓库，这里gitlab比github更nice的地方是支持免费的私有仓库服务，见<a href="https://docs.gitlab.com/ee/user/project/repository/" target="_blank" rel="noopener">创建资源仓库</a></li><li>然后编写gitlab能够自动部署的脚本命令 <a href="https://docs.gitlab.com/ee/ci/yaml/" target="_blank" rel="noopener">.gitlab-ci.yml</a></li><li>在编写上一步yml文件时，可以模仿这个<a href="https://gitlab.com/pages/hexo#gitlab-ci" target="_blank" rel="noopener">gitlab hexo page</a><br>这里我将我项目中用的yml文件贴出来，供大家参考：</li></ol><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="comment"># longsl/hexo2simiki</span></span><br><span class="line"><span class="attr">image:</span> <span class="string">$你的docker-image镜像，如：longsl/hexo2simiki</span></span><br><span class="line"></span><br><span class="line"><span class="attr">pages:</span></span><br><span class="line"><span class="attr">  cache:</span></span><br><span class="line"><span class="attr">    paths:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">/builds/longshilin/longshilindotcom/blog/node_modules</span></span><br><span class="line"><span class="attr">  script:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">HOME=`pwd`</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">REMOTE_PROJECT=longshilindotcom</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">git</span> <span class="string">config</span> <span class="bullet">--global</span> <span class="string">user.email</span> <span class="string">"583297550@qq.com"</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">git</span> <span class="string">config</span> <span class="bullet">--global</span> <span class="string">user.name</span> <span class="string">"longsl"</span></span><br><span class="line">    <span class="comment"># 构建hexo</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">cd</span> <span class="string">$HOME/blog</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">npm</span> <span class="string">install</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">hexo</span> <span class="string">g</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">cp</span> <span class="bullet">-r</span> <span class="string">blog</span> <span class="string">$HOME/public</span></span><br><span class="line">    <span class="comment"># 构建simiki</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">cd</span> <span class="string">$HOME/wiki</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">simiki</span> <span class="string">g</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">cp</span> <span class="bullet">-r</span> <span class="string">wiki</span> <span class="string">$HOME/public</span></span><br><span class="line">    <span class="comment"># 同步静态网页到国内git.dev.tencent.com仓库</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">cd</span> <span class="string">~;</span> <span class="string">git</span> <span class="string">clone</span> <span class="string">git@git.dev.tencent.com:yilong0722/longshilindotcom.git;</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">rm</span> <span class="bullet">-rf</span> <span class="string">$REMOTE_PROJECT/*;</span> <span class="string">cp</span> <span class="bullet">-rf</span> <span class="string">$HOME/public/*</span> <span class="string">$REMOTE_PROJECT;</span> <span class="string">cd</span> <span class="string">$REMOTE_PROJECT</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">git</span> <span class="string">add</span> <span class="string">*;</span> <span class="string">git</span> <span class="string">commit</span> <span class="bullet">-a</span> <span class="bullet">-m</span> <span class="string">"Site updated：`date`"</span><span class="string">;</span> <span class="string">git</span> <span class="string">push</span> <span class="bullet">-f</span></span><br><span class="line"><span class="attr">  artifacts:</span></span><br><span class="line"><span class="attr">    paths:</span></span><br><span class="line"><span class="bullet">    -</span> <span class="string">public</span></span><br><span class="line"><span class="attr">  only:</span></span><br><span class="line"><span class="bullet">  -</span> <span class="string">master</span></span><br></pre></td></tr></table></figure><p>在你的新增资源push到gitlab的代码仓库中，会自动触发CI/CD程序，即运行这个默认脚本。最终上面的artifacts中表示的是你要发布的静态资源。默认情况下它是可以通过<a href="https://docs.gitlab.com/ee/user/project/pages/" target="_blank" rel="noopener">gitlab page</a>来访问，如果你觉得gitlab pages对国内的访问速度不够快的话，<a href="http://xn--publicpagescoding-jy50af2jj0cwxs9ey3mk8csxm4vaj94d6h7aseyp5t1tevvow9ynk3c7f6cra0130h6qzec9ualpbe51o.net" target="_blank" rel="noopener">你还可以将public下的静态资源再推送到国内同样支持pages功能的coding.net</a>，ps:最近coding.net被腾讯开发者平台收购了，成为了腾讯开发者平台的资源仓库。</p><ol start="5"><li>在gitlab pages或者coding pages上绑定你的域名，可以参考<a href="https://longshilin.com/blog/57c188a8.html">gitlab绑定域名</a>，并设置国内和国外的分区访问和DNS解析。实现国内国外对资源的加载速度的优化。下面是我的域名在腾讯云控制台中的设置详情。<br><img src="https://i.imgur.com/MdwYs8q.png" alt=""></li></ol><p><strong>这样能够使得国内用户和海外用户能相对快读的加载你的静态资源网页。</strong></p><blockquote><p>2019-01-09 更新<br>目前直接是通过将静态网站部署在github上，并开启page功能向全网同步。 资源CI/CD依然是gitlab，<a href="http://xn--coding-hs2ji57af9rzh6cped.net" target="_blank" rel="noopener">弃用国内的coding.net</a> 因为在我使用期间经常出现域名解析出问题，访问不了的情况。</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
            <tag> CI/CD </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>优秀文章合集</title>
      <link href="/blog/f04e.html"/>
      <url>/blog/f04e.html</url>
      
        <content type="html"><![CDATA[<p><a href="https://liuyandong.com/2017/10/25/122/" target="_blank" rel="noopener">1. 十年学会编程</a></p>]]></content>
      
      
      <categories>
          
          <category> 资源 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 优秀文章 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>学习资源（一期）</title>
      <link href="/blog/649f.html"/>
      <url>/blog/649f.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>Avalon js在线教程<br><a href="http://edu.51cto.com/center/course/lesson/index?id=46928" target="_blank" rel="noopener">http://edu.51cto.com/center/course/lesson/index?id=46928</a></p></blockquote><a id="more"></a><blockquote><p>「极客时间」极客专栏-左耳听风 [PDF] (268M) <a href="https://time.geekbang.org/column/intro/48?from=trial" target="_blank" rel="noopener">https://time.geekbang.org/column/intro/48?from=trial</a><br>地址：<a href="http://www.52studyit.com/thread-1242-1-1.html" target="_blank" rel="noopener">http://www.52studyit.com/thread-1242-1-1.html</a><br>链接: <a href="https://pan.baidu.com/s/1xTnzDZxMOMU7kLWD1XCyjA" target="_blank" rel="noopener">https://pan.baidu.com/s/1xTnzDZxMOMU7kLWD1XCyjA</a><br>提取码: xzcw</p></blockquote><blockquote><p>Docker+Kubernetes(k8s)微服务容器化实战 [MP4]<br>地址：<a href="http://www.52studyit.com/forum.php?mod=viewthread&amp;tid=193&amp;highlight=Kubernetes" target="_blank" rel="noopener">http://www.52studyit.com/forum.php?mod=viewthread&amp;tid=193&amp;highlight=Kubernetes</a><br>链接: <a href="https://pan.baidu.com/s/1B1hqpzEBWH-du-aEMZFBoQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1B1hqpzEBWH-du-aEMZFBoQ</a><br>密码: vmgv</p></blockquote><blockquote><p>Docker技术高级应用实战（含KubernetesK8S）[MP4] (8.33G)<br>地址：<a href="http://www.52studyit.com/thread-909-1-1.html" target="_blank" rel="noopener">http://www.52studyit.com/thread-909-1-1.html</a></p></blockquote><blockquote><p>Kubernetes企业级Docker容器集群管理平台实战 [MP4] (3.52G)<br>地址：<a href="http://www.52studyit.com/thread-907-1-1.html" target="_blank" rel="noopener">http://www.52studyit.com/thread-907-1-1.html</a></p></blockquote><blockquote><p>AWS 云计算-AWS 操作指南系列课程 [MP4] (1.58G)<br><a href="http://www.52studyit.com/thread-1426-1-1.html" target="_blank" rel="noopener">http://www.52studyit.com/thread-1426-1-1.html</a><br>链接: <a href="https://pan.baidu.com/s/1m3Q2MEJ0d8xRHmcRzDwrrw" target="_blank" rel="noopener">https://pan.baidu.com/s/1m3Q2MEJ0d8xRHmcRzDwrrw</a><br>提取码: hj34</p></blockquote><blockquote><p>魔乐科技 SpringBoot框架开发详解 [MP4] (6.13G)<br><a href="http://www.52studyit.com/thread-1290-1-1.html" target="_blank" rel="noopener">http://www.52studyit.com/thread-1290-1-1.html</a><br>链接: <a href="https://pan.baidu.com/s/1smc0b14lo6Nx1Bgx120DzQ" target="_blank" rel="noopener">https://pan.baidu.com/s/1smc0b14lo6Nx1Bgx120DzQ</a><br>提取码: iwfa</p></blockquote><blockquote><p>Spring源码解读与设计详析 [MP4] (3.87G)<br><a href="http://www.52studyit.com/thread-715-1-1.html" target="_blank" rel="noopener">http://www.52studyit.com/thread-715-1-1.html</a><br>链接: <a href="https://pan.baidu.com/s/1ZUf_LM7z2xcnJ2DEbZnFtw" target="_blank" rel="noopener">https://pan.baidu.com/s/1ZUf_LM7z2xcnJ2DEbZnFtw</a><br>密码: xdn9</p></blockquote><blockquote><p>极客时间零基础学Python [MP4] (2.06G)<br><a href="http://www.52studyit.com/thread-1513-1-1.html" target="_blank" rel="noopener">http://www.52studyit.com/thread-1513-1-1.html</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 资源 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 极客时间 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>风移影动 | 珊珊可爱</title>
      <link href="/blog/157464b7.html"/>
      <url>/blog/157464b7.html</url>
      
        <content type="html"><![CDATA[<script src="//cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script><div id="hbe-security">  <div class="hbe-input-container">  <input type="password" class="hbe-form-control" id="pass" placeholder="Welcome to my blog, enter password to read." />    <label for="pass">Welcome to my blog, enter password to read.</label>    <div class="bottom-line"></div>  </div></div><div id="decryptionError" style="display: none;">Incorrect Password!</div><div id="noContentError" style="display: none;">No content to display!</div><div id="encrypt-blog" style="display:none">U2FsdGVkX1/Kxk05pu1dnoMaoCNYg3ssuZyT6rUAnIF9rFcKXrZAl+VR0hxGnBFKGeFj/flKpCmobYuKdK+ht/uBNusOKCNvTD86xQXTASyp4IWfT2HHc1c1ub8+ejCeIA7GjFG+fHvKeRsgmvkYQ+d4V5KM5yF1Gw2ywoN1D2Ro1V5/hPYWrAKaXo4sP1EiWhA4xaa0fzqgT5p9d/uk3JPALatkJS+ROAVuy2sf5O2zEhwwxUajfPhkd/n2pSy6IOrvsT403KRTgm0kSA2q0d2QTiedHW/qYBbbvphPQ9uycOxLxifU5gaziYf7Zba2091ANvQ4vwalmNlJ12Yfrbh1tVZKlaEnI+NgrFePbbsRhe0PyoLY+gJ3UfqrgH2ROaF6kdND0xO3ACBJucmAKGuNqo2KGeBhK4oMttujlT6e2znt02GWjZRJlX21oNWsurvOUBXqDGucNbi664h07NZ4IZg3RxU6NfS/WZv5WU6XUL0AK31IYlqOJJyr9gPrbn3svVxuWPlwU0L0CnAOVxcw9S7vesfGhLVs3WAAFsbYsQw3SufQ3uX5KqYbAj22XBtkXpg89kTE4LRzLgHWRxYnwhu0JaJmW8zCIxD9cOFLqif3F0WxvDJnzvW2KJvxqkrNFa7VmM/gAFbolhiEXnIi2KLXSfc7obedFnDJSWOOszlRmu20uNNYW1YwNyqfQvfOAOS3z2rrnCg6bGz/QUikiirOwBug1kN3ELerj9iRSkmVSfy3wNJYptffYOj/Cq99llZVQCtWEMC1TZXr3g==</div><script src="/blog/lib/crypto-js.js"></script><script src="/blog/lib/blog-encrypt.js"></script><link href="/blog/css/blog-encrypt.css" rel="stylesheet" type="text/css">]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>轻量化去广告的内存清理工具</title>
      <link href="/blog/70c155f1.html"/>
      <url>/blog/70c155f1.html</url>
      
        <content type="html"><![CDATA[<p>其实我的需求仅仅是需要是在电脑上安装一个清理内存的软件，但是在安装了那么多大型软件之后，我发现或多或少会有一些广告的成分在其中，因为他们需要从广告中获利，才能继续维系他们的产品开发也业务拓展，所以对于用这些大型管理软件和我的需求是有冲突的，因为我不喜欢臃肿的功能以及广告弹窗的出现，因此我选择了这个应用，它足够轻量 免费，而且无广告。</p><a id="more"></a><p>下面，我介绍一款自认为足够满足轻量化与去广告需求的开源应用，并且仅仅是具备清理机器内存这一个功能，非常有针对性，很Nice！</p><p>它是个人项目，这里是网址：<a href="https://www.henrypp.org/product/memreduct" target="_blank" rel="noopener">https://www.henrypp.org/product/memreduct</a></p><p><img src="https://i.imgur.com/GoD9FAR.jpg" alt="memreduct"></p><p>轻松上手，下面是自定义设置，可以达到双击右下角图标自动清理机器内存的目的。<br><img src="https://i.imgur.com/pnTJ47T.png" alt=""></p><p>下面是个人设置，实现上面的双击图标自动清理内存，并没有任何弹窗，极其方便~ 🌛<br><img src="https://i.imgur.com/rOtHQQV.png" alt=""><img src="https://i.imgur.com/pX6btFW.png" alt=""><img src="https://i.imgur.com/lJFvaGL.png" alt=""><img src="https://i.imgur.com/nbSPcPl.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tools </tag>
            
            <tag> memreduct </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>站点开源框架</title>
      <link href="/blog/49462.html"/>
      <url>/blog/49462.html</url>
      
        <content type="html"><![CDATA[<h1>DokuWiki</h1><p>DokuWiki是一个简单易用，功能多样的开源维基软件，不需要数据库。用户喜欢它清晰易读的语法。易于维护，备份和集成使其成为管理员的最爱。内置的访问控制和身份验证连接器使DokuWiki在企业环境中特别有用，其充满活力的社区贡献的大量插件允许广泛的用例超越传统的wiki。   <a href="https://www.dokuwiki.org" target="_blank" rel="noopener">https://www.dokuwiki.org</a><br><img src="https://longshilin.com/images/20190507102234.png" alt="DokuWiki Home"></p><h1>Homeland</h1><p>开源、免费、不限制商业使用的社区/论坛系统   <a href="https://gethomeland.com" target="_blank" rel="noopener">https://gethomeland.com</a><br><img src="https://longshilin.com/images/20190507102245.png" alt="Homeland Home"></p>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>website优化之路</title>
      <link href="/blog/baf88334.html"/>
      <url>/blog/baf88334.html</url>
      
        <content type="html"><![CDATA[<p>在这里我会从上到下介绍我所做的优化和调整，会把每个模块的修改都覆盖到，而中间踩过的坑都给大家添上。同时也会记录自己的创新点和实践心得等等，欢迎大家评论~</p><a id="more"></a><p><img src="https://i.imgur.com/5x7OAkx.png" alt="Blog首页"></p><p>其中这个样式的原版是下面这样的：<br><img src="https://i.imgur.com/a6hZnuv.png" alt="原版博客样式"><br><a href="https://www.cgmartin.com" target="_blank" rel="noopener">详见原作者博客</a> | 另外该样式的<a href="https://github.com/cgmartin/hexo-theme-bootstrap-blog" target="_blank" rel="noopener">Github地址</a></p><blockquote><p>乍一看，感觉两者没有实质上的改变，大部分都相同。确实是这样的，下面我会一一列举我新增的内容和填过的坑。😄</p></blockquote><h2 id="导航栏">导航栏</h2><p>首先是导航栏部分，可以看到左侧导航栏出现了多个标签页，这里我需要在<code>theme/_config.yml</code>文件中新增menu菜单栏，实质上脚本文件会自动解析你的config配置项，然后到<code>masthead.ejs</code>文件中将所有的menu以遍历的形式进行展现出来，具体看下面相关的代码：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[第16行开始]</span><br><span class="line">&lt;!-- Collect the nav links, forms, and other content for toggling --&gt;</span><br><span class="line">&lt;div class=&quot;collapse navbar-collapse&quot; id=&quot;main-menu-navbar&quot;&gt;</span><br><span class="line">  &lt;ul class=&quot;nav navbar-nav&quot;&gt;</span><br><span class="line">    &lt;% for (var i in theme.menu)&#123; %&gt;</span><br><span class="line">      &lt;li&gt;&lt;a class=&quot;&lt;%= is_current(theme.menu[i]) ? &apos;active&apos; : &apos;&apos; %&gt;&quot;</span><br><span class="line">             href=&quot;&lt;%- url_for(theme.menu[i]) %&gt;&quot;&gt;&lt;%= i %&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">    &lt;% &#125; %&gt;</span><br><span class="line">  &lt;/ul&gt;</span><br><span class="line">......</span><br></pre></td></tr></table></figure><h2 id="导航栏右侧的个人站点图表">导航栏右侧的个人站点图表</h2><p>在 <kbd>masthead.ejs</kbd> 文件中加入下面的内容，这是整个顶部导航栏右侧的图表列表。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;ul class=&quot;nav navbar-nav navbar-right&quot; id=&quot;sub-menu-navbar&quot;&gt;</span><br><span class="line">  &lt;% if (theme.rss)&#123; %&gt;</span><br><span class="line">    &lt;li&gt;&lt;a href=&quot;&lt;%- theme.rss %&gt;&quot; title=&quot;RSS Feed&quot;&gt;&lt;i class=&quot;fa fa-rss&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">  &lt;% if (theme.github)&#123; %&gt;</span><br><span class="line">    &lt;li&gt;&lt;a href=&quot;https://github.com/&lt;%- theme.github %&gt;&quot; title=&quot;Github&quot;&gt;&lt;i class=&quot;fa fa-github&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">  &lt;% if (theme.gitlab)&#123; %&gt;</span><br><span class="line">    &lt;li&gt;&lt;a href=&quot;https://gitlab.com/&lt;%- theme.gitlab %&gt;&quot; title=&quot;Gitlab&quot;&gt;&lt;i class=&quot;fa fa-gitlab&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">  &lt;% if (theme.stackoverflow)&#123; %&gt;</span><br><span class="line">    &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/users/&lt;%- theme.stackoverflow %&gt;&quot; title=&quot;StackOverFlow&quot;&gt;&lt;i class=&quot;fa fa-stack-overflow&quot;&gt;&lt;/i&gt;&lt;/a&gt;&lt;/li&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">&lt;/ul&gt;</span><br></pre></td></tr></table></figure><h2 id="个人头像专栏设置">个人头像专栏设置</h2><p><img src="https://i.imgur.com/BBSVXSH.png" alt=""></p><p>这里也是有一个文件保存这些信息 <kbd>header.ejs</kbd></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;div class=&quot;blog-header hidden-xs&quot;&gt;</span><br><span class="line">  &lt;img id=&quot;blog-logo&quot; width=&quot;80&quot; height=&quot;80&quot; alt=&quot;Long Shilin&quot; src=&quot;https://www.gravatar.com/avatar/06c71e5f94c268040fd4068f336188e7.png&quot;&gt;</span><br><span class="line">  &lt;h2 class=&quot;blog-title&quot;&gt;&lt;%= config.title %&gt;&lt;/h2&gt;</span><br><span class="line">  &lt;% if (theme.subtitle)&#123; %&gt;</span><br><span class="line">    &lt;p class=&quot;lead blog-description&quot;&gt;&lt;%= theme.subtitle %&gt;&lt;/p&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">&lt;/div&gt;</span><br><span class="line">&lt;div class=&quot;blog-header visible-xs&quot;&gt;</span><br><span class="line">  &lt;h1 class=&quot;blog-title&quot;&gt;&lt;%= config.title_mobile %&gt;&lt;/h1&gt;</span><br><span class="line">  &lt;% if (theme.subtitle)&#123; %&gt;</span><br><span class="line">    &lt;p class=&quot;lead blog-description&quot;&gt;&lt;%= theme.subtitle_mobile %&gt;&lt;/p&gt;</span><br><span class="line">  &lt;% &#125; %&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><h2 id="博客分享按钮的添加">博客分享按钮的添加</h2><p><img src="https://i.imgur.com/Ycqq7iw.png" alt=""><br>这里我加入来五种平台的分享，其中除微信外，其他集中可以直接跳转到对应的Web或App中进行分享。</p><p>下面主要介绍这几个平台分享URL是如何进行拼接的。当时找微博的url拼接的方法还是找了好一圈，不过最后还是解决了～😄 (有问题可以在下面评论区留言)</p><ul><li>微博、微信、Twitter、Facebook、Google+</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;a href=&quot;http://service.weibo.com/share/share.php?appkey=&amp;title=&apos;+ &quot;【&quot; + title + &quot;】&quot; + description +&apos;&amp;url=&apos; + encodedUrl + &apos;&amp;searchPic=false&amp;style=simple&quot; class=&quot;article-share-weibo&quot; target=&quot;_blank&quot; title=&quot;微博&quot;&gt;&lt;/a&gt;,</span><br><span class="line">&lt;a href=&quot;http://qr.liantu.com/api.php?text=&apos; + encodedUrl + &apos;&quot; class=&quot;article-share-weixin&quot; target=&quot;_blank&quot; title=&quot;微信&quot;&gt;&lt;/a&gt;,</span><br><span class="line">&lt;a href=&quot;https://twitter.com/intent/tweet?url=&apos; + encodedUrl + &apos;&quot; class=&quot;article-share-twitter&quot; target=&quot;_blank&quot; title=&quot;Twitter&quot;&gt;&lt;/a&gt;,</span><br><span class="line">&lt;a href=&quot;https://www.facebook.com/sharer.php?u=&apos; + encodedUrl + &apos;&quot; class=&quot;article-share-facebook&quot; target=&quot;_blank&quot; title=&quot;Facebook&quot;&gt;&lt;/a&gt;,</span><br><span class="line">&lt;a href=&quot;https://plus.google.com/share?url=&apos; + encodedUrl + &apos;&quot; class=&quot;article-share-google&quot; target=&quot;_blank&quot; title=&quot;Google+&quot;&gt;&lt;/a&gt;</span><br></pre></td></tr></table></figure><h2 id="博客中增加emoji表情">博客中增加emoji表情</h2><p>我推荐这篇博文 <a href="https://www.cnblogs.com/fsong/p/5929773.html" target="_blank" rel="noopener">Hexo中添加emoji表情</a>，持久化保存的pdf版<a href="https://pan.baidu.com/s/1PQdP5PQ-6HqFYAOq0AZY5A" target="_blank" rel="noopener">下载</a></p><h2 id="hexo文章置顶功能">hexo文章置顶功能</h2><p>npm插件：<a href="https://www.npmjs.com/package/hexo-generator-topindex" target="_blank" rel="noopener">hexo-generator-topindex</a></p><p>另外在index页面的文章简介出，修改了显示置顶文章的样式，新增了一个小图标<br><img src="https://i.imgur.com/c5soh5Y.png" alt=""></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;% if (post.top)&#123; %&gt;&lt;i class=&quot;fa fa-thumb-tack&quot;&gt;&lt;font color=7D26CD&gt;&amp;nbsp;置顶 &amp;nbsp;&amp;nbsp;&lt;/font&gt;|&lt;/i&gt;&lt;% &#125; %&gt;</span><br></pre></td></tr></table></figure><h2 id="对接评论系统">对接评论系统</h2><p>在我的博客中，我先后接入评论有四款，按照时间先后顺序分别是：<a href="https://disqus.com" target="_blank" rel="noopener">disqus</a> -&gt; <a href="https://github.com/imsun/gitment" target="_blank" rel="noopener">gitment</a> -&gt; <a href="https://changyan.kuaizhan.com/" target="_blank" rel="noopener">畅言</a> -&gt; <a href="https://valine.js.org/quickstart.html" target="_blank" rel="noopener">valine</a>，现在选择了valine 也是我比较看好的。首先它不像disqus，需要翻墙才能评论，虽然有博友已经开发了对于disqus的代理，但是还是有缺陷；后来转战gitment，这个由于<a href="https://github.com/imsun/gitment/issues/170" target="_blank" rel="noopener">存在第三方验证</a>的事情，感觉会导致评论系统的不稳定；转而国内比较稳的畅言，也因为我的域名有备案，因此还是很顺利的接入了，但是个人感觉UI实在不适合技术类博客，因此最后选择valine这一款评论系统，它是基于<a href="https://leancloud.cn/" target="_blank" rel="noopener">leancloud</a>，我们使用免费版即可满足小规模的评论，这里推荐一个valine上手参考博文<a href="https://panjunwen.com/valine-admin-document/" target="_blank" rel="noopener">Valine Admin 配置手册</a>，支持邮件通知新评论。<a href="https://pan.baidu.com/s/13chCFPv-NX1B0kJUdFl2lw" target="_blank" rel="noopener">Valine Admin pdf版下载</a></p>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>第一次来南京</title>
      <link href="/blog/adb7c3d4.html"/>
      <url>/blog/adb7c3d4.html</url>
      
        <content type="html"><![CDATA[<p>这是我第一次来到珊珊的城市，心情无比的激动和兴奋，在这里和我宝开心的渡过了四天假期，下面是我宝精心制作的视频剪辑哦，非常Nice，另外她还给我秀了一波她剪辑的小工具“Vlog” 👯</p><a id="more"></a><video src="http://repo.longshilin.com/video/%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%8E%BB%E5%8D%97%E4%BA%AC.mp4" poster="https://ws1.sinaimg.cn/large/005EsThygy1fxfy8o5oiij30k20qrnpd.jpg" preload="" controls style="max-width: 60%; display: block; margin-left: auto; margin-right: auto;">your browser does not support the video tag</video>]]></content>
      
      
      
    </entry>
    
    <entry>
      <title>利用Docker快速部署wordpress站点</title>
      <link href="/blog/b39d28f7.html"/>
      <url>/blog/b39d28f7.html</url>
      
        <content type="html"><![CDATA[<p>在学习Docker的时候，看到关于通过Docker容器的方式快速搭建WordPress站点，这里加以记录，便于日后的查看。</p><a id="more"></a><h2 id="在单一容器中使用supervisor运行wordpress">在单一容器中使用Supervisor运行WordPress</h2><h3 id="问题">问题</h3><p>你希望在一个容器中运 行应用程序所需的所有服务。以运行 WordPress 为例，你想在一个容器中同时运行 MySQL 和 HTTPD 服务。由于 Docker 运行的是前台进程，所以你需要找到一种同时运行多个“前 台”进程的方式。</p><h3 id="解决方案">解决方案</h3><p>使 用 Supervisor(<a href="http://supervisord.org/index.html" target="_blank" rel="noopener">http://supervisord.org/index.html</a>) 来 监 控 并 运 行 MySQL 和 HTTPD。<br>Supervisor 不是一个 init 系统，而是一个用来控制多个进程的普通程序。</p><p>本范例是一个在容器中使用 Supervisor 同时运行多个进程的例子。你可以以 此为基础在一个 Docker 镜像中运行多个服务(比如 SSH、Nginx)。本范例 中，WordPress 的配置是一个最精简的可行配置，并不适用于生产环境。</p><p>示例中的文件可以在 GitHub(<a href="https://github.com/how2dock/docbook/tree/master/ch01/supervisor" target="_blank" rel="noopener">https://github.com/how2dock/docbook/tree/master/ch01/supervisor</a>) 下载。这些文件中包括一个用于启动虚拟机的 Vagrantfile，Docker 运行在该虚拟机 中，还包含一个 Dockerfile 来定义要创建的镜像，此外还有一个 Supervisor 的配置文件(supervisord.conf)和一个 WordPress 的配置文件(wp-config.php)。</p><p>为了运行 WordPress，你需要安装 MySQL、Apache 2(即 httpd)、PHP 以及最新版本 的 WordPress。你将需要创建一个用于 WordPress 的数据库。在该范例的配置文件中， WordPress 数据库用户名为 root，密码也是 root，数据库名为 wordpress。如果你想对数据 库的配置进行修改，需要同时修改 wp-config.php 和 Dockerfile 这两个文件，并让它们的设 置保持一致。</p><p>这里定义了两个被监控和运行的服务:mysqld 和 httpd。每个程序都可以使用诸如 autorestart 和 autostart 等选项。最重要的指令是 command，其定义了如何运行每个程序。 在这个例子中，Docker 容器只需要运行一个前台进程:supervisord。从 Dockerfile 中的 CMD [“/usr/bin/supervisord”] 这一行也能看出来。</p><p>在你的 Docker 主机上，构建该镜像并启动一个后台容器。如果按照例子中的配置文件使 用了基于 Vagrant 的虚拟机，可以执行如下命令。</p><pre><code> $ cd /vagrant $ docker build -t wordpress . $ docker run -d -p 80:80 wordpress</code></pre><p>容器启动后还会在宿主机和 Docker 容器之间为 80 端口进行端口映射。现在只需要在浏览 器中打开 http://&lt;ip_of_docker_host&gt;，就可以进入到 WordPress 的配置页面了。</p><h3 id="讨论">讨论</h3><p>尽管通过 Supervisor 在一个容器内同时运行多个应用服务工作起来非常完美，但是你最好 还是使用多个容器来运行不同的服务。这能充分利用容器的隔离优势，也能帮助你创建基 于微服务设计思想的应用(参见《微服务设计》1)。最终这也将会使你的应用更具弹性和可 扩展性。</p><h3 id="参考">参考</h3><p>• Supervisor 文档(<a href="http://supervisord.org/index.html" target="_blank" rel="noopener">http://supervisord.org/index.html</a>)<br>• Docker Supervisor 文档(<a href="https://docs.docker.com/articles/using_supervisord/" target="_blank" rel="noopener">https://docs.docker.com/articles/using_supervisord/</a>)</p><h2 id="使用两个链接在一起的容器运行wordpress博客程序">使用两个链接在一起的容器运行WordPress博客程序</h2><h3 id="问题-v2">问题</h3><p>你希望通过容器来运行一个 WordPress 网站(<a href="http://wordpress.com/" target="_blank" rel="noopener">http://wordpress.com/</a>)，但是你不想让 MySQL 和 WordPress 在同一个容器中运行。你时刻谨记对关注点进行分离的原则，并尽 可能地对应用程序的不同组件进行解耦。</p><h3 id="解决方案-v2">解决方案</h3><p>启动两个容器:一个运行来自 Docker Hub(<a href="http://hub.docker.com/" target="_blank" rel="noopener">http://hub.docker.com/</a>)的官方 WordPress， 另一个运行 MySQL 数据库。这两个容器通过 Docker 命令行工具的 --link 选项链接在 一起。<br>开始下载最新的 WordPress(<a href="https://hub.docker.com/" target="_blank" rel="noopener">https://hub.docker.com/</a><em>/wordpress/)和 MySQL(https://.hub. <a href="http://docker.com/" target="_blank" rel="noopener">docker.com/</a></em>/mysql/)镜像，如下所示。$ docker pull wordpress:latest<br>$ docker pull mysql:latest</p><p>启动一个 MySQL 容器，并通过命令行工具的 --name 选项为这个容器设置一个名称，通过 MYSQL_ROOT_PASSWORD 环境变量来设置 MySQL 的密码，如下所示。</p><pre><code> $ docker run --name mysqlwp -e MYSQL_ROOT_PASSWORD=wordpressdocker -d mysql</code></pre><p>如果在使用 mysql 镜像时没有指定标签，Docker 会自动使用 latest 标签， 这也是前面刚刚下载的镜像。容器通过 -d 选项以守护式的方式开始运行。</p><p>现在就可以基于 wordpress:latest 镜像启动 WordPress 容器了。这个容器将会通过 --link 选 项链接到 MySQL 容器，这样 Docker 会自动进行网络配置，让 WordPress 容器能访问到 MySQL 容器中暴露出来的端口，如下所示。</p><pre><code> $ docker run --name wordpress --link mysqlwp:mysql -p 80:80 -d wordpress</code></pre><p>两个容器都会以后台方式运行，WordPress 容器的 80 端口会被映射到宿主机的 80 端口上， 如下所示。</p><pre><code>[root@ip-172-31-1-1 supervisor]# docker ps -aCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                 NAMES48b9c0cabe31        wordpress           &quot;/usr/bin/supervisord&quot;   5 seconds ago       Up 4 seconds        0.0.0.0:80-&gt;80/tcp    wordpress3756d0e220bf        mysql               &quot;docker-entrypoint...&quot;   44 seconds ago      Up 43 seconds       3306/tcp, 33060/tcp   mysqlwp</code></pre><p>在浏览器中打开 http://&lt;ip_of_host&gt; 就会看到 WordPress 的安装界面，里面有选择语言的窗 口，如图 1-10 所示。完成了 WordPress 的安装过程，你将会得到一个在两个链接到一起的 容器之上运行的 WordPress 网站。</p><h3 id="讨论-v2">讨论</h3><p>这两个 WordPress 和 MySQL 镜像都是官方镜像，分别由 WordPress 和 MySQL 的社区来维护。Docker Hub 这些镜像的页面都有关于如何进行配置以从这些镜像创建容器的详细文档。</p><p>令人感兴趣的是，你可以通过设置几个环境变量来创建一个数据库，并且只有具有相应权 限的用户才能操作数据库:MYSQL_DATABASE、MYSQL_USER 和 MYSQL_PASSWORD。在前面的例子中，WordPress 使用了 MySQL 的 root 用户，这并不是一个好实践。最好是创建一个名 为 wordpress 的数据库，并为其创建一个用户，像下面这样。</p><pre><code>$ docker run --name mysqlwp -e MYSQL_ROOT_PASSWORD=wordpressdocker \                                 -e MYSQL_DATABASE=wordpress \                                 -e MYSQL_USER=wordpress \                                 -e MYSQL_PASSWORD=wordpresspwd \                                 -d mysql</code></pre><p>数据库容器启动之后，可以启动 WordPress 容器并指定你设置好的数据库表，如下所示。</p><pre><code>$ docker run --name wordpress --link mysqlwp:mysql -p 80:80 \                                   -e WORDPRESS_DB_NAME=wordpress \                                   -e WORDPRESS_DB_USER=wordpress \                                   -e WORDPRESS_DB_PASSWORD=wordpresspwd \                                   -d wordpress</code></pre><p>如果你需要删除所有容器，可以使用下面这种嵌套 shell 的快捷方式。</p><pre><code>$ docker stop $(docker ps -q)$ docker rm -v $(docker ps -aq)</code></pre><p>docker rm 命令的 -v 选项用来删除 MySQL 镜像中定义的数据卷。</p><h2 id="docker搭建wordpress的最佳实践">docker搭建WordPress的最佳实践</h2><p>上面两个模块的内容是出自于《docker实战》这本书中，下面我结合自己的实际操作，总结出搭建的实际步骤。</p><h3 id="1-docker-mysql">1.docker mysql</h3><p>实战模块还待完善…敬请期待</p><h2 id="linux搭建wordpress环境">Linux搭建Wordpress环境</h2><p>最后附上不是通过docker方式搭建博客的教程实例<a href="https://docs.ksyun.com/read/latest/129/_book/%E6%90%AD%E5%BB%BAwordpress%E7%8E%AF%E5%A2%83.html" target="_blank" rel="noopener">Linux搭建Wordpress环境</a> <a href="https://github.com/longshilin/files-repo/blob/master/Linux%E6%90%AD%E5%BB%BAWordpress%E7%8E%AF%E5%A2%83%20%C2%B7%20%E9%87%91%E5%B1%B1%E4%BA%91.pdf" target="_blank" rel="noopener">pdf下载</a></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> wordpress </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>个人建站SEO优化之路</title>
      <link href="/blog/75faa0d3.html"/>
      <url>/blog/75faa0d3.html</url>
      
        <content type="html"><![CDATA[<h2 id="个人站点优化">个人站点优化</h2><ol><li><a href="https://neilpatel.com/wp-content/uploads/2018/10/Neil-Patels-Advanced-Cheatsheet-to-SEO.pdf" target="_blank" rel="noopener">ADVANCED CHEAT SHEET TO SEO.pdf</a></li><li><a href="https://neilpatel.com/blog/personal-branding-seo/" target="_blank" rel="noopener">Personal SEO: 14-Point Checklist to Dominate Your Personal Brand on Google</a></li></ol><h3 id="none"></h3><h2 id="向搜索引擎推送站点链接">向搜索引擎推送站点链接</h2><h3 id="baidudotcom">Baidudotcom</h3><ol><li>获取自动推送脚本 <a href="https://longshilin.com/blog/js/baidu_js_push.js">baidu_push_js</a></li><li>将上述脚本放入<kbd>$themes/layout/layout.ejs</kbd>的<kbd><code>&lt;body&gt;&lt;/body&gt;</code></kbd>体内，即可实现在每个页面的HTML代码中包含以下自动推送JS代码。</li></ol><h3 id="参考">参考</h3><p><a href="https://ziyuan.baidu.com/college/courseinfo?id=267&amp;page=2#h2_article_title19" target="_blank" rel="noopener">百度站长工具</a></p>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
            <tag> SEO </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>个人建站的心路历程</title>
      <link href="/blog/d6a28195.html"/>
      <url>/blog/d6a28195.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.imgur.com/qOzpn01.jpg" alt=""></p><p><a href="https://blog.csdn.net/Coder__CS" target="_blank" rel="noopener"><code>CSDN BLOG</code></a> --&gt; <code>WordPress.com</code> --&gt; <code>wordpress vps</code> --&gt; <a href="https://gitlab.com/longshilin/longshilindotcom" target="_blank" rel="noopener"><code>Hexo + Github</code></a> --&gt; <a href="https://longshilin.gitlab.io/longshilindotcom/blog" target="_blank" rel="noopener"><code>gitlab pages</code></a> (阶段性胜利)</p><a id="more"></a><p>从2016年春天开始我的博客记录以来，这里走过了两年半的时间，其中前面两年是大学期间，因此主要是以CSDB Blog为主，而在最近的半年，才开始往自定义博客方面来考虑，下面我会分享我在上面给出的建站历程中的收获和好玩的事。</p><h2 id="为什么要写博客">为什么要写博客</h2><h3 id="它的意义是什么？">它的意义是什么？</h3><p>写博的过程是一个对知识的总结和提炼的过程，它是知识通过大脑加工后的产物，可以被持久化的有价值的东西。当你想通过文字的方式记录时，或者你想和他人分享时，写博这个举动也就应运而生。</p><h3 id="它的好处在哪？">它的好处在哪？</h3><ul><li>其一、由于它是一个持久化的东西，相较而言短期内不会消失，因此可以通过博客，窥探你最近的个人动态。</li><li>其二、可以作为归档地，将总结的知识整理为博客，方便日后查找。</li><li>其三、作为自己学习的成功展示和短期总结，更是为日后的二次更新优化提供基础蓝本。</li></ul><h3 id="为什么选择迁移自定义博客">为什么选择迁移自定义博客?</h3><p>所谓迁移自定义博客，指的是通过自己的域名，访问自己搭建的博客，使用DIY博客主题，撰写所学所感所悟…<br>那为什么要这么做呢，首先与第三方博客平台解耦合，能够脱离原博客平台对博客主题的限制，以此更能凸显自定义需求。另外，自定义博客是一个言论自由的地方，不会被第三方博客平台干涉。对于博客的持久化发布也更加彻底。另外，可以完全抛除博客界面嵌入广告之嫌，这也是我不选择第三方博客平台的原因，他们博客网站需要盈利，因此需要广告要盈收这个无可厚非。</p><h2 id="阶段性胜利的前因后果">阶段性胜利的前因后果</h2><h3 id="csdn-blog">CSDN BLOG</h3><p>这是我经历时间最长的<a href="https://blog.csdn.net/Coder__CS" target="_blank" rel="noopener">博客</a>，伴随着我大学后半段时间，因此上面记录着我大学期间的所学所感所悟。</p><h3 id="wordpress-com"><a href="http://WordPress.com" target="_blank" rel="noopener">WordPress.com</a></h3><p>第一次尝试是在<a href="https://wordpress.com/" target="_blank" rel="noopener">wordpress.com</a>搭建在线的博客网站，你无须掌握任何的网站搭建技术，只需要轻点鼠标即可完成所有设置操作，并可以更换博客主题以及自定义插件等等。</p><h3 id="wordpress-vps">wordpress vps</h3><p>算起来，从五一劳动节申请服务器主机到现在也就五个月不到，在那个三天假期，完成了WordPress在服务器上的搭建，并成功部署了自己的博客网站，并将访问站点添加了SSL证书，由http访问转为https。（开销：1核500MB内存，每个月500G流量的搬瓦工VPS服务器，上面搭建的ssr服务现在也还在用）</p><h3 id="hexo-github">Hexo + Github</h3><p>在上一步中提到的在服务器上搭建WordPress博客框架，各方面体验都很不错，但是我马上意识到这不是一个长久的办法，虽然部署博客网站以及自定义博客主题都非常方便，是以界面点击方式来进行的，但是你需要为服务器长期支付费用，而我的需求仅仅是一个博客站点，无需通过服务器来大动干戈。</p><p>所以采用下面的静态网站构建的方式：利用<a href="https://hexo.io" target="_blank" rel="noopener">hexo</a>框架构建静态网页，利用<a href="https://pages.github.com/" target="_blank" rel="noopener">github pages</a>功能来部署和发布。在实践过程中，时间主要花在git分支处理，以及博客主题的修改上。学习和了解了hexo主题模板构造模块和修改技巧等。</p><h3 id="gitlab-pages">Gitlab Pages</h3><p>采用Gitlab Page功能，并提供pages模板，你能够轻松部署静态网站，并通过Gitlab提供的CI/CD功能，轻松实现持续集成持续发布，修改提交即发布的理念, 详见<a href="https://longshilin.com/blog/2018/09/22/website-on-gitlab/">GitLab CI/CD</a>.</p><p>更多内容，详见 <a href="https://docs.gitlab.com/ee/university/README.html" target="_blank" rel="noopener">GitLab University</a></p><h3 id="wiki">wiki</h3><p>通过<a href="http://simiki.org/" target="_blank" rel="noopener">simiki框架</a>，搭建了自己的<a href="https://longshilin.gitlab.io/longshilindotcom/wiki" target="_blank" rel="noopener">wiki站点</a>，这个是个人网站的一部分，用来记录流程化的东西。</p><h2 id="个人写博的最佳实践">个人写博的最佳实践</h2><p>这里我会将我最终总结沉淀下来的写博最佳实践加以分享。</p><ul><li><p>编辑器 <a href="https://atom.io/" target="_blank" rel="noopener">Atom</a> ，另外新增了几个插件，能够非常方便的支持我的写作：<br><img src="https://i.imgur.com/WL0Sz2j.png" alt="atom-plugin-for-markdown"></p></li><li><p>发布及部署平台 <a href="https://pages.github.com/" target="_blank" rel="noopener">GitLab Pages</a><br><img src="https://i.imgur.com/nwIAHtY.png" alt="pages-group"></p></li></ul><p>在前期的搭建环境及其部署工作准备就绪后，我后期需要关注的仅仅是写作本身，另外将我新增的文件推送到远程代码仓库，仅此而已。然后稍微等待one minute，就可以在 <a href="https://longshilin.com/blog">页面</a> 上看到我的新增内容了。</p><h2 id="总结">总结</h2><p>回顾这整个搭建历程，给我感触最深的体会就是：**兴趣驱使着我往前走，折腾精神使得我有所收获！**不满足才能探索更大的未来，不是未来没有而是自己认知不够，因为你还远远没有触及边界。</p><blockquote><p>Change Log<br>2018/10/15 - 新增个人写博的最佳实践</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>有关AWS考证的思考</title>
      <link href="/blog/112641e9.html"/>
      <url>/blog/112641e9.html</url>
      
        <content type="html"><![CDATA[<p>对于之前做的几件事，其实会把对于aws复习的内容有所推迟，比如最近一直在做的个人建站以及工作上的开发事情，但是话说回来，虽然时间花在其他事情上面，但是要想达到备考AWS的状态，你就必须每天抽出一定的时间来定时学习和看书刷题等等。</p><p>下面我会做出一个细致的计划安排，在转正之前能够顺利考证（预计转正是2018-11-15，离现在还有50天不到），仔细想一想🤔，时间真的不多了哈。</p><a id="more"></a><p>按照之前的学习情况，自己做了有关<code>aws sdk for java</code>的接入，能够实现EC2对象的信息获取。现在准备起来，应该更加顺利～</p><table><thead><tr><th>备考要求</th><th>备考类型</th><th>备考内容</th><th>时间节点</th></tr></thead><tbody><tr><td>⚠️重点</td><td>看书</td><td>AWS-SYSOPS认证</td><td>两个星期左右的时间，书籍➕章节习题</td></tr><tr><td>重点</td><td>每个服务后面的Q&amp;A</td><td>常见问题的熟悉</td><td>一个星期轮完所有服务</td></tr><tr><td>重点</td><td>白皮书</td><td>每个类型服务的白皮书以及最佳实践</td><td>一个星期轮完</td></tr><tr><td>重点</td><td>练习题</td><td>历史公布的题库</td><td>最后的时间</td></tr></tbody></table><p>每天都要抽出大量的时间来进行自我备考学习，抽出全部的私人时间来投入最后的备考，真的不能再拖了… 时间节点很重要，时间计划和进度把控也要具备。</p><h2 id="aws-sysops书籍学习">AWS-SYSOPS书籍学习</h2><p><img src="https://i.imgur.com/cdGh95j.png" alt=""><br><a href="https://www.wiley.com/WileyCDA/WileyTitle/productCd-1119377420,miniSiteCd-SYBEX.html" target="_blank" rel="noopener">AWS Certified SysOps Administrator Official Study Guide: Associate Exam</a></p><p>电子版英文原版书籍 通过配合<a href="https://drive.google.com/drive" target="_blank" rel="noopener">Google Driver</a>和<a href="https://docs.google.com/document" target="_blank" rel="noopener">Google Document</a>在线编辑工具来整篇翻译PDF。这里需要注意的是：Google Document支持的在线翻译内容长度有限制，需要借助<a href="https://www.ilovepdf.com/zh_cn/fencai_pdf" target="_blank" rel="noopener">PDF分割工具</a>进行拆分。</p><p>在本书中访问<a href="http://www.wiley.com/go/sybextestprep" target="_blank" rel="noopener">http://www.wiley.com/go/sybextestprep</a>，使用学习工具注册并访问此交互式在线学习环境和测试库。具体的注册流程如下：</p><ul><li>进入<a href="http://customer.wiley.com/CGI-BIN/lansaweb?procfun+shopcart4+SH4FN19+funcparms+PARMKEYG%28A0060%29:SYBEX" target="_blank" rel="noopener">链接</a>，通过注册回答有关问题，获取访问PIN码。</li><li>收到包含PIN码的邮件。</li><li>点击<a href="http://testbanks.wiley.com/" target="_blank" rel="noopener">Register PIN or Login</a>，填入你的个人信息以及PIN码完成账号注册。</li><li>你就可以开始您自己的自定进度测试准备，<a href="https://testbanks.wiley.com/WPDACE/Products" target="_blank" rel="noopener">点击进入</a></li></ul><p><img src="https://i.imgur.com/2rGORyT.png" alt=""></p><p><img src="https://i.imgur.com/6KKopqj.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> 认证 </category>
          
          <category> AWS认证 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> aws </tag>
            
            <tag> sysops </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>gitlab绑定域名</title>
      <link href="/blog/57c188a8.html"/>
      <url>/blog/57c188a8.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.imgur.com/rQfzXnn.png" alt=""></p><p>通过gitlab部署并发布静态网站，通过绑定自己的域名来实现自定义访问。</p><a id="more"></a><p>在项目的设置中，选择pages项，里面配置你的域名证书，这里可以使用域名的注册商提供给你的证书，或者是通过<a href="https://freessl.org/" target="_blank" rel="noopener">FreeSSL</a>提供免费证书申请，将得到的证书文件依次拷贝到Pages中，创建 <strong>New Pages Domain</strong></p><p><img src="https://i.imgur.com/vO69TtL.png" alt=""></p>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
            <tag> 域名 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>构建hexo和simiki工具的docker镜像</title>
      <link href="/blog/fd401cf5.html"/>
      <url>/blog/fd401cf5.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.imgur.com/flxHs0r.png" alt=""><br>基于<a href="https://hexo.io" target="_blank" rel="noopener">hexo</a>和<a href="http://simiki.org/" target="_blank" rel="noopener">simiki</a>工具构建基础镜像环境，用作我website项目中<a href="https://longshilin.com/blog/2018/09/22/website-on-gitlab/">website-on-gitlab CI/CD</a>的基础镜像。并将其发布到Docker Hub的<br><a href="https://hub.docker.com/r/longsl/hexo2simiki/" target="_blank" rel="noopener">PUBLIC REPOSITORY</a></p><a id="more"></a><h1>DockerFile</h1><p>通过官方centos7镜像为基础来构建包含构建website的构建环境。具体的环境包括：hexo, python3.6, nodejs, simiki, pip3.6</p><pre><code>FROM centos#安装EPEL依赖 Python3.6 pip3 nodejs npm hexo simikiRUN yum install -y epel-release; yum install -y https://centos7.iuscommunity.org/ius-release.rpm; yum install -y python36u; yum install -y python36u-pip; pip3.6 install --upgrade pip; curl -sL https://rpm.nodesource.com/setup_8.x | bash -; yum install -y nodejs; npm install hexo-cli -g; pip3.6 install simiki</code></pre><h1>Docker Build</h1><pre><code>docker build -t longsl/hexo2simiki .</code></pre><h1>Docker Container</h1><pre><code>docker run -it --rm longsl/hexo2simki /bin/sh</code></pre><h1>Supply</h1><p>关于下载最新版本的nodejs费了笔者一些功夫，因为通过<code>yum install -y nodejs</code>直接下载的包不是最新的版本，在hexo构建时，会提示npm版本过低的报错，因此我通过参考 <a href="https://linuxize.com/post/how-to-install-node-js-on-centos-7/" target="_blank" rel="noopener">How to install Node.js on CentOS 7</a>，更新yum源来下载node 8.x的版本。</p><pre><code>curl -sL https://rpm.nodesource.com/setup_8.x | sudo bash -sudo yum install nodejs</code></pre>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
            <tag> hexo </tag>
            
            <tag> simiki </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>GitLab CI/CD</title>
      <link href="/blog/b4ccb03f.html"/>
      <url>/blog/b4ccb03f.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.imgur.com/BrmSMBD.png" alt="GitLab CI/CD"></p><p>在之前的github上搭建静态资源网站的基础上有了新的提升。把原来在本地机器编译的工作，通过gitlab实现自动化编译部署及发布，我只用专注于写作及将写好的内容上传到gitlab上即可。这也是现在最流行的CI/CD理念（持续集成持续发布）。</p><a id="more"></a><blockquote><p>下面着重来说明下我在部署过程中主要过程及填的坑等等，主要是提供一个部署思路和学习路径。</p></blockquote><h2 id="注册gitlab账号">注册gitlab账号</h2><p>在进行此次迁移资源之前，我是没有接触过<a href="https://gitlab.com/" target="_blank" rel="noopener">gitlab</a>的，自己这一块也是现用现学，所以说都是自己需要才会真正去接触它和学习它，好了继续~</p><h2 id="官方pages">官方Pages</h2><p>在gitlab上，有专门的<a href="https://gitlab.com/pages" target="_blank" rel="noopener">pages页面</a>，这里目前总共有三十种不同的模板，种类还是一应俱全。下面是首页的内容<br><img src="https://i.imgur.com/pXS3DgP.png" alt="gitlab pages"></p><p>我们的目的是通过官方<a href="https://gitlab.com/pages/hexo" target="_blank" rel="noopener">hexo pages</a>来学习快速入门及部署我们自己的静态网页。</p><h2 id="学习yaml">学习YAML</h2><p>通过gitlab<a href="https://docs.gitlab.com/ce/ci/yaml/README.html" target="_blank" rel="noopener">YAML官方文档</a>来学习YAML格式，这里我展示下在我的项目中的YAML文件</p><pre><code>image: longsl/hexo2simikipages:  cache:    paths:    - blog/node_modules/  script:    - HOME=/builds/longshilin/longshilindotcom    - cd blog    - npm install    - hexo deploy    - cp -r blog $HOME/public    - cd $HOME/wiki    - simiki g    - cp -r wiki $HOME/public/  artifacts:    paths:    - public  only:  - master</code></pre><p>这里还是比较清晰，容易理解的。上面的YAML文件主要进行的操作时通过longsl/hexo2simiki镜像作为基础环境，并在上面进行项目构建，并将指定的文件目录public作为发布资源进行发布。详细说明：</p><ol><li>image是本次构建所使用的镜像，这里我使用的是包含hexo和simiki构建工具的自定义镜像，详见<a href="https://longshilin.com/blog/2018/09/22/hexo2simiki/">hexo2simiki</a></li><li>pages表示是一个默认的任务</li><li>cache表示在该任务下将指定的路径下的文件进项缓存（其实就是在构建开始的时候将上次打包并保存的文件再次拿来用），这样在每次构建并发布时，可以省去很多固定资源的构建开销。</li><li>script下面是编写shell脚本，这些脚本命令直接就是在docker容器中执行的，因此如果你的镜像时以linux为基础，可以直接执行。</li><li>artifacts是发布项，可以指定本次构建出的哪部分内容会被保留下来，并作为该pipline的发布源文件。</li><li>only指的是本次job任务只会涉及到master分支上的内容。</li></ol><p>更多信息详见 <a href="https://docs.gitlab.com/ce/ci/" target="_blank" rel="noopener">GitLab Continuous Integration</a></p><h2 id="部署及发布">部署及发布</h2><p>编写自己的.gitlab-ci.yml文件，并放在项目的根目录下，这样在每次有新的更新提交到gitlab上时，会自动触发进行项目构建。</p>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CI/CD </tag>
            
            <tag> website </tag>
            
            <tag> gitlab </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>给自定义博客增加totop按钮</title>
      <link href="/blog/5b764c97.html"/>
      <url>/blog/5b764c97.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://longshilin.com/images/20190507102321.png" alt=""><br>新增图片右下角的totop按钮</p><a id="more"></a><h3 id="新增totop-ejs文件">新增totop.ejs文件</h3><p>在文件目录<code>blog/themes/hexo-theme-bootstrap-blog/layout/_partial</code>中新增 <a href="https://gitlab.com/longshilin/longshilindotcom/blob/master/blog/themes/hexo-theme-bootstrap-blog/layout/_partial/totop.ejs" target="_blank" rel="noopener">totop.ejs</a></p><h3 id="修改layout-ejs">修改layout.ejs</h3><p>layout.ejs表示的是blog页面的主页面样式排版，将上一步中的文件纳入到最终的页面样式中。</p><pre><code>&lt;%- partial('_partial/totop') %&gt;</code></pre><h3 id="js文件">js文件</h3><p><a href="https://gitlab.com/longshilin/longshilindotcom/blob/master/blog/themes/hexo-theme-bootstrap-blog/source/js/totop.js" target="_blank" rel="noopener">totop.js</a></p><h3 id="css格式">css格式</h3><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-id">#totop</span> &#123;</span><br><span class="line">  <span class="attribute">position</span>: fixed;</span><br><span class="line">  <span class="attribute">bottom</span>: <span class="number">4em</span>;</span><br><span class="line">  <span class="attribute">right</span>: <span class="number">2em</span>;</span><br><span class="line">  <span class="attribute">cursor</span>: pointer;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> website </category>
          
      </categories>
      
      
        <tags>
            
            <tag> longshilindotcom </tag>
            
            <tag> hexo </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>在玖万里的第一次团建</title>
      <link href="/blog/1b08df06.html"/>
      <url>/blog/1b08df06.html</url>
      
        <content type="html"><![CDATA[<p><img src="https://i.imgur.com/7umqiRu.jpg" alt="团建公司合照"><br>今天（2018-09-15）迎来了90km的2018年秋季团建。上面这张是我们本次团建收获的团队合照~~</p><a id="more"></a><h1>清晨集合 准备出发</h1><p>早市八点在公司集合，八点半准时出发。我们搭乘的旅游车分成经济舱和商务舱两个部分，两辆大巴塞下了全公司。</p><p>这次我们要驱车200多公里，来到浙江省湖州市安吉县的百草原景区参加公司团建，下面是我google地图路线。<br><img src="https://i.imgur.com/zExtQ8S.png" alt="永和路--安吉百草原"></p><p>经过上午的颠簸，我们在中午12点多到达安吉县，在进行午饭会餐后，我们又驱车前往下午的活动场地–<a href="http://www.znc.cn/" target="_blank" rel="noopener">中南百草原</a></p><p>来到了浙江湖州的安吉县-百草原景区，开展我们的团建活动，整个园区的风景还是非常nice的~<br><img src="https://i.imgur.com/JZmpRcN.jpg" alt=""></p><h1>团建活动</h1><h2 id="团队制作赛车大比拼">团队制作赛车大比拼</h2><p>我们将公司70多位员工分为6组，每组需要在一个小时内，完成整个赛车的拼装与装饰，而且要求能够乘人，即精美与实用性并存的赛车制作大赛。</p><p>下面我通过图片的形式加以展现每个队的风采</p><p><img src="https://i.imgur.com/oGUrs5k.jpg" alt="Part 1"></p><p><img src="https://i.imgur.com/LypeUUI.jpg" alt="Part 2"></p><p><img src="https://i.imgur.com/lcAYaj4.jpg" alt="Part 3"></p><p><img src="https://i.imgur.com/3TYqbjM.jpg" alt="Part 4"></p><p><img src="https://i.imgur.com/ADHbxKI.jpg" alt="Part 5"></p><p><img src="https://i.imgur.com/SAV6hqx.jpg" alt="Part 6"></p><h2 id="真人cs">真人CS</h2><p><img src="https://i.imgur.com/0nOTCW0.jpg" alt=""></p><p><img src="https://i.imgur.com/AR2I9Yw.jpg" alt=""></p><p><img src="https://i.imgur.com/oj0j4B2.jpg" alt=""></p><h2 id="卡丁车项目">卡丁车项目</h2><p><img src="https://i.imgur.com/RNGwfmO.jpg" alt=""></p><p><img src="https://i.imgur.com/xMLCSPL.jpg" alt=""></p><h1>团建晚会</h1><h2 id="bbq烧烤">BBQ烧烤</h2><p><img src="https://i.imgur.com/tU7ofPq.jpg" alt=""></p><p><img src="https://i.imgur.com/AxXd7h3.jpg" alt=""></p><p><img src="https://i.imgur.com/JDHIaGV.jpg" alt=""></p><h2 id="游戏晚会">游戏晚会</h2><p><img src="https://i.imgur.com/nkq5aYw.jpg" alt="被打扮得婀娜多姿的公司男同事们 (/ω＼)"></p><p><img src="https://i.imgur.com/NgEqL2e.jpg" alt=""></p><h2 id="篝火晚会">篝火晚会</h2><p><img src="https://i.imgur.com/BprN7Ql.jpg" alt=""></p><p><img src="https://i.imgur.com/BAZpIdh.jpg" alt=""></p><p><img src="https://i.imgur.com/pALx5qf.jpg" alt=""></p><p><img src="https://i.imgur.com/CJSMKaz.jpg" alt=""></p><p><img src="https://i.imgur.com/Op5uCqP.jpg" alt=""></p><h1>参观大熊猫</h1><p><img src="https://i.imgur.com/QoH3HfE.jpg" alt="so cute"></p><div class="video-container"><iframe src="//www.youtube.com/embed/LW2D7D5kbro" frameborder="0" allowfullscreen></iframe></div><h1>感触</h1><p>在这两天一晚的团建活动中，我收获了很多，其中除了放松身心外，还包括对公司文化的人士，朋友结交以及团队的建设方面都有深刻的体会。</p><p>谢谢公司的良好氛围，另外附上一段当晚住的四星级酒店的小视频 YouTube视频需要科学上网：)</p><div class="video-container"><iframe src="//www.youtube.com/embed/njhPDOC1hqY" frameborder="0" allowfullscreen></iframe></div>]]></content>
      
      
      <categories>
          
          <category> 旅游 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 90km </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>制作带有java监控命令的镜像</title>
      <link href="/blog/5544e824.html"/>
      <url>/blog/5544e824.html</url>
      
        <content type="html"><![CDATA[<p>在生产环境中，经常有docker容器的CPU消耗太高而导致宿主机的CPU占用率攀升，引起AWS CloudWatch的警报邮件等，另外在批量重启容器的时候，也会经常遇到短暂对宿主机的CPU消耗过高的情况，现阶段准备再原来的容器中新增java的监控命令。具体是在镜像中包含如：jps、jstat和ps等命令的容器。</p><a id="more"></a><h2 id="寻找包含jstat的openjdk">寻找包含jstat的openjdk</h2><p>可以通过以下命令查找哪些openjdk版本包中含需要的命令。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yum provides &quot;*jstat&quot; --enablerepo=epel</span><br><span class="line">yum provides &quot;*jps&quot; --enablerepo=epel</span><br><span class="line">yum provides &quot;*ps&quot; --enablerepo=epel</span><br></pre></td></tr></table></figure><p>最终选择你需要的版本，这里我选择的是<code>java-1.8.0-openjdk-devel-1.8.0.181-3.b13.el7_5.x86_64</code>,在这个版本包中包含所需的java监控命令。</p><blockquote><p><strong>(一步到位, 一个Dockerfile搞定所有)</strong></p></blockquote><h2 id="制作非缩减版镜像">制作非缩减版镜像</h2><h3 id="dockfile">Dockfile</h3><figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> docker.io/centos:<span class="number">7.4</span>.<span class="number">1708</span></span><br><span class="line"><span class="comment"># author</span></span><br><span class="line"><span class="keyword">MAINTAINER</span> longsl &lt;<span class="number">583297550</span>@qq.com&gt;</span><br><span class="line"><span class="comment"># install jdk including jps,jstat,ps command.</span></span><br><span class="line"><span class="keyword">RUN</span> yum -y update &amp;&amp; yum -y install java-1.8.0-openjdk-1.8.0.191.b12-0.el7_5.x86_64 &amp;&amp; yum -y install java-1.8.0-openjdk-devel-1.8.0.191.b12-0.el7_5.x86_64</span><br><span class="line"># add group and user in OS</span><br><span class="line">RUN groupadd -g 2003 kpgame &amp;&amp; useradd -u 2003 -g 2003 -s /sbin/nologin -d /dev/null -M kpgame</span><br><span class="line"># add tomcat gzfile</span><br><span class="line">ADD tomcat8.tar /usr/local/tomcat/</span><br><span class="line">ADD kp.tar /usr/local/tomcat/webapps/kp/</span><br><span class="line"># set env</span><br><span class="line">ADD https://s3.amazonaws.com/resource.pinball-heroes.com/dockerfiles/common/tomcat/setenv.sh /usr/local/tomcat/bin</span><br><span class="line">RUN chown -R kpgame:kpgame /usr/local/tomcat/</span><br><span class="line">ENV CATALINA_HOME /usr/local/tomcat</span><br><span class="line">ENV CATALINA_BASE /usr/local/tomcat</span><br><span class="line">ENV PATH $&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/bin::$CATALINA_HOME/lib:$CATALINA_HOME/bin</span><br><span class="line"># run container with base path:/usr/local/tomcat/bin/catalina.sh</span><br><span class="line">CMD ["/usr/local/tomcat/bin/catalina.sh", "run"]</span><br></pre></td></tr></table></figure><h3 id="build-image">build image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">sudo docker build -t longsl/kp .</span><br></pre></td></tr></table></figure><h3 id="进入容器内部">进入容器内部</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker <span class="built_in">exec</span> -it KP_KP_SERVER_s8009_kp_29 /bin/bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过镜像的方式直接启动容器并进入</span></span><br><span class="line">sudo docker run -it ec2-18-221-193-240.us-east-2.compute.amazonaws.com/kp-longsl /bin/bash</span><br></pre></td></tr></table></figure><h3 id="镜像大小">镜像大小</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ec2-18-221-193-240.us-east-2.compute.amazonaws.com/kp   latest   4870cc895b20   3 hours ago   499 MB</span><br></pre></td></tr></table></figure><blockquote><p><strong>（分步完成）</strong></p></blockquote><h2 id="制作openjdk-tomcat8-image-第一步">制作openjdk+tomcat8 Image（第一步）</h2><p>上面那种是一次构建完成所有步骤，在我们的集群环境中，可能存在项目包的版本更新，所有需要制作一个基础的jdk+tomcat的基础版本，然后在这个镜像之上进行项目的版本迭代。</p><h3 id="dockerfile">Dockerfile</h3><figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> docker.io/centos:<span class="number">7.4</span>.<span class="number">1708</span></span><br><span class="line"><span class="comment"># author</span></span><br><span class="line"><span class="keyword">MAINTAINER</span> longsl &lt;<span class="number">583297550</span>@qq.com&gt;</span><br><span class="line"><span class="comment"># install jdk including jps,jstat,ps command.</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> yum -y install java-1.8.0-openjdk-devel-1.8.0.181-3.b13.el7_5.x86_64</span></span><br><span class="line"><span class="bash"><span class="comment"># add group and user in OS</span></span></span><br><span class="line"><span class="bash">RUN groupadd -g 2003 kpgame &amp;&amp; useradd -u 2003 -g 2003 -s /sbin/nologin -d /dev/null -M kpgame</span></span><br><span class="line"><span class="bash"><span class="comment"># add tomcat gzfile</span></span></span><br><span class="line"><span class="bash">ADD tomcat8.tar.gz /usr/<span class="built_in">local</span>/tomcat/</span></span><br></pre></td></tr></table></figure><h3 id="build-image-v2">build image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker build -t longsl/tomcat:8.0 .</span><br></pre></td></tr></table></figure><h2 id="制作镜像-第二步">制作镜像（第二步）</h2><h3 id="dockerfile-v2">Dockerfile</h3><figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="comment"># kpserver dockerfile</span></span><br><span class="line"><span class="keyword">FROM</span> ec2-<span class="number">18</span>-<span class="number">221</span>-<span class="number">193</span>-<span class="number">240</span>.us-east-<span class="number">2</span>.compute.amazonaws.com/tomcat</span><br><span class="line"><span class="keyword">MAINTAINER</span> dingxing &lt;<span class="number">303159963</span>@qq.com&gt;</span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> https://s3.amazonaws.com/resource.pinball-heroes.com/dockerfiles/common/tomcat/setenv.sh /usr/<span class="built_in">local</span>/tomcat/bin</span></span><br><span class="line"><span class="bash">ADD kp.tar /usr/<span class="built_in">local</span>/tomcat/webapps/kp/</span></span><br><span class="line"><span class="bash">RUN chown -R kpgame:kpgame /usr/<span class="built_in">local</span>/tomcat/</span></span><br><span class="line"><span class="bash">CMD [<span class="string">"/usr/local/tomcat/bin/catalina.sh"</span>, <span class="string">"run"</span>]</span></span><br></pre></td></tr></table></figure><h3 id="build-image-v3">build image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker build -t ec2-18-221-193-240.us-east-2.compute.amazonaws.com/kp:P11_1_V1.05.longsl .</span><br></pre></td></tr></table></figure><h3 id="镜像大小-v2">镜像大小</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ec2-18-221-193-240.us-east-2.compute.amazonaws.com/kp  P11_1_V1.05.longsl   8bcfe44ba2e5   9 minutes ago   617 MB</span><br></pre></td></tr></table></figure><h2 id="测试centos官方镜像">测试centos官方镜像</h2><h3 id="官方centos7镜像">官方centos7镜像</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker run -it centos:7.5.1804 /bin/bash</span><br><span class="line"></span><br><span class="line">sudo docker images | grep centos7.5.1804</span><br><span class="line">docker.io/centos      7.5.1804     fdf13fa91c6e      4 weeks ago      200 MB</span><br></pre></td></tr></table></figure><h3 id="官方最小版centos7镜像">官方最小版centos7镜像</h3><p>操作系统模板缓存是安装在容器中的操作系统模板，然后打包到gzip压缩包中。使用这样的缓存，可以在几分钟内创建新容器。<br>导入模板并启动容器 <a href="https://download.openvz.org/template/precreated/" target="_blank" rel="noopener">模板下载地址</a></p><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker import https://download.openvz.org/template/precreated/centos-7-x86_64-minimal.tar.gz longsl/centos-7-x86_64-minimal</span><br><span class="line"></span><br><span class="line">sudo docker images | grep centos-7-x86_64-minimal</span><br><span class="line">longsl/centos-7-x86_64-minimal     latest    08001dd5a457    7 minutes ago     435 MB</span><br><span class="line"></span><br><span class="line">sudo docker run -it longsl/centos-7-x86_64-minimal /bin/bash</span><br></pre></td></tr></table></figure><h2 id="制作精简版镜像-一个dockerfile搞定所有">制作精简版镜像(一个Dockerfile搞定所有)</h2><p>精简版镜像主要缩减的地方：centos不能缩减，jdk可以缩减到只需要最基本的功能。</p><h3 id="缩减jdk源文件">缩减jdk源文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#首先下载jre，下载地址是https://www.java.com/en/download/manual.jsp，大概是77M。</span></span><br><span class="line">wget http://javadl.oracle.com/webapps/download/AutoDL?BundleId=234464_96a7b8442fe848ef90c96a2fad6ed6d1</span><br><span class="line">mv AutoDL\?BundleId\=234464_96a7b8442fe848ef90c96a2fad6ed6d1 jre-8u181-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="comment">#解压，删除根目录下文本文件，然后删除其他不必要文件。</span></span><br><span class="line">tar xvcf jre-8u161-linux-x64.tar.gz</span><br><span class="line"><span class="comment">#进入目录</span></span><br><span class="line"><span class="built_in">cd</span> jre1.8.0_161/</span><br><span class="line"><span class="comment">#删除文本文件</span></span><br><span class="line">rm -rf COPYRIGHT LICENSE README release THIRDPARTYLICENSEREADME-JAVAFX.txt THIRDPARTYLICENSEREADME.txt Welcome.html</span><br><span class="line"><span class="comment">#删除其他无用文件</span></span><br><span class="line">rm -rf  lib/plugin.jar \</span><br><span class="line">           lib/ext/jfxrt.jar \</span><br><span class="line">           bin/javaws \</span><br><span class="line">           lib/javaws.jar \</span><br><span class="line">           lib/desktop \</span><br><span class="line">           plugin \</span><br><span class="line">           lib/deploy* \</span><br><span class="line">           lib/*javafx* \</span><br><span class="line">           lib/*jfx* \</span><br><span class="line">           lib/amd64/libdecora_sse.so \</span><br><span class="line">           lib/amd64/libprism_*.so \</span><br><span class="line">           lib/amd64/libfxplugins.so \</span><br><span class="line">           lib/amd64/libglass.so \</span><br><span class="line">           lib/amd64/libgstreamer-lite.so \</span><br><span class="line">           lib/amd64/libjavafx*.so \</span><br><span class="line">           lib/amd64/libjfx*.so</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新打包所有文件(不打包也可以，在Dockerfile里ADD这个目录即可，当前精简完jre目录大小是107M，压缩后是41M)</span></span><br><span class="line"><span class="comment"># 注意这里打出的包解压出来就直接是各个文件模块，不包含外层的文件夹</span></span><br><span class="line">tar -zcvf jre8.tar.gz *</span><br></pre></td></tr></table></figure><h3 id="dockerfile-v3">Dockerfile</h3><figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> docker.io/centos:<span class="number">7.4</span>.<span class="number">1708</span></span><br><span class="line"><span class="comment"># author</span></span><br><span class="line"><span class="keyword">MAINTAINER</span> longsl &lt;<span class="number">583297550</span>@qq.com&gt;</span><br><span class="line"><span class="comment"># install slim jdk</span></span><br><span class="line"><span class="keyword">ADD</span><span class="bash"> jre8.tar.gz /usr/<span class="built_in">local</span>/jdk/</span></span><br><span class="line"><span class="bash"><span class="comment"># add group and user in OS</span></span></span><br><span class="line"><span class="bash">RUN groupadd -g 2003 kpgame &amp;&amp; useradd -u 2003 -g 2003 -s /sbin/nologin -d /dev/null -M kpgame</span></span><br><span class="line"><span class="bash"><span class="comment"># add tomcat gzfile</span></span></span><br><span class="line"><span class="bash">ADD tomcat8.tar /usr/<span class="built_in">local</span>/tomcat/</span></span><br><span class="line"><span class="bash">ADD kp.tar /usr/<span class="built_in">local</span>/tomcat/webapps/kp/</span></span><br><span class="line"><span class="bash"><span class="comment"># set env</span></span></span><br><span class="line"><span class="bash">ADD https://s3.amazonaws.com/resource.pinball-heroes.com/dockerfiles/common/tomcat/setenv.sh /usr/<span class="built_in">local</span>/tomcat/bin</span></span><br><span class="line"><span class="bash">RUN chown -R kpgame:kpgame /usr/<span class="built_in">local</span>/tomcat/</span></span><br><span class="line"><span class="bash">ENV JAVA_HOME /usr/<span class="built_in">local</span>/jdk/</span></span><br><span class="line"><span class="bash">ENV CATALINA_HOME /usr/<span class="built_in">local</span>/tomcat</span></span><br><span class="line"><span class="bash">ENV CATALINA_BASE /usr/<span class="built_in">local</span>/tomcat</span></span><br><span class="line"><span class="bash">ENV PATH <span class="variable">$&#123;PATH&#125;</span>:<span class="variable">$&#123;JAVA_HOME&#125;</span>/bin:<span class="variable">$CATALINA_HOME</span>/lib:<span class="variable">$CATALINA_HOME</span>/bin</span></span><br><span class="line"><span class="bash"><span class="comment"># run container with base path:/usr/local/tomcat/bin/catalina.sh</span></span></span><br><span class="line"><span class="bash">CMD [<span class="string">"/usr/local/tomcat/bin/catalina.sh"</span>, <span class="string">"run"</span>]</span></span><br></pre></td></tr></table></figure><h3 id="build-image-v4">build image</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo docker build ec2-18-221-193-240.us-east-2.compute.amazonaws.com/kp:kp-slim .</span><br></pre></td></tr></table></figure><h3 id="镜像大小-v3">镜像大小</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ec2-18-221-193-240.us-east-2.compute.amazonaws.com/kp   kp-slim   3187e2e4a18d   36 minutes ago   458 MB</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>致  谢</title>
      <link href="/blog/e3e209fc.html"/>
      <url>/blog/e3e209fc.html</url>
      
        <content type="html"><![CDATA[<p>从论文的选题、资料收集再到撰写的整个过程，在碰到基因测序的疑难问题时，得到了许多老师和同学的热情帮助。</p><p>首先，我要感谢的是我的导师邝祝芳老师，当他得知我的毕设课题需要大数据平台与基因测序结合，询问我是否需要高性能计算的服务器，并随后立马帮我申请了湖南大学的天河一号超级计算机的使用账号。在我毕业设计的基因样本数据的准备过程，和基于Hadoop大数据平台的搭建过程中，他对我的研究提出了很多宝贵的意见，这也使我基因测序的研究方向更加清晰了，最后得以顺利开发出系统平台。</p><a id="more"></a><p>其次要感谢我们学院计算机教研室的各位老师们，在大学四年的求学生涯中，是他们循循善诱的教导开启了我的计算机专业的大门，并给了我无尽的启迪，使我毕业后能从事计算机行业相关工作，非常感谢老师们！</p><p>最后我要感谢陪伴在我身边的父母、同学和朋友们，谢谢他们在我成长过程中提出的意见和建议，感谢他们给我的生活提供的支持和帮助！</p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>参 考 文 献</title>
      <link href="/blog/3a1e37f9.html"/>
      <url>/blog/3a1e37f9.html</url>
      
        <content type="html"><![CDATA[<p>[1] Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating[P]. 74, 5463–5467 (1977).<br>[2] Struster SC.Next-generation sequencing transform today’s biology[J].Nat Methods.5(1):16-18 (2008).</p><a id="more"></a><p>[3] 解增言,林俊华,谭军,舒坤贤. DNA测序技术的发展历史与最新进展[J]. 生物技术通报. 2010(08).<br>[4] Rusk N. Cheap third-generation sequecing[J]. Nature. 6(4): 244-245 (2011).<br>[5] J. Craig Venter, Mark D. Adams, Eugene W. Myers. The Sequence of the Human Genome[J]. Science, 2001, 291(5507): 1304-1351.<br>[6] 高通量DNA测序技术及其应用进展[J]. 于聘飞,王英,葛芹玉. 南京晓庄学院学报2010-05-20 (05).<br>[7] 衣春翔. 哈工大牵头启动十万人基因组计划[N]. 黑龙江日报. 2017-12-29 (003).<br>[8] Jeffrey Dean, Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters[C]. America:Google, Inc., 2004. 137-149.<br>[9] Garry Turkington. Hadoop基础教程[M]. 张治起译. 人民邮电出版社 第1版, 2014.<br>[10] 新一代基因组测序-通往个性化医疗[M]. 贾尼特编著,薛庆中等译. 科学出版社, 2012.<br>[11] 蔡斌, 陈湘萍. Hadoop 技术内幕：深入解析Hadoop Common 和HDFS 架构设计与实现原理[M]. 机械工业出版社, 2013.<br>[12] 董西成. Hadoop技术内幕：深入解析MapReduce架构设计与实现原理[M]. 机械工业出版社, 2013.<br>[13] 董西成. Hadoop技术内幕：深入解析YARN架构设计与实现原理[M]. 机械工业出版社, 2013.<br>[14] 陈浩锋. 新一代基因组测序技术[M]. 科学出版社, 2017.<br>[15] Richard M, Leggett. Sequencing quality assessment tools to enable data-driven informatics for high throughput genomics[R]. US National Library of Medicine, 2013. 4-28.<br>[16] FastQC. The FastQC Toolkit[EB/OL]. <a href="https://www.bioinformatics.babraham.ac.uk/projects/fastqc/" target="_blank" rel="noopener">https://www.bioinformatics.babraham.ac.uk/projects/fastqc/</a>, 2018.<br>[17] Li H1, Durbin R. Fast and accurate short read alignment with Burrows-Wheeler transform[R]. US National Library of Medicine, 2009.<br>[18] BioITeam. Burrows-Wheeler Aligner[EB/OL]. <a href="https://github.com/lh3/bwa" target="_blank" rel="noopener">https://github.com/lh3/bwa</a>, 2018.<br>[19] Heng, Li. SAMtools[EB/OL]. <a href="http://samtools.sourceforge.net/" target="_blank" rel="noopener">http://samtools.sourceforge.net/</a>, 2018.<br>[20] BroadInstitute. The Genome Analysis Toolkit[EB/OL]. <a href="https://software.broadinstitute.org/gatk/" target="_blank" rel="noopener">https://software.broadinstitute.org/gatk/</a>, 2018.<br>[21] Joseph M. Caswell. Equilibrium and Association Analyses for Single Biallelic SNPs with Multiple Genetic Models: A SAS Macro with Simulated Data Examples[D]. Northeast Cancer Centre.<br>[22] Mohammad Shabbir Hasan, Xiaowei Wu, Layne T. Watson &amp; Liqing Zhang. UPS-indel: a Universal Positioning System for Indels[J]. nature, 2017.<br>[23] wikipedia. FASTA format[EB/OL]. <a href="https://en.wikipedia.org/wiki/FASTA_format" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/FASTA_format</a>, 2018.<br>[24] wikipedia. FASTQ format[EB/OL]. <a href="https://en.wikipedia.org/wiki/FASTQ_format" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/FASTQ_format</a>, 2018.<br>[25] Samy Ghoneimy, Samir Abou El-Seoud. A MapReduce Framework for DNA Sequencing Data Processing[D]. British University 2017.<br>[26] Apache. Apache FreeMarker™[EB/OL]. <a href="https://freemarker.apache.org/" target="_blank" rel="noopener">https://freemarker.apache.org/</a>, 2018.<br>[27] Mahmoud Parsian. 数据算法（Hadoop/Spark大数据处理技巧）[M]. 苏金国,杨健康等译. 清华大学出版社 第四版 2016.<br>[28] Tom White. Hadoop权威指南[M]. 王海,华东,刘喻,吕粤海译. 清华大学出版社 第四版 2017.<br>[29] Apache. Apache MRUNIT™[EB/OL]. <a href="http://mrunit.apache.org/" target="_blank" rel="noopener">http://mrunit.apache.org/</a>, 2018.<br>[30] The sratoolkit Toolkit [EB/OL]. <a href="https://github.com/ncbi/sra-tools/wiki/Downloads" target="_blank" rel="noopener">https://github.com/ncbi/sra-tools/wiki/Downloads</a>, 2018.</p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>结  论</title>
      <link href="/blog/88854671.html"/>
      <url>/blog/88854671.html</url>
      
        <content type="html"><![CDATA[<p>在本次的论文中，主要对Hadoop大数据框架做了深入了解，并对生物信息学中基因测序领域有了一个全新的认识。在这次基于Hadoop的基因组测序大数据分析平台研究的课题中，构建了针对生物全基因组的测序流程，并将自己所学的大数据领域的知识与全基因组测序流程相结合，利用Hadoop特有的HDFS分布式存储系统的特性，来容错的存储样本数据，并通过MapReduce计算框架将原本串行分析的WGS流程构建成不同的Map任务和Reduce任务，达到对不同的样本流程进行并行分析，提高基因测序的时效性和高扩展性。</p><a id="more"></a><p>该平台的研发可以适配大部分的全基因组测序处理流程，对于不同物种的全基因组测序，我们只需修改WGS测序流程模板和参考基因组序列即可，其他模块如数据存储与访问模式是一样的，另外FreeMarker相关的模板转换为脚本流程也是通用的等等。</p><p>在本次基于Hadoop的基因组测序大数据分析平台研发中，不仅让我了解Hadoop大数据平台的组件和框架应用，也让我看到了基因测序与个性化医疗的前景所在，完成一个人的全基因组测序所花的时间将会越来越短，个性化医疗离我们的脚步也越来越近！</p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>5 系统的测试与扩展</title>
      <link href="/blog/c7060cce.html"/>
      <url>/blog/c7060cce.html</url>
      
        <content type="html"><![CDATA[<h2 id="5-1-mrunit测试类编写">5.1 MRUnit测试类编写</h2><p><a href="http://mrunit.apache.org" target="_blank" rel="noopener">MRUnit</a>是一个MapReduce的测试库，它能将已知的输入通过函数的形式直接传递给Map()函数，然后该函数直接被调用运行，或者检查Reduce()函数的输出和测试的指定输出匹配，看是否符合预期输出。MRUnit与标准的测试执行框架（如：JUnit）一起使用<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>，因此可以在正常的开发环境中运行MapReduce作业的测试。</p><a id="more"></a><h3 id="5-1-1-map任务的测试类编写与调度">5.1.1 Map任务的测试类编写与调度</h3><p>这里通过对Mapper函数传入一个样本名称，如SRR3226035，测试其程序是否调用流程分析脚本，并输出正常结果。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">wgsMapperTest</span> </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        测试Mapper</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">wgs</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Text value = <span class="keyword">new</span> Text(“SRR3226035”);</span><br><span class="line">        <span class="keyword">new</span> MapDriver&lt;LongWritable, Text, Text, Text&gt;()</span><br><span class="line">                .withMapper(<span class="keyword">new</span> wgsMapper())</span><br><span class="line">                .withInput(<span class="keyword">new</span> LongWritable(<span class="number">0</span>), value)</span><br><span class="line">                .withOutput(<span class="keyword">new</span> Text(“<span class="number">1</span>”), value)</span><br><span class="line">                .runTest();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-1-2-reduce任务的测试类编写与调度">5.1.2 Reduce任务的测试类编写与调度</h3><p>这里通过对Reducer函数传入多个样本名称，如SRR3226035，测试其程序是否调用shell脚本，整合多个gvcf变异文件，并整合为一个VCF文件。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">wgsReducerTest</span> </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        测试Reducer</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Test</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">merge</span><span class="params">()</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        Text value1 = <span class="keyword">new</span> Text(<span class="string">"SRR3226035"</span>);</span><br><span class="line">        Text value2 = <span class="keyword">new</span> Text(<span class="string">"SRR3226039"</span>);</span><br><span class="line">        Text value3 = <span class="keyword">new</span> Text(<span class="string">"SRR3226042"</span>);</span><br><span class="line">        <span class="keyword">new</span> ReduceDriver&lt;Text, Text, Text, Text&gt;()</span><br><span class="line">                .withReducer(<span class="keyword">new</span> wgsReducer())</span><br><span class="line">                .withInputKey(<span class="keyword">new</span> Text(<span class="string">"1"</span>))</span><br><span class="line">                .withInputValues(Arrays.asList(value1,value2,value3))</span><br><span class="line">                .withOutput(<span class="keyword">new</span> Text(<span class="string">"1"</span>), value3)</span><br><span class="line">                .runTest();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-2-hadoop测序平台的测试">5.2 Hadoop测序平台的测试</h2><p>人类的参考基因组数据文件很大，其序列长度大概在3GB，包含30亿个碱基信息。而一个人的高深度测序所需要准备的样本数据往往是这个数字的30倍多，大约达到100GB。如果直接用这样的数据来完成基因测序分析，会下载大量的数据以及花费大量的处理时间，而且对机器性能也要有很高的要求。</p><p>因此在保证WGS全基因组测序流程的的完整性和正确性的情况下，使用一种大肠杆菌E.coli K12的全基因组数据作为代替。这是美国国家生物技术信息中心的一个研究课题，主要是针对一种大肠杆菌的全基因组测序的实验项目。该E.coli K12生物的基因组比较单一，全基因组中仅包含NC_000913.3这一种染色体，数据量相比于人类来说算很小的，它的全基因组的文本数据总大小只有4.6Mb，非常适合用来测试基于Hadoop的全基因组测序分析平台。以后对人类基因组数据作分析时再作替换即可。</p><h3 id="5-2-1-测试环境与测试数据准备">5.2.1 测试环境与测试数据准备</h3><p>在测试环境的准备中，在上一章中着重介绍了Hadoop伪分布式的搭建和Hadoop分布式的搭建流程，由于机器的限制，就本次的测试选择Hadoop伪分布环境下进行，我可以在我自己学习用的笔记本上直接搭建好该环境，接着对测试样本数据进行下载和预处理。</p><p>1）处理E.coli K12的参考基因组序列<br>物种的基因组参考序列可以在NCBI上获取，下面是E.coli K12参考序列的<a href="ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz" target="_blank" rel="noopener">ftp地址</a>。为了接下来表达上的清晰和操作上的方便，通过bgzip将这个序列文件进行解压并重命名为E.coli_K12_MG1655.fa<br><code>$ gzip -dc GCF_000005845.2_ASM584v2_genomic.fna.gz &gt; E.coli_K12_MG1655.fa</code><br>接着，使用Samtools软件为参考文件建立索引，方便其他数据分析工具（比如GATK）能够快速地获取fasta上的任何位点的序列信息。<br><code>$ /home/elon/biosoft/samtools/1.0/bin/samtools faidx E.coli_K12_MG1655.fa</code></p><p>2）下载E.coli K12的测序数据<br>基因组参考序列准备好之后，我们还需要准备该物种的测序样本数据。这里使用一组科研的E.coli K12实验数据<a href="https://www.ncbi.nlm.nih.gov/biosample/SAMN04505064/" target="_blank" rel="noopener">E.coli K12测试数据 </a>，其作为一种供研究使用的模式生物，已经在NCBI上有许多的测序数据了。我这里选取某次科研组织的样本数据共9组，样本号序列分别为：SRR3226034、SRR3226035 … SRR3226042。</p><p>通过<a href="http://www.ebi.ac.uk/ena/data/view/SRR_number" target="_blank" rel="noopener">点击下载数据样本</a>，并把最后SRR_number替换成指定的样本号。这些样本数据来自Illumina MiSeq测序平台，read长度是300bp，测序类型Pair-End，说明样 本有R1和R2双端测序数据。</p><p>从NCBI上下载下来的测序数据是一种NCBI的特殊格式SRA，它具较高的压缩率。我们需要对下载下来的SRA数据进行转换，得到我们需要的FASTQ格式数据，需要用到NCBI的官方工具包sratoolkit<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>，解压之后就可以使用了。</p><p>sratoolkit是一个工具包，所有的执行程序都在它解压后的bin文件夹下，我们要把SRA转换为fastq，只需直接通过工具包中的fastq-dump即可完成。<br><code>$ /home/elon/biosoft/sratoolkit/2.9.0/bin/fastq-dump --split-files SRR_number.sra</code></p><p>然后我们就会得到这个E.coli K12数据的read1和read2了：SRR_number_1.fastq和SRR_number_2.fastq。在得到R1和R2序列对之后，可以用bgzip命令将其压缩为.gz文件，这样可以节省空间，并且不会对接下来的数据分析产生影响。</p><pre><code>$ bgzip -f SRR_number_1.fastq$ bgzip -f SRR_number_2.fastq</code></pre><p>在得到gz格式的FASTQ样本数据之后，接下来就可以进行下面具体的测序分析流程了。</p><h3 id="5-2-2-系统平台的测试">5.2.2 系统平台的测试</h3><p>在本机的伪分布式的Hadoop环境中，用IDEA开发平台进行测试。首先通过以下指令运行Hadoop集群：<br><code>$ start-dfs.sh &amp;&amp; start-yarn.sh</code></p><p>编辑Mapper阶段的输入文本sample.txt，每行指定一个样本数据：</p><pre><code>SRR3226034SRR3226035SRR3226036SRR3226037SRR3226038SRR3226039SRR3226040SRR3226041SRR3226042</code></pre><p>接下来，需要通过以下命令将样本数据上传到HDFS上的指定文件夹：<br><code>$ hadoop fs -put sample.txt /wgsv2-input/</code></p><p>最后，调用wgsDriver驱动程序，这是整个项目的入口程序。从这里开始整个项目的MapReduce分析流程，并在指定文件夹中打印日志和脚本信息，最终得到VCF变异检测文件，并已上传到HDFS的指定位置。</p><p>对gVCF文件和VCF文件的解读在第二章的相关文件格式介绍中已经提及，其中的样本数据就是来自这一组测试数据得到的最终变异集合。</p><h2 id="5-3-测序平台的分析与优化">5.3 测序平台的分析与优化</h2><h3 id="5-3-1-测序平台与传统测序流程的比较">5.3.1 测序平台与传统测序流程的比较</h3><p>与传统的处理流程相比，对于这九组样本数据，在传统的分析流程中，需要手动编写九组测序的脚本，这个重复性的工作在FreeMarker的引入之后可以简化了。只需编写一套测序分析模板脚本，通过模版生成引擎，得到九组不同的测序分析脚本。</p><p><img src="https://i.imgur.com/LTjKPzf.png" alt="图5-1  Map阶段测序分析运行时间比对"></p><p>Map阶段测序分析运行时间比对如图5-1所示，横坐标表示的是用于测序的样本个数，纵坐标表示的是测序流程总的运行时间。在传统的测序分析是进行串行处理的方式，在处理单个样本对数据时，耗时大概12分钟左右，因此对于样本个数逐个递增的情况其总耗时时间呈线性递增；而对于分布式环境的Map任务执行，各个map任务在集群中是同时提交并行执行的，在每一种测序样本数处理情况中，以处理用时最长那一组的测序时间作为总运行时间。<br>因此由图5-1可知，随着样本数的增多，传统的测序方法的运行时间呈线性增加，而基于Hadoop的分布式测序所需要的时间基本保持不变。</p><p><img src="https://i.imgur.com/ACMv0sK.png" alt="图5-2  Reduce阶段测序分析运行时间比对"></p><p>Reduce阶段测序分析运行时间比对如图5-2所示，在Reducer规约阶段需要将上一步骤中各个样本产生的单个变异检测样本合并，而这里都是经由一个脚本程序处理，因此传统基因测序时间和分布式基因测序时间相近。</p><h3 id="5-3-2-增加测序流程处理的时间戳标记">5.3.2 增加测序流程处理的时间戳标记</h3><p>在每个分析流程执行的时候，在各个Shell命令中加入time命令，可以在日志中打印该流程所花费的时间。比如：程序执行的开始时间和程序执行的结束时间，一共花费了多少时间等，这样可以对分析流程中各个程序的执行时间和执行流程顺序更加了解。</p><h3 id="5-3-3-引入log4j日志框架">5.3.3 引入Log4j日志框架</h3><p>在整个项目中引入Log4j日志框架，将不同的日志类型的信息导向不同的文件中存储，便于项目的开发和调试。下面是我的log4j的配置文件：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Log4j建议只使用四个级别，优先级从高到低分别是ERROR、WARN、INFO、DEBUG。###</span></span></span><br><span class="line">log4j.rootLogger = stdout,D,E,I</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 输出信息到控制台 ###</span></span></span><br><span class="line">log4j.appender.stdout = org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.Target = System.out</span><br><span class="line">log4j.appender.stdout.layout = org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern = [%-5p] %d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125; method:%l%n%m%n</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 输出DEBUG 级别以上的日志到 $PROJECT_HOME/wgsv2/wgs-logs/debug.log ###</span></span></span><br><span class="line">log4j.appender.D = org.apache.log4j.DailyRollingFileAppender</span><br><span class="line">log4j.appender.D.File = ./wgs-logs/debug.log</span><br><span class="line">log4j.appender.D.Append = true</span><br><span class="line">log4j.appender.D.Threshold = DEBUG</span><br><span class="line">log4j.appender.D.layout = org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.D.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125;  [ %t:%r ] - [ %p ]  %m%n</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 输出ERROR 级别以上的日志到 $PROJECT_HOME/wgsv2/wgs-logs/error.log ###</span></span></span><br><span class="line">log4j.appender.E = org.apache.log4j.DailyRollingFileAppender</span><br><span class="line">log4j.appender.E.File =./wgs-logs/error.log</span><br><span class="line">log4j.appender.E.Append = true</span><br><span class="line">log4j.appender.E.Threshold = ERROR</span><br><span class="line">log4j.appender.E.layout = org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.E.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125;  [ %t:%r ] - [ %p ]  %m%n</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## 输出INFO 级别以上的日志到=$PROJECT_HOME/wgsv2/wgs-logs/TemplateEngine-info.log ###</span></span></span><br><span class="line">log4j.appender.I = org.apache.log4j.DailyRollingFileAppender</span><br><span class="line">log4j.appender.I.File =./wgs-logs/info.log</span><br><span class="line">log4j.appender.I.Append = true</span><br><span class="line">log4j.appender.I.Threshold = INFO</span><br><span class="line">log4j.appender.I.layout = org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.I.layout.ConversionPattern = %-d&#123;yyyy-MM-dd HH:mm:ss&#125;  [ %t:%r ] - [ %p ]  %m%n</span><br></pre></td></tr></table></figure><p>从配置文件中可以看到，日志优先级共四种，优先级从高到低分别是ERROR、WARN、INFO、DEBUG，对于不同日志等级的日志文件进行分文件存储。这样对于后期的系统调试至关重要。</p><h2 id="5-4-基于hadoop基因测序平台的扩展">5.4 基于Hadoop基因测序平台的扩展</h2><p>在对不同物种进行基于Hadoop平台的全基因组测序流程开发时，只需对WGS处理模板进行重构，适配特定的物种即可，在本章的测试样本中，我使用的是大肠杆菌的K12生物来进行测序流程构建，而将其换作是其他生物的全基因组数据完全可以。下图是测序平台的扩展性示意图，由于每个样本的测序分析流程都是封装在Map任务和Reduce任务中的，因此可以同时并行化的开展对不同物种的测序流程，Hadoop大数据测序平台扩展如图5-3所示。</p><p><img src="https://i.imgur.com/A7ytgpS.png" alt="图5-3 Hadoop大数据测序平台扩展图"></p><p>在该Hadoop基因测序平台中，对于其他物种如人类，只需将人类的全基因组测序样本上传至HDFS分布式存储系统中，将人类的参考基因组数据在集群的各个主机上进行分布，再设计人类基因测序流程的模板文件即可适配该平台。因为各个Map任务和Reduce任务是独立存在，并隔离在不同的container中执行的，因此各个子任务之间互不干扰，达到完全并行化计算处理。<br>##5.5 本章小结<br>本章主要对搭建的基于Hadoop的WGS分析流程平台的一个测试，在保证WGS全基因组测序流程的的完整性和正确性的情况下，用 E.coli K12生物的数据作为代替，测试平台的可用性和健壮性等。对传统WGS分析流程与Hadoop大数据框架整合，从测序源到产生最终的变异结果gVCF和VCF文件的过程进行测试。<br>在这次测试中，通过观察运行过程的日志信息，发现尝试分析最耗时间的步骤基本聚集在比对和变异检测这两步中，最后对于测序平台的扩展性进行论述，说明其如何适配不同物种的基因测序流程，并进行并行化计算等。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Apache. Apache MRUNIT™[EB/OL]. <a href="http://mrunit.apache.org/" target="_blank" rel="noopener">http://mrunit.apache.org/</a>, 2018. <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>The sratoolkit Toolkit [EB/OL]. <a href="https://github.com/ncbi/sra-tools/wiki/Downloads" target="_blank" rel="noopener">https://github.com/ncbi/sra-tools/wiki/Downloads</a>, 2018. <a href="#fnref2" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>4 基于Hadoop的平台搭建与MapReduce作业设计</title>
      <link href="/blog/3fd0f93f.html"/>
      <url>/blog/3fd0f93f.html</url>
      
        <content type="html"><![CDATA[<h2 id="4-1-基于hadoop的伪分布式平台搭建">4.1 基于Hadoop的伪分布式平台搭建</h2><p>在搭建Hadoop分布式系统平台<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>时，我们有两种方式可以选择，分别是伪分布的模式和完全分布式模式。这两种模式的主要区别在于前者是在本地一台机器中运行Hadoop框架中的各种服务，是一种模拟分布式的集群环境。而完全分布式的环境就是在真实的多个主机上配置Hadoop，并搭建整个集群环境。</p><a id="more"></a><h3 id="4-1-1-搭建hadoop伪分布式平台">4.1.1 搭建Hadoop伪分布式平台</h3><p>使用分布式的环境对全基因组测序的环境进行搭建，下面介绍整个分布式环境的搭建流程。<br><img src="https://i.imgur.com/kBOP38U.png" alt="图4-1 Hadoop伪分布式集群架构图"></p><p>1）安装并检查Java版本<br>必须确保Hadoop集群安装的是合适版本的Java。可以通过Hadoop wiki界面<a href="http://wiki.apache.org/hsdoop/HadoopJavaVersions" target="_blank" rel="noopener">Hadoop wiki界面</a>来查看具。通过键入以下命令，查看本机的java版本信息：</p><pre><code>elon@longsl:~$ java -versionjava version &quot;1.8.0_161&quot;Java(TM) SE Runtime Environment (build 1.8.0_161-b12)Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)</code></pre><p>2）创建Unix用户账号<br>最好创建特定的Unix用户账号以区分各Hadoop进程，及区分同一机器上的其他服务。在我的主机上，我创建一个名叫elon的用户名来执行Hadoop程序。</p><p>3）安装Hadoop<br>从<a href="http://hadoop.apache.org/common/realease.html" target="_blank" rel="noopener">Apache Hadoop发布页</a>下载一个稳定版的二进制发布版本包（通常打包为一个tar.gz结尾的文件），再解压缩到本地文件系统。在我的主机上执行以下命令即可：<br><code>elon@longsl:~$ tar -zxvf hadoop-2.7.6.tar.gz -C .</code></p><p>4）SSH配置<br>SSH免密码登录的原理是首先在主机A的本地生成公钥和私钥，然后将本机的公钥发送到要免密登录的主机上，然后在访问目的主机时，本机的私钥和目的主机中的公钥配对成功，即可以免密登录。<br>在本次伪分布式环境中，只需配置本机用户自己和自己免密登陆以及自己和localhost用户免密登陆即可。我本机的主机名为longsl，因此我需要为longsl用户生成公私密钥对:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">elon@longsl:~$ ssh-keygen -t rsa</span><br><span class="line"><span class="meta">#</span><span class="bash"> 并且配置ssh longsl和ssh localhost能免密登陆:</span></span><br><span class="line">elon@longsl:~$ ssh-copy-id longsl</span><br><span class="line">elon@longsl:~$ ssh-copy-id localhost</span><br></pre></td></tr></table></figure><p>5）配置Hadoop<br>Hadoop集群的配置文件在$HADOOP_HOME/etc/hadoop目录下，主要修改五个配置文件，分别是slaves、core-site.xml、hdfs-site.xml、mapred-site.xml和yarn-site.xml。<br>修改slaves文件，这个文件表明该集群中的子节点的主机名，在伪分布式环境中，直接将localhost添加进去即可。<br>配置core-site.xml，主要设置NameNode运行的主机信息。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://longsl:8080/<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/elon/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 配置hdfs-site.xml，需要设置的dfs.replication的参数，主要用于设置集群中副本的个数，由于这个是伪分布模式，因此副本数设置为1即可。 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置mapred-site.xml，首先设置了日志聚合相关的参数配置，我们可以理解为将集群环境中原本存储于系统tmp目录下的作业运行日志作了归档处理，并存储在了HDFS上。接下来，对集群中在各个容器中作业的运行环境做了相应的配置，具体是设置了每个作业在map阶段和reduce阶段能够被分配的最大运行内存，另外设置了其可以使用的java虚拟机的最大内存资源数。最后对map阶段和reduce阶段能够被使用的CPU核数做了相应配置。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- log aggreation --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>localhost:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/mr-history/done_intermediate<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/mr-history/done<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- configure RAM for a Container --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1024m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1024m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- configure cpu core on map and reduce --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.cpu.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置yarn-site.xml，主要需配置RM运行的主机，在伪分布式环境中就是指本地主机，另外NM节点上运行的服务是mapreduce_shuffle，这个选项是必须配置的，因为我们需要处理的就是mapreduce程序。最后开启了yarn上的日志聚合功能，并将各个容器上产生的日志存储在HDFS上。</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>longsl<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- log aggreation --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/container/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>6）环境设置<br>需要设置Hadoop系统的Java安装的位置，通过在etc/hadoop/hadoop-env.sh文件中设置JAVA_HOME项。<br><code>export JAVA_HOME=/opt/jdk1.8</code></p><p>7）格式化HDFS文件系统<br>在能够使用之前，全新的HDFS安装需要进行格式化。通过创建存储目录和初始版本的namenode持久数据结构，格式化进程将创建一个空的文件系统。格式化HDFS是一个快速操作，以hdfs用户身份运行命令：hadoop namenode -format</p><h3 id="4-1-2-启动和停止hadoop集群">4.1.2 启动和停止Hadoop集群</h3><p>Hadoop自带脚本，可以运行脚本命令，启动或者停止如hdfs、yarn或者日志聚合等服务进程。为了使用这些脚本，需要告诉Hadoop集群中有哪些机器。文件slaves用于此目的，该文件包含了机器主机名或IP地址的列表，每行代表一个机器信息。文件slaves列举了可以运行datanode和节点管理器的机器。</p><p>1）启动HDFS守护进程<br><a href="http://xn--elonstart-dfs-lm3uka20b011ffp0dkh7dc9zdt71ahjf.sh" target="_blank" rel="noopener">以elon用户身份运行命令start-dfs.sh</a>，可以启动HDFS守护进程。默认情况下，该命令从core-site.xml配置项fs.defaultFS中找到namenode的主机名。更具体一些，start-dfs.sh脚本所做的事情如下：<br>1)在每台机器上启动一个namenode。<br>2)在slaves文件列举的每台机器上启动一个datanode<br>3)在每台机器上启动一个辅助namenode。</p><p>2）启动YARN守护进程<br>YARN守护进程以相同的方式启动，通过以yarn用户身份在托管资源管理器的机器上运行命令：<a href="http://start-yarn.sh" target="_blank" rel="noopener">start-yarn.sh</a>。默认情况下，资源管理器总是和start-yarn.sh脚本运行在同一机器上。脚本明确完成以下事情。<br>1)在本地机器上启动一个资源管理器。<br>2)在slaves文件列举的每台机器上启动一个节点管理器。</p><p>同样，还提供了stop-dfs.sh和stop-yarn.sh脚本用于停止由相应的启动脚本启动的守护进程。下面是在我的集群环境中开启集群环境的实例：</p><pre><code>elon@longsl:~$ start-dfs.sh &amp;&amp; start-yarn.shStarting namenodes on [longsl]longsl: starting namenode, logging to /home/elon/hadoop/logs/hadoop-elon-namenode-longsl.outlocalhost: starting datanode, logging to /home/elon/hadoop/logs/hadoop-elon-datanode-longsl.outStarting secondary namenodes [0.0.0.0]0.0.0.0: starting secondarynamenode, logging to /home/elon/hadoop/logs/hadoop-elon-secondarynamenode-longsl.outstarting yarn daemonsstarting resourcemanager, logging to /home/elon/hadoop/logs/yarn-elon-resourcemanager-longsl.outlocalhost: starting nodemanager, logging to /home/elon/hadoop/logs/yarn-elon-nodemanager-longsl.out</code></pre><h2 id="4-2-伪分布式环境的mapreduce作业构建">4.2 伪分布式环境的MapReduce作业构建</h2><p><img src="https://i.imgur.com/NvL3uNw.png" alt="图4-2 伪分布式的MapReduce任务流程"></p><p>伪分布式的MapReduce任务执行图如图4-2所示，MapReduce框架中共分为Mapper阶段和Reducer阶段，本节会主要研究Mapper和Reducer的流程构造。在伪分布式环境中，Map任务和Reduce任务是串行执行下去的，其中先执行完所有Map任务再执行Reduce任务，另外Map任务和Reduce任务都会在本地执行器中执行。</p><h3 id="4-2-1-mapper流程构造">4.2.1 Mapper流程构造</h3><p>在Mapper阶段中，重点关注单样本的基因测序流程，从而得到变异检测的中间gVCF文件，基因测序Mapper阶段的流程如图4-3所示。</p><p><img src="https://i.imgur.com/BRr8fqM.png" alt="图4-3 基因测序Mapper阶段的流程图"></p><p>在上图中，主要是对Mapper阶段从输入sample.txt文本文件开始，一直到输出端得到output输出文件并把得到的gVCF文件上传到HDFS上的展示。其中在输入的文本文件中保存的是本次测序中样本组中各样本的名称信息，作为测序对象传入模版文件中，在模版引擎的调用下生成测序可执行脚本，然后在Shell脚本执行引擎中调用该脚本，以脚本命令的模式执行基因测序分析流程。<br>在脚本执行过程中，需要访问HDFS系统，获取该测序样本的数据文件，最后生成gVCF文件并上传至HDFS上存储。另外对于Mapper的输入端的输出文本，设计是将key设置为同一个，这样在reduce程序调用时，可以在同一个reduce程序中读取到，这样样本就可以在同一个测序分析流程中被调用，形成最终的VCF文件。</p><h3 id="4-2-2-reducer流程构造">4.2.2 Reducer流程构造</h3><p>在Reduer阶段，主要侧重的是基因测序的单样本变异检测结果的合并，得到最终的VCF文件的过程，基因测序Reduce阶段流程图如图4-4所示。</p><p>在该阶段的分析流程中，主要是将上一步Mapper产生的输出文件作为此阶段的输入文件，而reduce程序对于键相同的元素和经过shuffle混洗之后在同一个reduce对象中处理，这样就可以将这一组样本生成的gVCF文件统一传递到reduce程序的shell分析流程脚本中处理，再调用FreeMarker模板引擎来生成合并gVCF文件的脚本文件，最后调用shell脚本执行引擎来调用该脚本文件，得到该组样本统一的变异检测文件。</p><p><img src="https://i.imgur.com/Oa91nGQ.png" alt="图4-4 基因测序Reduce阶段流程图"></p><p>同样，在最后需要将得到的VCF文件上传至HDFS上存储并同意管理，这样就完成了基因测序的完整流程，从单个样本的测序分析得到变异位点信息，到最后合并各个样本的变异位点信息得到最终能反映该生物物种的全基因组变异位点信息集合。<br>##4.3 基于Hadoop分布式环境搭建<br>在对Hadoop平台的分布式环境的搭建过程，其中的Java环境配置、用户创建、Hadoop二进制包的安装等步骤都和伪分布式的搭建方式一致，主要的不同之处在于其对Hadoop配置文件的配置不同。<br>###4.3.1 Hadoop分布式架构<br>在我的Hadoop分布式环境配置中，我利用了三台虚拟机来模拟真实主机，三台主机的主机名分别是node1、node2和node3，其中一台作为Master主机，另外两台作为slaves从节点。<br>Hadoop完全分布式架构图如图4-5所示，在该图中，主要通过配置集群中各个机器的Java环境变量和SSH免密登录，并在各个机器上配置Hadoop可执行包和修改Hadoop配置文件，搭建Hadoop分布式集群环境。</p><p><img src="https://i.imgur.com/aLtO039.png" alt="图4-5 Hadoop完全分布式架构图"></p><h3 id="4-3-2-hadoop完全分布式配置">4.3.2 Hadoop完全分布式配置</h3><p>首先在node1上进行下面各项的配置，最后再将各个配置项同步复制到node2和node3节点上。</p><pre><code>scp /home/node1/hadoop/etc/hadoop/ root@node2:/home/node2/hadoop/etc/Hadoopscp /home/node1/hadoop/etc/hadoop/ root@node3:/home/node2/hadoop/etc/hadoop</code></pre><p>(1) 配置slaves文件<br>$HADOOP_HOME/etc/Hadoop/slaves文件是用来配置Hadoop集群中的从节点的主机，将集群中所有的从节点都在slaves中进行记录，通常这些从节点就是承担计算任务的节点。在我的完全分布式环境中，我配置了两个从节点，一个主节点。因此需将从节点的主机名node2和node3写入slaves文件中。进行如下配置：</p><pre><code>node2node3</code></pre><p>(2) 配置core-site.xml文件<br>在对core-site.xml文件进行配置时，指定node1为Master主节点，因此NameNode服务就指定在node1主机上。进行如下配置：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node1:8080<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>(3) 配置hdfs-site.xml文件<br>在对hdfs-site.xml文件进行配置时，主要和伪分布式不同的地方在于可以指定多个副本数了，其副本数的多少代表这个文件系统的可靠程度，副本数越高越可靠，但为了兼顾存储资源和高可靠性也不能设置太高，官方默认的副本数是3。另外我们可以对namenode和datanode的存储地址进行自定义，最后指定NameNode的辅助进程的运行主机。具体配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hadoop/tmp/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/hadoop/tmp/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:8081<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>(4) 配置mapred-site.xml文件<br>在mapred-site.xml文件中，主要是与MapReduce任务的相关配置。其中要配置MapReduce使用的资源调度框架，这里默认是yarn。另外还要对map任务和reduce任务执行时的可用资源和java虚拟机的资源进行配置。具体配置如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>1600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1300m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>-Xmx1300m<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>(5) 配置yarn-site.xml文件<br>在yarn-site.xml文件中，可以对resourcemanager运行的主机进行指定，这里我将其放在Master节点node1上运行，用于对集群中的作业进行任务调度和资源分配。由于我所创建的三个虚拟机上在我的单个真实主机上的，因此可用资源是非常紧张的，这要求我不同直接使用默认配置，而必须对yarn上的资源进行更为细致化的重新分配。</p><p>首先需要用户配置每个节点上可用的物理内存资源和CPU核数，因为一个node计算节点除了跑相关作业外，还可能运行着其他应用和服务等。下一步需要对yarn中可调度的单个容器资源进行配置，如配置调度的最小分配内存和最大分配内存，其中最小分配内存决定着该节点上最多可部署的容器个数，可通过yarn可用的物理内存除以每个容器最小可分配的内存来得到容器个数，同理CPU核数也是这样来指定。</p><p>接着还需要定义每个Map和Reduce任务需要的最大内存量。由于每个Map和每个Reduce都将在单独的Container中运行，因此这些最大内存设置应至少等于或大于YARN最小Container容量分配。最后，每个Map和Reduce任务的虚拟内存（物理+分页内存）上限由每个允许YARN容器的虚拟内存比决定，默认值为2.1。具体的配置细节如下：</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2500<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>2000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2500<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.increment-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.increment-allocation-vcores<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.resource.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>300<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>4<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>(6) SSH免密登陆配置<br>在配置SSH免密登陆时，主要对针对Master能免密登陆到各个slave节点上，保证NameNode和ResourceManager服务能在集群的各个节点上进行便携访问。在我的分布式环境中，由于NN和RM都是运行在主节点node1上，因此我需要配置node1到node2和node3的免密登陆。具体操作：在node1上使用命令“ssh-keygen –t rsa”生成node1的密钥对，并将node1的公钥分别发送到node2和node3的~/.ssh/authorized文件中，即可实现SSH免密登陆。</p><h3 id="4-3-3-启动和停止hadoop集群">4.3.3 启动和停止Hadoop集群</h3><p>完全分布式和伪分布式一样，<a href="http://xn--start-dfs-zo31ac0a.sh" target="_blank" rel="noopener">通过start-dfs.sh</a>、<a href="http://start-yarn.xn--shstop-dfs-ih7q.sh" target="_blank" rel="noopener">start-yarn.sh和stop-dfs.sh</a>、stop-yarn.sh脚本，启动或者停止如hdfs、yarn或者日志聚合等服务进程。下面是在我的完全分布式集群中的启动实例：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">root@node1:~# start-dfs.sh &amp;&amp; start-yarn.sh</span><br><span class="line">Starting namenodes on [node1]</span><br><span class="line">node1: starting namenode, logging to /root/hadoop/logs/hadoop-root-namenode-node1.out</span><br><span class="line">node2: starting datanode, logging to /root/hadoop/logs/hadoop-root-datanode-node2.out</span><br><span class="line">node3: starting datanode, logging to /root/hadoop/logs/hadoop-root-datanode-node3.out</span><br><span class="line">node1: starting datanode, logging to /root/hadoop/logs/hadoop-root-datanode-node1.out</span><br><span class="line">Starting secondary namenodes [node1]</span><br><span class="line">node1: starting secondarynamenode, logging to /root/hadoop/logs/hadoop-root-secondarynamenode-node1.out</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /root/hadoop/logs/yarn-root-resourcemanager-node1.out</span><br><span class="line">node3: starting nodemanager, logging to /root/hadoop/logs/yarn-root-nodemanager-node3.out</span><br><span class="line">node2: starting nodemanager, logging to /root/hadoop/logs/yarn-root-nodemanager-node2.out</span><br><span class="line">node1: starting nodemanager, logging to /root/hadoop/logs/yarn-root-nodemanager-node1.out</span><br></pre></td></tr></table></figure><p>从上面打印的信息可以看出，NameNode是运行在node1主节点上的，在集群中的三个节点都作为计算节点运行这DataNode进程和NodeManager进程。</p><h2 id="4-4-分布式环境下mapreduce作业构建">4.4 分布式环境下MapReduce作业构建</h2><p><img src="https://i.imgur.com/SdyGul2.png" alt="图4-6 分布式环境下的MapReduce的流程"></p><p>在分布式环境中，与伪分布式不同的地方在于，多个slave节点可以提供多个container容器供MapReduce任务来运行，分布式环境下的MapReduce的流程构造如图4-6所示。</p><p>从图中可以看出，在完全分布式环境中总共有多个计算节点，其中包含多个container容器，而我们提交的Map作业和Reduce作业就是在多个容器中分别进行运算，最后再将Map作业运行的结果通过一个Reduce作业进行规约计算。整个MapReduce运行流程，由原来的伪分布式中串行运算Map作业和Reduce作业，到现在分布式环境中并行运算Map作业，最后将Map的中间结果用一个Reduce作业来处理。</p><h2 id="4-5-shell脚本执行引擎的构建">4.5 Shell脚本执行引擎的构建</h2><p>在得到最终的执行脚本后，还需要通过特定的方法来调用执行该脚本文件。通过编写ShellScriptUtil工具类，执行指定的脚本并打印执行过程中出现的日志，并存放在指定的路径下，下面是shell脚本执行工具类的代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * shell脚本工具包</span></span><br><span class="line"><span class="comment">* 用于将一个指定的可执行脚本进行执行的Java程序</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShellScriptUtil</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 日志记录</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger theLogger = Logger.getLogger(TemplateEngine.class.getName());</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 调用Shell脚本执行的方法</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> paths 指定多个路径参数</span></span><br><span class="line"><span class="comment">     *              第一个指定的是shell模版</span></span><br><span class="line"><span class="comment">     *              第二个参数指定的是脚本执行结果存放路径</span></span><br><span class="line"><span class="comment">     *              第三个参数指定的是执行脚本中日志存放路径，是可选参数，未给出此参数则默认为无日志输出</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">callProcess</span><span class="params">(String... paths)</span> </span>&#123;</span><br><span class="line">        File outputFile;</span><br><span class="line">        File logFile;</span><br><span class="line">        Process process;</span><br><span class="line">        String scriptPath = paths[<span class="number">0</span>];</span><br><span class="line">        String chmod = <span class="string">"chmod u+x "</span> + scriptPath;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 为shell脚本增加可执行权限</span></span><br><span class="line">            Runtime.getRuntime().exec(chmod).waitFor();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">"执行脚本："</span> + scriptPath);</span><br><span class="line">        ProcessBuilder pb = <span class="keyword">new</span> ProcessBuilder(<span class="string">"./"</span> + scriptPath);</span><br><span class="line">        pb.inheritIO();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 指定shell脚本执行的结果输出路径和执行时日志文件的输出路径</span></span><br><span class="line">        <span class="keyword">if</span> (paths.length == <span class="number">3</span>) &#123;</span><br><span class="line">            outputFile = <span class="keyword">new</span> File(paths[<span class="number">1</span>]);</span><br><span class="line">            pb.redirectOutput(outputFile);</span><br><span class="line">            logFile = <span class="keyword">new</span> File(paths[<span class="number">2</span>]);</span><br><span class="line">            pb.redirectError(logFile);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 指定shell脚本执行的日志输出路径</span></span><br><span class="line">        <span class="keyword">if</span> (paths.length == <span class="number">2</span>) &#123;</span><br><span class="line">            logFile = <span class="keyword">new</span> File(paths[<span class="number">1</span>]);</span><br><span class="line">            <span class="keyword">if</span> (logFile.exists()) &#123;</span><br><span class="line">                logFile.delete();</span><br><span class="line">            &#125;</span><br><span class="line">            pb.redirectError(logFile);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            process = pb.start();</span><br><span class="line">            process.waitFor();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            theLogger.error(<span class="string">"发生I/O错误..."</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">            theLogger.error(<span class="string">"当前线程在等待时被另一个线程中断..."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-6-mapreduce作业编写与整体调度">4.6 MapReduce作业编写与整体调度</h2><p>在MapReduce计算框架中，通过Mapper和Reducer来整体调度整个WGS分析流程，这里需要自定义map()函数和reduce()函数。其中map()函数是用来传入要分析的样本名称，并通过样本名称对不同的样本分多个map任务来并行执行，从而调用模板类，并执行最终的WGS分析脚本。下面是wgsMapper类的代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    编写全基因组测序的Mapper类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">wgsMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String LOG_DIRECTORY = <span class="string">"./wgs-logs"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String SCRIPT_DIRECTORY = <span class="string">"./wgs-scripts"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String sample_name = value.toString();</span><br><span class="line">        wgs(sample_name);</span><br><span class="line">        context.write(<span class="keyword">new</span> Text(<span class="string">"1"</span>),<span class="keyword">new</span> Text(sample_name));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">wgs</span><span class="params">(String sampleName)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; templateMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        templateMap.put(<span class="string">"sample_name"</span>, sampleName);</span><br><span class="line"></span><br><span class="line">        String template = <span class="string">"wgsMapper.template"</span>;</span><br><span class="line">        String scriptPath = SCRIPT_DIRECTORY + <span class="string">"/wgs_mapper_"</span> + templateMap.get(<span class="string">"sample_name"</span>) + <span class="string">".sh"</span>;</span><br><span class="line">        String logPath = LOG_DIRECTORY + <span class="string">"/wgs_mapper_"</span> + templateMap.get(<span class="string">"sample_name"</span>) + <span class="string">".log"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从模板创建具体脚本</span></span><br><span class="line">        File scriptFile = TemplateEngine.createDynamicContentAsFile(template, templateMap, scriptPath);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (scriptFile != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ShellScriptUtil.callProcess(scriptPath, logPath);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而在reduce()函数中，通过传递的样本名称到Reducer脚本模版中，对指定的gvcf文件作合并处理，从而得到最终的VCF文件。下面是wgsReducer类的代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    编写全基因组测序的Reducer类</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">wgsReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>, <span class="title">Text</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String LOG_DIRECTORY = <span class="string">"./wgs-logs"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> String SCRIPT_DIRECTORY = <span class="string">"./wgs-scripts"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;Text&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;String, String&gt; templateMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        Iterator&lt;Text&gt; value = values.iterator();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">        String str = <span class="string">""</span>;</span><br><span class="line">        <span class="keyword">while</span> (value.hasNext()) &#123;</span><br><span class="line">            str = value.next().toString();</span><br><span class="line">            templateMap.put(<span class="string">"sample_name"</span> + (++count), str);</span><br><span class="line">        &#125;</span><br><span class="line">        mergeGVCF(templateMap);</span><br><span class="line">        context.write(<span class="keyword">new</span> Text(<span class="string">"1"</span>), <span class="keyword">new</span> Text(str));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">mergeGVCF</span><span class="params">(HashMap&lt;String, String&gt; templateMap)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        String template = <span class="string">"wgsReducer.template"</span>;</span><br><span class="line">        String scriptPath = SCRIPT_DIRECTORY + <span class="string">"/wgs_reducer_"</span> + <span class="string">".sh"</span>;</span><br><span class="line">        String logPath = LOG_DIRECTORY + <span class="string">"/wgs_reducer_"</span> + <span class="string">".log"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 从模板创建具体脚本</span></span><br><span class="line">        File scriptFile = TemplateEngine.createDynamicContentAsFile(template, templateMap, scriptPath);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (scriptFile != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ShellScriptUtil.callProcess(scriptPath, logPath);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="4-7-本章小结">4.7 本章小结</h2><p>在本章中，主要介绍了Hadoop伪分布式环境及分布式环境进行搭建，还对WGS分析流程与MapReduce计算框架如何结合起来，其中就涉及到如何分多个map任务并行执行分析流程，对于WGS测序过程在linux环境中运行的情况，引入FreeMarker第三方库，通过编写脚本模板，并结合map和reduce作业，使得不同的map或者reduce任务在执行过程中，以不同的样本数据来执行不同的WGS脚本，达到高效并行化的全基因组测序的目的，最后再通过调用Driver驱动类执行整个MapReduce程序。</p><p>下一章主要对构建的测序平台做一个多样本数据的测试，并进行优化和系统扩展。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Tom White. Hadoop权威指南[M]. 王海,华东,刘喻,吕粤海译. 清华大学出版社 第四版 2017. <a href="#fnref1" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>3 基于Hadoop基因测序数据处理关键技术的研究</title>
      <link href="/blog/34f6c0b5.html"/>
      <url>/blog/34f6c0b5.html</url>
      
        <content type="html"><![CDATA[<h2 id="3-1-测序处理流程与mapreduce结合">3.1 测序处理流程与MapReduce结合</h2><p>基因测序处理流程中有gVCF和VCF文件生成两个阶段，而MapReduce恰好是一种分阶段处理的编程模型，它拥有Mapper和Reducer两个处理阶段，因此将MapReduce框架和测序流程结合在一起，既能保证测序流程的有序进行，又能在MapReduce框架中进行并行调度，提升测序数据处理的效率。</p><h3 id="3-1-1-测序流程与mapreduce结合的可行性分析">3.1.1 测序流程与MapReduce结合的可行性分析</h3><p>MapReduce是一种可用于数据处理的编程模型，可以将数据分析处理流程分为映射阶段和归约阶段<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>。在映射阶段，各个Map程序在不同的Container中运行彼此互不干扰，即各个Mapper阶段的程序本质上是并行运行的；而到了Reducer规约阶段，将各个Mapper阶段产生的中间结果做规约整合得到最终的处理结果。因此可以将大规模的数据分析任务分发给任何一个拥有足够多机器的数据中心，也正是因为其强劲的数据分析和并行处理能力，所以选择其和测序处理流程作整合。</p><a id="more"></a><p>在原本传统的基因测序流程中，是将多个样本放在一台高性能服务器中，通过编写的测序分析脚本串行处理每一个样本数据，最终得到变异检测的结果。虽然这个过程能得到最终的结果，但需要配置高性能服务器，而且往往需要耗费大量的处理时间，这其中大部分时间花在测序比对上面。后期要想提升测序效率的话，需要进一步配置性能更高的服务器，这样一个高时间花费和昂贵开销，不利于基因测序的低成本普及和个性化医疗的推广应用。</p><p>因此，我们需要一种高效并行的处理平台，达到使原本串行处理多个样本的测序过程，能够以一个样本为测序处理单位细分为多个Map任务达到并行处理分析，再通过Reduce任务的到最终的结果。</p><h3 id="3-1-2-测序流程与mapreduce框架的结合">3.1.2 测序流程与MapReduce框架的结合</h3><p>要想将测序处理流程与MapReduce框架结合，首先要将对多个样本的测序切分为多个Map任务，在运行MapReduce任务的时候，将基因测序的各个流程封装在shell脚本中，而这个shell脚本是以脚本模版的形式呈现。另一方面，主要的测序分析流程是固定的，而只有分析样本对象的信息是动态传入的。这样可以通过编写一个特定的Map程序，再通过调用MapReduce驱动程序来运行作业，对于传入的不同样本名生成不同的Map任务，即各个基因测序脚本，达到基因测序的并行化处理。</p><p>在得到以样本为单位的单变异检测的文件后，通过一个Reduce任务合并所有的gVCF变异文件，从而得到最总对于整个样本数据的变异检测VCF文件。</p><p>一个集群中有三台机器，总共有九个样本数据，那么可以并行三次可以分析完所有的数据，但是若是通过传统的流程分析过程的话，需要串行分析九次才能完成整体的测序处理流程。因此，通过流程处理逻辑的MapReduce化，可以成倍的提升样本测序的处理效率，使基因测序更加高效。</p><h2 id="3-2-不同格式数据的访问与存储">3.2 不同格式数据的访问与存储</h2><h3 id="3-2-1-数据访问于存储面临的问题">3.2.1 数据访问于存储面临的问题</h3><p>在进行MapReduce处理程序时，通过shell脚本的方式处理数据，由于不同样本的处理流程需要拉取不同的数据源，并且输出的中间结果也是不同的。这个时候，我们需要一个统一的数据存储和访问规则来统一管理这些数据源以及中间结果等。<br>###3.2.2 数据的存储与访问分析<br>首先，对于参考序列这一部分数据源，因为其涉及的处理流程比较固定，可以事先在机器的本地环境先搭建好。而MapReuce程序是在集群中的不同机器上运行的，因此数据是在MapReduce程序中动态获取的，我们需要事先上传到HDFS上，这也是MapReduce计算框架常用来获取数据源的方式，其他方式还包括通过访问Amason S3等。</p><p>将所有的样本数据存储在HDFS上之后，通过在Map程序的输入数据中指定样本名称和存储的位置信息，可以轻松访问到指定的样本数据，从而实现不同的Map程序访问并处理不同的样本数据。而对于中间其他剩余几个测序流程，如排序、标记重复、创建索引以及变异检测这几步，上一步的结果是下一步处理流程的输入数据，这些处理流程所得到的中间数据都可以本地化存储，既能节省网络传输的开销，也能最大程度保证数据的完整性，不会由于网络中断导致数据不完整存储。</p><p>在所有的样本并行化处理完成之后，每个Map程序最终得到的是该样本数据的gVCF变异检测文件，对每个样本变异检测数据作一个合并得到最终VCF格式文件，这时需要用到Reduce归约操作，因此需要对Map程序最终得到的结果再上传到HDFS上，供接下来的Reduce程序使用。<br>而最终得到的VCF文件，也是需要上传到HDFS上存储，保证结果文件的容错性和完整性。</p><h2 id="3-3-分析流程的完整性">3.3 分析流程的完整性</h2><h3 id="3-3-1-分析流程面临的问题">3.3.1 分析流程面临的问题</h3><p>WGS分析处理流程中主要的问题是如何判定并以一个样本数据作为切分标准，对整体的多样本数据以样本为单位进行切分处理。因为只有以单样本为数据测序分析的单位，才能既保证数据处理流程完整性的同时，又能最大程度上保证并行化处理，从而提升整体的测序效率。</p><p>理论上对于不同样本的分析流程其实都是一样的，因此需要引入一套流程分析机制，保证分析流程的统一性，便于分析流程的统一操作和中间结果管理。</p><h3 id="3-3-2-整体流程设计分析">3.3.2 整体流程设计分析</h3><p>在MapReduce框架中，其Mapper阶段和Reducer阶段的输入文件都是文本格式的文件，在我的整体流程搭建中，将测序的样本文件存于HDFS上，map和reduce的输入文件只是保存样本文件的列表，达到能够将不同的样本处理流程进行隔离和分批处理的目的。</p><p>具体是在Map任务的输入数据文本中，每一行指定一对R1和R2样本数据名称，在驱动程序中设置输入文件中的每一行数据用一个Mapper来处理，这样能够将每一个样本数据处理分到不同的Map任务中执行，从而达到流程设计的完整性以及流程处理的并行化。</p><p>考虑到基因测序的分析流程，像完成比对、重复标记和变异检测的大多数开源工具（如BWA、SAMtools和GATK）都提供了Linux命令行界面，所有各个MapReduce阶段中的map()和Reduce()函数将调用Linux Shell脚本，并提供适当的参数。要执行这些shell脚本，将使用FreeMarker模版语言。</p><h2 id="3-4-freemarker引擎与测序流程的模版化">3.4 FreeMarker引擎与测序流程的模版化</h2><p>Apache FreeMarker<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>是一种模板引擎，目前是Apache基金会的一个孵化项目，模板是用FreeMarker模板语言（FTL）编写的，它是一种简单的专用语言。通常，使用通用编程语言（如Java）来准备数据（发布数据库查询，进行业务计算）。然后，FreeMarker使用模板显示准备好的数据。</p><p>在我们基因测序流程搭建中，先编写shell分析流程的脚本模板，其中分析对象用参数代替，再通过FreeMarker API将参数传入分析流程模版中，得到最终可执行的shell脚本文件。FreeMarker模版引擎如图3-1所示。</p><p><img src="https://i.imgur.com/p3Ki9jT.png" alt="图3-1 FreeMarker模版引擎"></p><h3 id="3-4-1-构建freemarker模版引擎">3.4.1 构建FreeMarker模版引擎</h3><p>在我们最终的处理流程中，通过FreeMarker模版引擎类把脚本模版转换为可执行的shell脚本后，最终通过调用shell脚本工具类执行该脚本<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>。在本节中，会分别对这两个工具类作出说明。</p><p>其中模版引擎类需要引入org.freemarker包来调用FreeMarker API方法，其中最主要的方法是createDynamicContentAsFile，它需要传入一个模版文件、一个存储着键值对的map对象和最终生成的shell脚本文件路径，通过map传入的对象信息映射到模版中，生成最终的WGS测序流程的分析脚本。下面是具体的模版引擎类方法：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 这个类使用FreeMarker(http://freemarker.apache.org) FreeMarker是一个模板引擎，这是一个根据模板生成文本输出的通用工具</span></span><br><span class="line"><span class="comment"> * (从shell脚本到自动生成的源代码都是文本输出)</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TemplateEngine</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 日志记录</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Logger theLogger = Logger.getLogger(TemplateEngine.class.getName());</span><br><span class="line">    <span class="comment">// 通常在整个应用生命周期中只执行一次</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Configuration TEMPLATE_CONFIGURATION = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> AtomicBoolean initialized = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">false</span>);</span><br><span class="line">    <span class="comment">// freemarker templates 存放目录</span></span><br><span class="line">    <span class="keyword">protected</span> <span class="keyword">static</span> String TEMPLATE_DIRECTORY = <span class="string">"./wgs-templates"</span>;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (initialized.get()) &#123;</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        initConfiguration();</span><br><span class="line">        initialized.compareAndSet(<span class="keyword">false</span>, <span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (!initialized.get()) &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                init();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">                theLogger.error(<span class="string">"在初始化阶段，模版引擎初始化错误..."</span>, e);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 初始化配置</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">initConfiguration</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        TEMPLATE_CONFIGURATION = <span class="keyword">new</span> Configuration();</span><br><span class="line">        TEMPLATE_CONFIGURATION.setDirectoryForTemplateLoading(<span class="keyword">new</span> File(TEMPLATE_DIRECTORY));</span><br><span class="line">        TEMPLATE_CONFIGURATION.setObjectWrapper(<span class="keyword">new</span> DefaultObjectWrapper());</span><br><span class="line">        TEMPLATE_CONFIGURATION.setWhitespaceStripping(<span class="keyword">true</span>);</span><br><span class="line">        TEMPLATE_CONFIGURATION.setClassicCompatible(<span class="keyword">true</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 通过模板和keyValuePairs动态创建shell脚本</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> templateFileName 一个模板文件名，如：script.sh.template，其模板目录已在configuration中指定</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyValuePairs 存储数据模型的&lt;K,V&gt;Map</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> outputScriptFilePath 生成的脚本文件路径</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> 一个可执行的Shell脚本文件</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> File <span class="title">createDynamicContentAsFile</span><span class="params">(String templateFileName,</span></span></span><br><span class="line"><span class="function"><span class="params">        Map&lt;String, String&gt; keyValuePairs, String outputScriptFilePath)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> ((templateFileName == <span class="keyword">null</span>) || (templateFileName.length() == <span class="number">0</span>)) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        Writer writer = <span class="keyword">null</span>;</span><br><span class="line">        File outputFile = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Template template = TEMPLATE_CONFIGURATION.getTemplate(templateFileName);</span><br><span class="line">            <span class="comment">// 合并数据模型和模板，生成shell脚本</span></span><br><span class="line">            outputFile = <span class="keyword">new</span> File(outputScriptFilePath);</span><br><span class="line">            writer = <span class="keyword">new</span> BufferedWriter(<span class="keyword">new</span> FileWriter(outputFile));</span><br><span class="line">            template.process(keyValuePairs, writer);</span><br><span class="line">            writer.flush();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            theLogger.error(<span class="string">"创建文件失败..."</span>, e);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (TemplateException e) &#123;</span><br><span class="line">            theLogger.error(<span class="string">"freeMarker动态创建shell脚本失败..."</span>, e);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (writer != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    writer.close();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                    theLogger.error(<span class="string">"创建shell脚本，写入文件时出现IO异常..."</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> outputFile;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3-4-2-基因测序分析流程模版设计">3.4.2 基因测序分析流程模版设计</h3><p>用模版化的思想来构建全基因组分析流程，对要分析的样本对象用参数来代替，将整个分析流程分为Mapper阶段和Reduce阶段两个模版，在Mapper阶段模版主要完成基因测序的样本下载到本地机器、以及测序阶段中的比对、排序、标记重复、创建比对索引文件和对单个样本的变异检测生成中间阶段的变异检测gVCF文件，最后，将每个脚本生成的gVCF文件上传到上传到HDFS上，供Reduce阶段的分析流程使用。下面是Mapper阶段的分析模版：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 脚本所需的环境变量定义</span></span><br><span class="line">export BWA=~/biosoft/bwa/0.7.12</span><br><span class="line">export SAMTOOLS=~/biosoft/samtools/1.0/bin</span><br><span class="line">export GATK=~/biosoft/gatk/3.6/GenomeAnalysisTK.jar</span><br><span class="line">export PICARD=~/biosoft/picard/2.18.2/picard.jar</span><br><span class="line">export HADOOP=~/hadoop/bin</span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义参考序列</span></span><br><span class="line">export REF=~/wgs/input/fasta/E.coli_K12_MG1655.fa</span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义单个节点可利用的线程数</span></span><br><span class="line">THREAD=4</span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义单个节点可利用的内存资源</span></span><br><span class="line">MEMORY=4G</span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义RG INFO</span></span><br><span class="line">INFO_RG='@RG\tID:$&#123;sample_name&#125;\tPL:illumina\tSM:E.coli_K12_$&#123;sample_name&#125;'</span><br><span class="line"></span><br><span class="line">echo "############" `date "+%Y-%m-%d %H:%M:%S"` "############"</span><br><span class="line">echo "--- 开始处理样本 $&#123;sample_name&#125; ---"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义模板变量</span></span><br><span class="line">INPUT_FILE_R1=$&#123;sample_name&#125;_1.fastq.gz</span><br><span class="line">INPUT_FILE_R2=$&#123;sample_name&#125;_2.fastq.gz</span><br><span class="line"></span><br><span class="line">cd /tmp</span><br><span class="line"><span class="meta">#</span><span class="bash"> 从HDFS上下载测序样本Block块</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "$HADOOP/hadoop fs -get /wgsv2/input/fastq/$INPUT_FILE_R1" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time $HADOOP/hadoop fs -get /wgsv2/input/fastq/$INPUT_FILE_R1 &amp;&amp; echo "" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "$HADOOP/hadoop fs -get /wgsv2/input/fastq/$INPUT_FILE_R2" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time $HADOOP/hadoop fs -get /wgsv2/input/fastq/$INPUT_FILE_R2</span><br><span class="line">echo "*** 从HDFS上下载样本数据块" &amp;&amp; echo "" &gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ××××××  基因测序 ×××××××</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 步骤一 比对</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "$BWA/bwa mem -t $THREAD -R $INFO_RG $REF $INPUT_FILE_R1 $INPUT_FILE_R2 | $SAMTOOLS/samtools view -Sb - &gt; $&#123;sample_name&#125;.bam" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time $BWA/bwa mem -t $THREAD -R $INFO_RG $REF $INPUT_FILE_R1 $INPUT_FILE_R2 | $SAMTOOLS/samtools view -Sb - &gt; $&#123;sample_name&#125;.bam &amp;&amp; echo "*** 基因比对操作完成" &amp;&amp; echo "" &gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 步骤二 排序</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "$SAMTOOLS/samtools sort -@ $THREAD -m $MEMORY -O bam -o $&#123;sample_name&#125;.sorted.bam $&#123;sample_name&#125;.bam -T PREFIX.bam" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time $SAMTOOLS/samtools sort -@ $THREAD -m $MEMORY -O bam -o $&#123;sample_name&#125;.sorted.bam $&#123;sample_name&#125;.bam -T PREFIX.bam &amp;&amp; echo "*** 基因数据排序完成" &amp;&amp; echo "" &gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">rm -f <span class="variable">$&#123;sample_name&#125;</span>.bam</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 步骤三 标记重复</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "java -jar $PICARD MarkDuplicates I=$&#123;sample_name&#125;.sorted.bam O=$&#123;sample_name&#125;.sorted.markdup.bam M=$&#123;sample_name&#125;.sorted.markdup_metrics.txt" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time java -jar $PICARD MarkDuplicates I=$&#123;sample_name&#125;.sorted.bam O=$&#123;sample_name&#125;.sorted.markdup.bam M=$&#123;sample_name&#125;.sorted.markdup_metrics.txt 1&gt;&amp;2 &amp;&amp; echo "*** 对BAM文件进行重复标记" &amp;&amp; echo "" &gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 步骤四 创建比对索引文件</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "time $SAMTOOLS/samtools index $&#123;sample_name&#125;.sorted.markdup.bam" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time $SAMTOOLS/samtools index $&#123;sample_name&#125;.sorted.markdup.bam &amp;&amp; echo "*** 对BAM文件创建比对索引" &amp;&amp; echo "" &gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 步骤五 变异检测 | 生成各个样本的中间变异检测文件gvcf</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "java -jar $GATK -T HaplotypeCaller -R $REF --emitRefConfidence GVCF -I $&#123;sample_name&#125;.sorted.markdup.bam -o $&#123;sample_name&#125;.g.vcf" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time java -jar $GATK -T HaplotypeCaller -R $REF --emitRefConfidence GVCF -I $&#123;sample_name&#125;.sorted.markdup.bam -o $&#123;sample_name&#125;.g.vcf 1&gt;&amp;2 &amp;&amp; echo "*** 生成样本$&#123;sample_name&#125;的中间变异检测文件gvcf" &amp;&amp; echo "" &gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 将各个样本分区的变异检测gvcf文件上传到HDFS上</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "$HADOOP/hadoop fs -put -f $&#123;sample_name&#125;.g.vcf /wgsv2/output/gvcf" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time $HADOOP/hadoop fs -put -f $&#123;sample_name&#125;.g.vcf /wgsv2/output/gvcf &amp;&amp; echo "*** 变异检测结果gvcf文件上传到HDFS上" &amp;&amp; echo ""&gt;&amp;2</span><br><span class="line"></span><br><span class="line">rm -f rm -f $INPUT_FILE_R1 $INPUT_FILE_R2 $&#123;sample_name&#125;*</span><br><span class="line"></span><br><span class="line">echo "--- 结束处理样本 $&#123;sample_name&#125; ---"</span><br><span class="line">echo "############" `date "+%Y-%m-%d %H:%M:%S"` "############" &amp;&amp; echo ""</span><br></pre></td></tr></table></figure><p>对于Reduce阶段的分析模版，主要是对各个Mapper阶段产生的gVCF变异文件的合并，得到最终总的VCF文件，并将其创建tabix索引后上传至HDFS上存储。下面是具体的Reduce阶段分析模版：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Author: elon</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Description: 合并所有样本的GVCF文件的模板</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Time: 2018-4-5</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 该脚本需要完成的是对各个样本分区的VCGF文件做一个merge操作</span></span><br><span class="line">export SAMTOOLS=~/biosoft/samtools/1.0/bin</span><br><span class="line">export GATK=~/biosoft/gatk/3.6/GenomeAnalysisTK.jar</span><br><span class="line">export HADOOP=~/hadoop/bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义参考序列</span></span><br><span class="line">export REF=~/wgs/input/fasta/E.coli_K12_MG1655.fa</span><br><span class="line"></span><br><span class="line">echo "############" `date "+%Y-%m-%d %H:%M:%S"` "############"</span><br><span class="line">echo "--- 开始合并处理样本 $&#123;sample_name1&#125; $&#123;sample_name2&#125; $&#123;sample_name3&#125; $&#123;sample_name4&#125; $&#123;sample_name4&#125; $&#123;sample_name5&#125; $&#123;sample_name6&#125; $&#123;sample_name7&#125; $&#123;sample_name8&#125; $&#123;sample_name9&#125; ---"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义模板变量</span></span><br><span class="line">INPUT_FILE1=$&#123;sample_name1&#125;.g.vcf</span><br><span class="line">INPUT_FILE2=$&#123;sample_name2&#125;.g.vcf</span><br><span class="line">INPUT_FILE3=$&#123;sample_name3&#125;.g.vcf</span><br><span class="line">INPUT_FILE4=$&#123;sample_name4&#125;.g.vcf</span><br><span class="line">INPUT_FILE5=$&#123;sample_name5&#125;.g.vcf</span><br><span class="line">INPUT_FILE6=$&#123;sample_name6&#125;.g.vcf</span><br><span class="line">INPUT_FILE7=$&#123;sample_name7&#125;.g.vcf</span><br><span class="line">INPUT_FILE8=$&#123;sample_name8&#125;.g.vcf</span><br><span class="line">INPUT_FILE9=$&#123;sample_name9&#125;.g.vcf</span><br><span class="line"></span><br><span class="line">cd /tmp</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 从HDFS上download所有的GVCF文件</span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE1</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE2</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE3</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE4</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE5</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE6</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE7</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE8</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash">HADOOP/hadoop fs -get /wgsv2/output/gvcf/<span class="variable">$INPUT_FILE9</span></span></span><br><span class="line">echo "*** 从HDFS上获取所有相关的gVCF文件"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 合并所有的gVCF文件为VCF文件 -- E_coli_K12.vcf</span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "java -jar $GATK -T GenotypeGVCFs -nt 4 -R $REF --variant $INPUT_FILE1 --variant $INPUT_FILE2 \</span><br><span class="line">--variant $INPUT_FILE3 --variant $INPUT_FILE4 --variant $INPUT_FILE5 --variant $INPUT_FILE6 \</span><br><span class="line">--variant $INPUT_FILE7 --variant $INPUT_FILE8 --variant $INPUT_FILE9 \</span><br><span class="line">-o E_coli_K12.vcf" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time java -jar $GATK -T GenotypeGVCFs -nt 4 -R $REF --variant $INPUT_FILE1 --variant $INPUT_FILE2 \</span><br><span class="line">--variant $INPUT_FILE3 --variant $INPUT_FILE4 --variant $INPUT_FILE5 --variant $INPUT_FILE6 \</span><br><span class="line">--variant $INPUT_FILE7 --variant $INPUT_FILE8 --variant $INPUT_FILE9 \</span><br><span class="line">-o E_coli_K12.vcf 1&gt;&amp;2 &amp;&amp; echo "*** 合并所有的gVCF文件为VCF文件" &amp;&amp; echo ""&gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 1.将vcf文件压缩</span></span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "bgzip -f E_coli_K12.vcf" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time bgzip -f E_coli_K12.vcf &amp;&amp; echo "*** 将vcf文件进行压缩" &amp;&amp; echo ""&gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 2.构建tabix索引</span></span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "$SAMTOOLS/tabix -p vcf E_coli_K12.vcf.gz" &gt;&amp;2</span><br><span class="line">time $SAMTOOLS/tabix -p vcf E_coli_K12.vcf.gz \</span><br><span class="line">&amp;&amp; echo "*** 给压缩文件构建tabix索引" &amp;&amp; echo ""&gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 3. 上传最终的VCF文件以及索引文件到HDFS上</span></span></span><br><span class="line">echo "###COMMAND LINE###：" &gt;&amp;2</span><br><span class="line">echo "$HADOOP/hadoop fs -f -put E_coli_K12.vcf.gz* /wgsv2/output/vcf" &gt;&amp;2</span><br><span class="line"></span><br><span class="line">time $HADOOP/hadoop fs -f -put E_coli_K12.vcf.gz* /wgsv2/output/vcf \</span><br><span class="line">&amp;&amp; echo "*** 变异检测结果vcf文件上传到HDFS上" &amp;&amp; echo ""&gt;&amp;2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">rm -f E_coli_K12.vcf* *g.vcf*</span></span><br><span class="line"></span><br><span class="line">echo "--- 结束合并处理样本 $&#123;sample_name1&#125; $&#123;sample_name2&#125; $&#123;sample_name3&#125; $&#123;sample_name4&#125; $&#123;sample_name4&#125; $&#123;sample_name5&#125; $&#123;sample_name6&#125; $&#123;sample_name7&#125; $&#123;sample_name8&#125; $&#123;sample_name9&#125; ---"</span><br><span class="line">echo "############" `date "+%Y-%m-%d %H:%M:%S"` "############" &amp;&amp; echo ""</span><br></pre></td></tr></table></figure><h3 id="3-4-3-基因测序与模板引擎的结合">3.4.3 基因测序与模板引擎的结合</h3><p>在我的项目框架中，对于不同的样本文件，都是采用同一套基因测序分析流程，而测序流程都是通过Linux命令行界面，因此需要通过一定的转换，将这一套分析流程提炼成一个Shell模版，对于不同的样本文件生成和它相应的分析脚本文件，这样就能达到不同的样本使用这同一套分析流程的目的，最后在不同的map任务中分别执行每个样本的分析脚本。测序流程与分析脚本的模版化如图3-2所示。</p><p><img src="https://i.imgur.com/Am9wU9Z.png" alt="图3-2 测序流程与分析脚本的模版化"></p><h2 id="3-5-本章小结">3.5 本章小结</h2><p>在本章中，主要对基于Hadoop基因测序数据处理的关键技术作了一个剖析和研究，解决了测序流程如何与MapReduce程序结合的问题，以及对于测序流程中各个步骤产生的数据结果如何存储，是存储在本地还是分布式存储系统中的问题，以及对分析流程的完整行作了简要分析，提出了通过设置驱动程序的参数达到对每一行数据分为一个Map程序处理的设计方案，以及通过引入第三方FreeMarker框架来保证分析流程的统一性和完整性问题，最后构建FreeMarker引擎以及对测序流程进行模板化的设计等。</p><p>在下一章节中，对基于Hadoop大数据的基因测序平台的流程搭建进行设计与实现。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Samy Ghoneimy, Samir Abou El-Seoud. A MapReduce Framework for DNA Sequencing Data Processing[D]. British University 2017. <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>Apache. Apache FreeMarker™[EB/OL]. <a href="https://freemarker.apache.org/" target="_blank" rel="noopener">https://freemarker.apache.org/</a>, 2018. <a href="#fnref2" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>Mahmoud Parsian. 数据算法（Hadoop/Spark大数据处理技巧）[M]. 苏金国,杨健康等译. 清华大学出版社 第四版 2016. <a href="#fnref3" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>2 相关技术及原理</title>
      <link href="/blog/5290c12.html"/>
      <url>/blog/5290c12.html</url>
      
        <content type="html"><![CDATA[<h2 id="2-1-hadoop相关技术和原理">2.1 Hadoop相关技术和原理</h2><p>本节主要对基于Hadoop平台的相关技术，如：HDFS、YARN和MapReduce三大模块进行原理介绍，为后续基因测序在Hadoop平台上的搭建作相应的准备。</p><h3 id="2-1-1-hdfs分布式存储系统">2.1.1 HDFS分布式存储系统</h3><p>Hadoop分布式存储系统，即Hadoop Dirstributed FileSystem，简称HDFS<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>。由于Hadoop集群的基础架构设施，使得HDFS是一个能提供可扩展性和高可靠性的基于Java的文件系统，并且适用于大型商业服务器集群或是廉价服务器上，从而可以提供PB级的存储容量和上千个服务器的计算能力。</p><a id="more"></a><p>此外，HDFS还是一个高容错的分布式存储系统，理论上可以无限扩展集群的存储能力，并与YARN服务（主要负责Hadoop集群中的资源管理和任务调度工作）协调各种并发任务调度。通过在多台服务器节点之间分配存储资源和计算资源，使得存储资源可以随需求线性增长，并协调各个服务器之间的计算能力。</p><p>HDFS有以下特点来确保数据高效存储在Hadoop集群中，并具有高可用性：</p><ol><li>机架感知意识，在Hadoop集群中客户端进行数据访问时，会自动考虑节点的物理位置，将离作业节点最近的数据块传输过去。</li><li>保证最小的数据转移，Hadoop将计算任务移动到数据所在的节点上，这样能保证处理任务发生在数据所在的物理节点，从而显著减少机架之间的网络传输的I/O流。</li><li>动态诊断HDFS存储系统的运行状况，并重新平衡不同节点上的数据。</li><li>回滚操作，HDFS支持对某一状态的文件系统进行快照保存，这样允许操作员在升级后恢复以前的HDFS版本，以防止发生人为或系统错误。</li></ol><p>在高可用的集群中，在两台独立的机器分别配置NameNode。在任何时候，只有一个NameNode处理Active状态，另一个处理Standby状态，在必要时候可以提供快速故障转移。HDFS的联邦政策，允许在一个集群中部署多个子NameNode，可以扩展文件块的镜像文件的大小，并管理联合集群。</p><p>在Unix系统架构中，每次磁盘都有默认的数据块，文件都以块的形式存储的，同样，在HDFS也有块（block）的概念，但是比Unix的文件系统块大得多，默认为128MB。HDFS的架构图如图2-1所示：</p><p><img src="https://i.imgur.com/ta21xdv.png" alt="图2-1 HDFS架构图"></p><p>对分布式文件系统中的块进行HDFS抽象可以使得一个文件的大小可以大于网络中任意一个磁盘的容量。从本地文件系统将一个文件复制到HDFS，该命令调度Hadoop文件系统的fs命令，并提供一系列子命令，包括-copyFromLocal。在下面的命令中，本地文件local.txt会被上传到HDFS中，路径为<code>/user/elon/remote.txt</code>。</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">%</span><span class="bash"> hadoop fs -copyFromLocal local.txt hdfs://localhost/user/elon/remote.txt</span></span><br></pre></td></tr></table></figure><h3 id="2-1-2-mapreduce并行计算框架与yarn资源调度器">2.1.2 MapReduce并行计算框架与Yarn资源调度器</h3><p>Hadoop MapReduce<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>是一个计算框架，在进行MapReduce编程时，要编写用于数据处理的map函数和reduce函数，并以可靠并容错的方式在数千个节点的大型群集的大型商业硬件上并行处理大量数据。</p><p>通常，计算节点和存储节点是相同的，允许框架在数据已经存在的节点上有效地调度任务，达到数据本地化的效果，从而在整个集群中具有非常高的执行效率。<br>在Hadoop2.0以后，Hadoop社区将资源管理和任务调度的功能从原来的MapReduce中分离出来，形成现在的YARN<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>，Apache Hadoop YARN架构 如图2-2所示：</p><p><img src="https://i.imgur.com/lhCLV4Q.png" alt="图2 2 Apache Hadoop YARN架构图"></p><p>从上面的YARN的架构图可以得知，YARN[13]是由资源管理器和作业调度及各个节点的监控所组成的，包含一个全局的资源管理器（ResourceManager，简称RM）和每个应用程序的管理器（ApplicationMaster，简称AM），其中应用程序可以是单个的作业或者是一个DAG（有向无环图）的作业流水线。</p><p>MapReduce程序就是在YARN上进行资源分配和作业调度的，集群中的每个计算节点的主机都运行着一个节点管理器（NodeManager，简称NM），负责监视机器上各个容器的资源私使用情况，而作业是部署在每个容器（Container）中进行计算的，对于一个大型的任务，可能会被拆分为多个map任务而被部署在各个容器中来并行计算。</p><p>对于一个作业的完整运行流程的阐述如下：首先从客户端提交作业，随后被分配到各个计算节点的container容器中，并会在其中一个计算节点生成AM任务管理器，AM了解本次任务执行所需的资源，并会向RM进行资源请求，随后RM将命令NM资源管理器向该节点中的容器分配相应的计算资源。在整个任务的计算周期内，Container会定时向AM发送自己MapReduce程序的运行进展情况，NM对自己节点上的Container容器的资源使用情况进行监控，并将其报告给RM。</p><h2 id="2-2-全基因组测序相关技术和处理流程">2.2 全基因组测序相关技术和处理流程</h2><p>在进行全基因组测序相关处理流程的设计<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>时，主要涉及三个大的流程步骤（如下图），全基因组的测序分析步骤如图2-3所示。</p><p><img src="https://i.imgur.com/3GaOomZ.png" alt="图2-3 全基因组的测序分析步骤"></p><p>第一步是对测序仪中得到的原始基因组数据进行数据指控，只有在对需要进行测序的数据进行数据质控后并保证了数据源的有效性，后面测序的结果才是真实有效的。在第二步中，对测序数据进行预处理，而这其中主要包括三个步骤：read比对、排序以及标记重复序列，并对标记的重复序列建立索引，最终作为变异检测的输入数据。在第三步中，进行主要的变异检测操作，而这里又分为两种方式，第一种是对单样本数据直接进行变异检测处理，直接得到变异结果。第二种是对多个样本组数据进行分批变异检测处理，得到分离的gVCF文件，最后再合并成一个VCF变异文件，得到最终的基因变异集合。</p><h3 id="2-2-1-原始数据质控">2.2.1 原始数据质控</h3><p>在全基因组测序研究中，以illumina为首的测序流程大多数都是运用边合成边测序的技术，其中需要经历一系列的化学反应。在这个合成的过程中随着化学合成链的增长，DNA聚合酶的活性会不断下降。另外，由于测序仪在刚开始化学合成反应时不够稳定，同样会带来碱基质量值的波动。</p><p>测序数据的质量好坏会直接影响我们的基因测序的下游分析和最终的变异检测的结果。因此我们需要在进行下游分析之前进行数据的质量控制检测<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>。但不同的测序平台其稳定性和测序的准确性也有差异，目前在数据指控方面使用较多的是FastQC这款软件。</p><p>FastQC<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>是一个Java程序，能够比较高效地来计算，并能以HTML的格式给出read的质量值和碱基测序错误率，形成最终的基因样本数据的测序报告。在给出的报告中，会包含几个重要的指标并以图谱的形式展现出来，指明原始数据可能还存在着哪些问题，并通过其质控结果作出相应的后续工作，比如是直接在后续分析流程中使用、或是进行部分切除后使用，亦或是将该原始数据直接丢弃不使用等。<br>在本次研究所使用的大肠杆菌的全基因组的测序样本中，对所有的样本都进行了数据质控的操作，并根据质控报告做了相应的处理。下面我选取SRR3226036_1号样本 进行数据质控的流程展示，基本的测序数据信息如图2-4所示。</p><p><img src="https://i.imgur.com/zW7EDJ1.png" alt="图2-4 基本的测序数据信息"></p><p>从上图中，我们不难得到该SRR3226036_1号样本数据的基本信息，其中就包括测序数据的文件名、文件类型，测序仪的编码类型等基本信息，后面还给出来本次质控的总测序read数是123万多条，测序read中被标记为低质量的测序读长为0，每条测序read的长度在32-76之间。</p><p><img src="https://i.imgur.com/Z0oBb5w.png" alt="图2-5 read的碱基质量值分布图"></p><p>read的碱基质量值分布如图2-5所示，展示的是该样本质控中read的碱基质量值分布图，其中横轴是read上碱基的位置分布，纵轴是碱基质量值。在该图中碱基质量值都大于35，而且各个read位置的碱基值波动很小，可以看到图中质量值的分布都在绿色背景（代表高质量）的区域，说明这个样本的碱基质量很稳定，这其实是一个非常高质量的样本质控结果。</p><p>各读长中碱基的含量占比如图2-6所示，展示的是该测序样本中各个碱基的含量占比情况，我们可以发现在读长为小于20以及读长大于75的区域，其碱基含量分布是波动较厉害的，说明在这两个区域的化学合成反应是不稳定所导致的，也就是在反应的开始阶段测序仪内部环境还处于不稳定的状态，以及在化学反应后期生物酶已逐渐失活所引起的，恰好应证了在本节最开始处提及的一般测序read不会太长的情况，这也是在数据质控的基本信息那张图片所展示的测序读长在32～76之间的缘故。因为只有这一块的碱基含量占比是稳定的，而且各个碱基之间的占比差距不大，表明这部分读长的数据是相对可靠的，适合拿来做基因测序的下游分析。</p><p><img src="https://i.imgur.com/5uuMmgE.png" alt="图2-6 各读长中碱基的含量占比图"></p><p>这一节主要对测序样本数据进行质量分析控制，将不适合进行下游分析的数据样本切除或丢弃。下一节会详细介绍基因测序处理流程的各个分析阶段，展现如何从一组样本数据得到最终的变异检测文件。</p><h3 id="2-2-2-数据预处理">2.2.2 数据预处理</h3><p>(1) 第一步：序列比对<br>全基因组测序的短序列（read）存储于FASTQ文件里面，在我的测序样本数据中是用fastq.gz的压缩格式表示的，这样的gz格式文件，在序列比对时也是被支持的。它会先进行解压再开始比对。虽然这些短序列在测序仪中进行化学反应时是在一起的，原本都来自于有序的基因组库中，但在经过化学合成和碱基提取之后，原本的read序列的前后顺序关系就已经不存在了。所以，FASTQ文件中紧挨着的两条read之间都是随机来自于原本基因组样本中某个位置的短序列而已。</p><p>因此，我们需要先把这一大堆的短序列捋顺，将这些短序列与该物种的参考基因组 作比对，通过一个read一个read的比对，找到每一个read在参考基因组上的位置，并在文件中标记好每个读长的位点信息，从而得到比对后的数据文件，这个过程就称为测序数据的序列比对。</p><p>测序数据的序列比对本质上是一个按照参考基因组来寻找最大公共子字符串的过程。而目前用的比较广泛的是BWA 软件，它将使用BWA-MEM算法[17]进行序列比对。下面是BWA的官方文档[18]说明，其中指出了用于序列比对的API：<br><code>Usage: bwa mem [options] &lt;idxbase&gt; &lt;in1.fq&gt; [in2.fq]</code></p><p>其中第一个参数[option]是可选参数，包括程序处理时所分配的线程数，最小的seed长度以及read的类型等信息。第二个参数<idxbase>是指代参考基因组的BW索引文件。第三个和第四个参数指代的是输入测序数据，包括R1.fastq和R2.fastq文件。输出数据包含各read序列的位点信息，可以采用重定向的方式输出到其他文件中。具体的示例如下：</idxbase></p><p>在上面的示例中，需要补充的是samtools软件的使用，这个在下面排序阶段会有提及，这里对其进行简单说明，最后将比对的文件进行重定向到管道中，并交由samtools的view方法进行自动检测数据格式，并最终输出BAM格式的文件并保存在SRR3226036.bam中。</p><p>(2) 第二步：排序<br>由于在上一步中只是将原始短序列读长与参考基因组进行序列比对，并记录各个序列的位点信息随即就输出了，而对原始序列未进行移动，它们还是随机分布于BAM文件中的。在这一步排序中，就是通过第一步中保存的各个序列的位点信息，需要将比对记录按照顺序从小到大排序下来，才能进行后续的去重复等操作。</p><p>这里，我们将会用到另外一款流行的基因测序的软件Samtools 。这里通过其官方API文档对Samtools的一个简单介绍。samtools一共有五个模块的功能，其中包括indexing（建立索引）、editing（对读长序列进行处理，包括替换BAM头、去除重复以及修复元数据信息等）、file operations（对测序样本文件操作，包括切分、合并、排序以及转换文件格式等）、stats（读取文件的状态信息等，包括BAM索引信息、计算测序深度以及生成状态信息等）和viewing（对BAM标记进行解释，以及将SAM、BAM和CRAM三种格式进行转换）。在本次测序流程中，主要使用这款软件完成排序和建立索引的操作，因此还会再后面的建立索引的流程中使用到它。在Samtools[19]官方文档中，比对命令如下：<br><code>Usage: samtools sort [options...] [in.bam]</code></p><p>其中第一个参数[options…]是可选参数，在本次排序处理中，会用到的有-@ 整数，设置用于排序和压缩的线程数；-m 整数，设置分配给每个线程的最大内存资源；-O 格式类型，指定输出的格式类型，这里包括sam、bam和cram三种格式，我们需要的是bam格式，用于下一步的标记重复操作；-o 文件，指定最终输出到一个文件中保存，而不是将其直接输出到屏幕上的这种标准输出形式；以及-T PREFIX，指定临时文件的文件命名中的首字母，最后这些临时文件会在排序结束后删除。第二个参数[in.bam]指代的就是已包含各个测序序列的参考位点信息的比对文件。具体的示例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">SAMTOOLS/samtools sort \</span></span><br><span class="line">-@ 4 \</span><br><span class="line">-m 4G \</span><br><span class="line">-O bam \</span><br><span class="line">-o SRR3226036.sorted.bam \</span><br><span class="line">SRR3226036.bam \</span><br><span class="line">-T PREFIX.bam \</span><br><span class="line">&amp;&amp; echo "*** BAM文件排序完成"</span><br></pre></td></tr></table></figure><p>(3) 第三步：标记重复序列<br>在基因测序的处理流程中，我们通过化学反应得到DNA液，并放到测序仪中将其测序并切分为每个短读长序列，分离成每个碱基对的形式，得到我们所需的原始基因组数据。这样一系列操作需要通过物理（超声）打断或者化学试剂（酶切）的方式来打断原始的DNA序列，其中就存在DNA的降解的问题，再加上有些基因数据在原本的基因组中含量就相对稀少，会存在局部DNA浓度过低的问题，因此需保证每种DNA片段在原来的基因组中的相对含量。</p><p>因此，我们需要选择适合实验的特定长度范围的序列来进行PCR扩增，并加入到测序仪中测序保证数据的完整性问题。而基因组中重复序列的来源，实际上就是PCR扩增过程所导致的。PCR扩增本质上就是通过化学反应的方式将原始的某一段DNA序列复制多次。</p><p>但是要注意的是，PCR技术是为了增大微量的DNA片段的浓度含量，但由于整个化学反应都在同一个试管中进行，因此其他一些密度并不低的DNA片段同时也会被放大好多倍，这时再取样去测序仪上测序的时候，这些浓度并不低的DNA片段就很可能会被重复取到相同的多条，并通过测序仪中的测序得到样本数据。因此我们需要对上一步中有重复的读长数据进行标记或者去除操作。</p><p>事实上，目前用的比较多的工具有Samtools和Picard，它们都可以去除重复序列，只是一个是直接删除重复序列，另一个可以仅标记但可保留重复序列，在最后的变异检测期间将其忽略。下面分别对这两种去除重复的方式进行介绍：<br>第一中，Samtools的方式：<br><code>Usage: samtools rmdup [-sS] &lt;input.srt.bam&gt; &lt;output.bam&gt;</code></p><p>在这个API中，可选参数-s指代的是仅对SE读长序列去除重复序列，-S指代的是对待PE读长和SE读长一样，都进行去除重复序列操作；第二个参数&lt;input.srt.bam&gt;指代的是输入数据源，第三个参数&lt;output.bam&gt;指代的是输出数据源，注意这两个数据源都是bam格式的文件。</p><p>第二种，Picard的方式，下面给出了我的测序流程中的处理命令：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java -jar picard.jar MarkDuplicates \</span><br><span class="line">I= SRR3226036.sorted.bam \</span><br><span class="line">O= SRR3226036.sorted.markdup.bam \</span><br><span class="line">M= SRR3226036.sorted.markup_metrics.txt \</span><br><span class="line">&amp;&amp; echo "*** 对BAM文件进行重复标记"</span><br></pre></td></tr></table></figure><p>这里总共需要三个参数，第一个是输入上一步已排序好的数据源BAM文件，第二个是输出的带有重复序列位点信息标记的BAM文件，第三个是对重复序列的位点信息进行标记的文件。这里还可以在第一个参数前使用REMOVE_DUPLICATE=true或REMOVE_SEQUENCING_DUPLICATES=true选项，表示在此操作中重复序列片段会从原BAM文件中直接删除掉。</p><p>这一步完成之后，我们需要为重复序列比对阶段得到的文件创建索引。创建索引的好处是它能够让我们可以随机访问这个文件中的任意位置并提供灵活性。通过在前面提及的Samtools软件来建立索引，命令如下：<br><code>Usage: samtools index [-bc] [-m INT] &lt;in.bam&gt; [out.index]</code></p><p>其中，第一个可选参数[-bc]分别是对BAI-format和CSI-format的序列格式转换为BAM格式的文件，第二个参数-m 整数是为CSI指数设置最小的区间大小。在我的处理测序流程中具体的示例如下：</p><h3 id="2-2-3-变异检测">2.2.3 变异检测</h3><p>到目前为止，我们已经完成了基因测序的前两大模块，数据质控和数据预处理，接下来就到了对样本数据进行变异检测的阶段了。这是本次全基因组分析流程创建的最终目标——得到样本集合的变异集合，可以为后续基因组学的科研提供有力的数据支撑。</p><p>变异检测的内容一般会包括：SNPs、indels、CNV和SV等。在这次的基因变异检测流程搭建中，主要涉及的是前两个SNPs和indels的变异检测。我们采用的生物信息学的分析工具The Genome Analysis Toolkit ，简称GATK。它是由The Broad Institute公司开发的用于二代重测序数据分析的一款软件。GATK是鉴定种系DNA和RNAseq数据中SNP和indels的行业标准，也是目前行业使用率最高的基因数据变异检测工具。GATK4.0的范围现在扩大到包括体细胞短变异调用，并且能够处理拷贝数（CNV）和结构变异（SV）。除了不同的调用者本身之外，GATK还包括许多实用工具，用于执行相关任务，如高通量测序数据的处理和质量控制，并捆绑了流行的Picard工具包。</p><p>通过GATK的官方API文档[20]中查看，可以得知和变异检测有关的方法有两个，它们分别是UnifiedGenotyper 和HaplotypeCaller 。<br>UnifiedGenotyper是根据每个基因组来调用SNPs[21]和indels[22]。该工具使用贝叶斯基因型似然性模型来同时估计N个样品群体中最可能的基因型和等位基因频率，为每个样品产生一种基因型。其输入数据可以是产生多种不同调用的read数据，输出则是一行未筛选高敏感性的VCF格式的文件。对于大量的样本组文件SNP调用，其分析示例如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">java -jar GenomeAnalysisTK.jar \</span><br><span class="line">    -T UnifiedGenotyper \</span><br><span class="line">    -R reference.fasta \</span><br><span class="line">    -I sample1.bam [-I sample2.bam ...] \</span><br><span class="line">    --dbsnp dbSNP.vcf \</span><br><span class="line">    -o snps.raw.vcf \</span><br><span class="line">    -stand_call_conf [50.0] \</span><br><span class="line">[-L targets.interval_list]</span><br></pre></td></tr></table></figure><p>其中，-T指代的是调用的方法类型；-R指代的是参考基因组；-I指代的是输入的样本数据，如果有多个则输入多个样本名；–dbsnp表示目前已知的某物种基因变异位点文件，一般会由大型的研究机构发布出来；-o是指代vcf格式的输出文件；-stand_call_conf 指代应该调用变体的最小phred-scaled置信度阈值，默认值是50.0；可选参数-L指代的是一个或多个要运行的基因组区间。</p><p>而对于另外一种基因变异检测的方法是通过HaplotypeCaller来实现，除了可以像UnifiedGenotyper一样，对变体的数据进行整体调用，得到变异检测的VCF文件，它和上一种方式的不同之处在于，它还可以通过局部重新组装haplotypes来调用SNPs和indels，侧重点在于对局部的样本文件进行变异检测，换句话说，只要程序遇到显示变化迹象的区域，它就会丢弃现有的映射信息，并完全重新组合该区域中的读取。</p><p>这一特性对于多样本数据的基因检测尤为有效，因为在对多样本进行变异检测时，可能刚开始的数据并不多，在测序流程进行到一部分的时候，该项目研究中需要再新增一部分新的样本数据，当使用原来UnifiedGenotyper的方式来进行变异检测生成VCF数据时，需要对全量数据重新进行分析计算。但是在这里，有了HaplotypeCaller的分析方式，就不存在全量数据的概念了。因为每个样本数据都视为整体的一部分，而对于每部分的样本变异检测都会生成gVCF文件，每个gVCF文件之间是隔离的，在有新的样本数据加入时，只需生成增量样本数据的gVCF文件即可。在我的测序处理流程中就是采用的这种先生成gVCF文件的方式。在我实际的测序流程中，具体的示例如下：</p><p>同样，这个HaplotypeCaller相关的分析命令和第一种UnifiedGenotyper方法的分析命令相似，唯一的不同之处在于新增了一项参数–emitRefConfidence GVCF，这项参数是固定的，指明本次命令变异检测的是单样本数据，产生的是gVCF文件格式。最后对于该样本组下的所有gVCF文件，通过GATK的另一个方法GenotypeGVCFs来进行合并处理，具体的使用示例如下。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java -jar GenomeAnalysisTK.jar \</span><br><span class="line">   -T GenotypeGVCFs \</span><br><span class="line">   -nt <span class="number">4</span></span><br><span class="line">   -R reference.fasta \</span><br><span class="line">   --variant SRR3226034.g.vcf \</span><br><span class="line">   --variant SRR3226035.g.vcf \</span><br><span class="line">…</span><br><span class="line">   --variant SRR3226042.g.vcf \</span><br><span class="line">   -o E_coli_K12.vcf \</span><br><span class="line">&amp;&amp; echo <span class="string">"*** 合并所有的gVCF文件为VCF文件"</span></span><br></pre></td></tr></table></figure><p>该方法是专门对HaplotypeCaller生成的gVCF文件进行genotyping联合，输入参数是一个或多个HaplotypeCaller gVCF进行genotype，输出参数是一个已经组合的genotyped的VCF。最终生成的E_coli_K12.vcf文件，即是E_coli_K12物种来自样本组中的全基因组变异检测结果。</p><h3 id="2-2-4-相关文件格式概述">2.2.4 相关文件格式概述</h3><p><strong>(1) FASTA文件格式</strong><br>FASTA[23]作为存储有顺序的序列数据的文件后缀，这里的序列数据指的其实就是能表示DNA或者蛋白质的一条字符串。FASTA文件表示的序列是有顺序的，我们可以通过各个read中碱基位点序列数，就可以知道某个DNA碱基在某个基因组上的准确位置，这个位置会用所在序列的名字和所在位置来表达，比如基因数据比对的结果等。</p><p>FASTA文件主要由两个部分构成：序列头信息（有时包括一些其它的描述信息）和第二行具体的基因序列数据。头信息独占一行，以大于号（&gt;）开头作为识别标记，包括染色体的名称信息和一些其他信息。</p><pre><code>&gt;NC_000913.3 Escherichia coli str. K-12 substr. MG1655, complete genomeAGCTTTTCATTCTGACTGCAACGGGCAATATGTCTCTGTGTGGATTAAAAAAAGAGTGTCTGATAGCAGCTTCTGAACTGGTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACTTTAACCAATATAGGCATAGCGCACAGACAGATAAAAATTACAGAGTACACAACATCCATGAAACGCATTAGCACCACCATTACCACCACCATCACCATTACCACAGGTAACGGTGCGGGCTGACGCGTACAGGAAACACAGAAAAAAGCCCGCACCTGACAGTGCGGGCTTTTTTTTTCGACCAAAGG</code></pre><p>上面是一个FASTA文件的例子，这是大肠杆菌K12.coli微生物基因的部分序列。在序列头信息中标注的是这个基因文件来自NC_000913.3号染色体，来自大肠杆菌K-12的全基因组数据。</p><p><strong>(2) FASTQ文件格式</strong><br>FASTQ[24]文件格式是目前存储测序数据最普遍、最公认的一个数据格式。前面FASTA文件存储的是来自该生物已排序已知的参考基因组的序列信息，而FASTQ存储的是来自测序仪得到的原始测序数据。这些原始数据是通过测序仪生成的图像数据转换过来，也是文本文件。文件大小依照测序量（或测序深度）的不同而存在较大差异，小的可能只有几M，如微生物的基因组，而大的则有几十甚至上百G，文件后缀通常都是.fastq.fq或者.fq.gz（gz压缩格式）。</p><pre><code>@SRR3226034.1 1/1TCCACGTTGCGACCGACAGCGCCACACGCTTCTGGTGGCGAGGCGACAACTCATTCCAGTTCAGCACATAGCGCT+@&lt;@6@=6C@@FFDGGEGGGGGGGGGGGEDGGGGG8EFFGD&gt;FGCEGCEDEEEFGG&lt;FFDGGAFFF,9&lt;,C+&gt;,@SRR3226034.2 2/1CCTTGCCGAGGGCCCGGCCCTCGACGCCCACGCGCCAGCCGCCGCTGGGCTGCCAGGCCAGCGCGCCGTACAGCG+AACCCC+@8::FGGGCGGGGGC7FEGGGGGGGFGGDGGGEGG7FF7=7C=:=E8C,DC87C+@+@F7@+B+,:+</code></pre><p>上面是从NCBI上下载的K12.coli物种的基因组测序样本SRR3226034的部分数据，从这个例子可以看出，FASTQ数据有着自己独特的格式：每四行成为一个独立的单元，我们称之为一个读长，简称为read，这是测序的最小单位。具体格式如下：</p><ol><li>第一行：以‘@’开头，是这一条read的名字，根据测序仪测序时的状态信息转换生成的，它是每一条read的唯一标识符，同一份FASTQ文件中不会重复出现；</li><li>第二行：测序read的序列，由A，C，G，T和N这五种字母构成，这是真正的DNA序列信息，其中N字符代表的是测序时那些无法被识别出来的碱基；</li><li>第三行：以‘+’开头，一般会空着；</li><li>第四行：用ASCII码标明测序read的质量值，表示的是每个测序碱基的可靠程度。<br>在后面的全基因组测序中要用到这两种数据格式，其中FASTA是参考基因组的存放格式，FASTQ是基因样本数据的存放格式。</li></ol><p><strong>(3) gVCF文件格式</strong><br>对单个样本Pair对数据进行基因测序时，最终得到Genome Variant调用格式文件,缩写为gVCF。它是一种文本文件格式，存储变体和非变体位置的测序信息，以一种紧凑的格式表示基因组中所有位点的基因型、注释和其他信息 。</p><p>gVCF是一种轻量的变体存储格式，可以将其压缩成gzip文件（<em>.genome.vcf.gz）。随后可以将得到的gCVF文件进行索引，创建一个</em>.tbi格式的文件，并与现有的VCF工具（如tabix和IGV）一起使用，使其既可以直接解释也可以作为三次分析的起点。</p><p>(1) 注释说明<br>gVCF文件的注释包含以下部分：</p><ol><li><p>元信息行以##开头，包含元数据，配置信息，并定义INFO，FILTER和FORMAT字段的值。</p></li><li><p>标题行以#开头，并命名数据行使用的字段。这些字段是#CHROM，POS，ID，REF，ALT，QUAL，FILTER，INFO，FORMAT，后跟一个或多个样本列。</p></li><li><p>数据行包含关于基因组中一个或多个位置的信息。</p></li></ol><p>(2) 字段说明<br>固定字段#CHROM，POS，ID，REF，ALT，QUAL在1000 Genomes Project提供的VCF4.2标准中定义，元信息中描述了字段ID，INFO，FORMAT和样本。</p><ul><li><ol><li>CHAROM：染色体：来自参考基因组的标识符。</li></ol></li><li><ol start="2"><li>POS：位置：参考位置，在每个参考序列CHROM中的第一个匹配的起始位置，位置按照递增顺序的数字排序，可以在一个POS中有多个记录。</li></ol></li><li><ol start="3"><li>ID：可用的唯一标识符。如果没有可用标识符，则使用默认值。</li></ol></li><li><ol start="4"><li>REF：参考碱基：A，C，G，T，N；可以有多个碱基。POS字段中的值指的是字符串中第一个碱基的位置。</li></ol></li><li><ol start="5"><li>ALT：逗号分隔的非等位基因列表，选项是：</li></ol></li><li><p>a) 由碱基A，C，G，T，N组成的碱基串。</p></li><li><p>b) ID字符串（“<id>”）</id></p></li><li><p>c) 或者是关于break-edns的部分中断替换字符串。</p></li></ul><p>如果没有其他等位基因，则使用缺失值。</p><ul><li><ol start="6"><li>QUAL：用于ALT中断的Phred-scale质量分数。即-10log_10概率。如果未知，则使用默认值。</li></ol></li><li><ol start="7"><li>FILTER：如果该位置已通过所有过滤器，即在该位置进行call并通过。否则，如果该位置未通过所有过滤器，则以分号分隔的过滤器代码列表表示。</li></ol></li><li><ol start="8"><li>INFO：其他信息。INFO字段被编码为分号分隔的一些列短键，其中可选值的格式如下：<key>=<data>[, data]。gVCF文件使用以下值：</data></key></li></ol></li><li><p>a) END：该记录中描述的区域的结束位置。</p></li><li><p>b) BLOCKAVG_min30p3a：非变量block块。</p></li><li><p>c) SNVSB：SNV位点链偏移</p></li><li><ol start="9"><li>FORMAT：实例字段的格式。FORMAT指定子字段的数据类型和顺序。gVCF文件使用以下值：</li></ol></li><li><p>a) GT：基因型</p></li><li><p>b) GQ：基因型质量</p></li><li><p>c) GQX：{基因型质量假设变异位置，基因型质量假定非变异位置}</p></li><li><p>d) DP：用于基因分型的过滤call深度。</p></li><li><p>e) DPF：在基因分型之前从输入中过滤掉基础调用。</p></li><li><p>f) AD：列出顺序中ref和alt等位基因的等位基因深度。对于indels，这个值只包括每个等位基因的读段。</p></li><li><p>g) DPI：读取与indel相关的深度，取自indel之前的集合。</p></li><li><ol start="10"><li>SAMPLE：样本实例在FORMAT中各个实例字段对应的值。</li></ol><pre><code>##fileformat=VCFv4.2##ALT=&lt;ID=NON_REF,Description=&quot;Represents any possible alternative allele at this location&quot;&gt;##FILTER=&lt;ID=LowQual,Description=&quot;Low quality&quot;&gt;##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=&quot;Allelic depths for the ref and alt alleles in the order listed&quot;&gt;##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=&quot;Approximate read depth (reads with MQ=255 or with bad mates are filtered)&quot;&gt;##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=&quot;Genotype Quality&quot;&gt;##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=&quot;Genotype&quot;&gt;##INFO=&lt;ID=MQ,Number=1,Type=Float,Description=&quot;RMS Mapping Quality&quot;&gt;##INFO=&lt;ID=MQRankSum,Number=1,Type=Float,Description=&quot;Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities&quot;&gt;##INFO=&lt;ID=RAW_MQ,Number=1,Type=Float,Description=&quot;Raw data for RMS Mapping Quality&quot;&gt;##INFO=&lt;ID=ReadPosRankSum,Number=1,Type=Float,Description=&quot;Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias&quot;&gt;##contig=&lt;ID=NC_000913.3,length=1769345&gt;##reference=file:///home/elon/wgs/input/fasta/E.coli_K12_MG1655.fa#CHROM   POS    ID    REF   ALT   QUAL   FILTER   INFO   FORMAT    E.coli_K12_SRR3226034NC_000913.3   1.   A   &lt;NON_REF&gt; .   .   END=1    GT:DP:GQ:MIN_DP:PL   0/0:8:24:8:0,24,334NC_000913.3   2.   G   &lt;NON_REF&gt;  ..END=2GT:DP:GQ:MIN_DP:PL0/0:8:0:8:0,0,174… …NC_000913.3   41   .   T   C,&lt;NON_REF&gt;   112.77   .BaseQRankSum=1.309;ClippingRankSum=0.000;DP=9;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-0.589;RAW_MQ=32164.00;ReadPosRankSum=-0.100   GT:AD:DP:GQ:PL:SB0/1:4,5,0:9:57:141,0,57,152,71,223:1,3,5,0NC_000913.3   46   .   T   TAA,&lt;NON_REF&gt;   135.73   .BaseQRankSum=0.566;ClippingRankSum=0.000;DP=8;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=-0.489;RAW_MQ=28564.00;ReadPosRankSum=1.292   GT:AD:DP:GQ:PL:SB0/1:3,5,0:8:98:173,0,98,183,113,296:2,1,3,2</code></pre></li></ul><p>上面是对样本SRR3226034的基因测序产生的gVCF文件的简化部分，从注释信息中可以看到文件格式是VCFv4.2 ，基因组用##contig注释信息表明，这里是指ID号为NC_000913.3的染色体，长度为1769345。参考基因组文件通过##reference注释信息标明，这里指的是路径为<code>file:///home/elon/wgs/input/fasta/E.coli_K12_MG1655.fa</code>下的文件。在最后给出的几条样例中，描述了在该物种的NC_000913.3染色体上第1号位点和第2号位点上的两段非变异性调用（分别以A和G开头）。另外最后两个是该物种的NC_000913.3染色体上第41号位点的变异信息和第46号位点的变异信息，这两条信息串的结束位点都是缺失的，因此说明变异信息只包含在该开始位点上。</p><p><strong>(4) VCF文件格式</strong><br>VCF是一种文本文件格式，通常会以“.gz”的压缩方式存储。它包含一个元信息行，一个标题行，然后数据行是包含基因组中一个位点的信息。这个格式也能包含样本组各个位点的基因型信息。</p><ol><li><p>样本组VCF文件<br>下面给出的VCF文件就是通过九组样本文件分别的变异检测得到的gVCF文件合并得到的，里面包含了该物种的这九组样本所检测到的基因变异位点的信息。VCF文件的格式大致上和gVCF相同，因为它们都是按照The Variant Call Format（VCF）Version 4.2 Specification规范来执行的，唯一不同之处是最后一列样本实例信息列，在单个gVCF文件中都只是包含单个样本文件的信息。而在VCF文件中，由于此处是通过九个gVCF文件合并得到的，因此样本列包含九个样本的所有位点信息。</p><p>##fileformat=VCFv4.2<br>##ALT=&lt;ID=NON_REF,Description=“Represents any possible alternative allele at this location”&gt;<br>##FILTER=&lt;ID=LowQual,Description=“Low quality”&gt;<br>##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=“Allelic depths for the ref and alt alleles in the order listed”&gt;<br>##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=“Approximate read depth (reads with MQ=255 or with bad mates are filtered)”&gt;<br>##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=“Genotype Quality”&gt;<br>##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=“Genotype”&gt;<br>##INFO=&lt;ID=MQ,Number=1,Type=Float,Description=“RMS Mapping Quality”&gt;<br>##INFO=&lt;ID=MQRankSum,Number=1,Type=Float,Description=“Z-score From Wilcoxon rank sum test of Alt vs. Ref read mapping qualities”&gt;<br>##INFO=&lt;ID=QD,Number=1,Type=Float,Description=“Variant Confidence/Quality by Depth”&gt;<br>##INFO=&lt;ID=RAW_MQ,Number=1,Type=Float,Description=“Raw data for RMS Mapping Quality”&gt;<br>##INFO=&lt;ID=ReadPosRankSum,Number=1,Type=Float,Description=“Z-score from Wilcoxon rank sum test of Alt vs. Ref read position bias”&gt;<br>##INFO=&lt;ID=SOR,Number=1,Type=Float,Description=“Symmetric Odds Ratio of 2x2 contingency table to detect strand bias”&gt;<br>##contig=&lt;ID=NC_000913.3,length=1769345&gt;<br>##reference=file:///home/elon/wgs/input/fasta/E.coli_K12_MG1655.fa<br>#CHROM   POS   ID   REF   ALT   QUAL   FILTER   INFO   FORMAT    E.coli_K12_SRR3226034E.coli_K12_SRR3226035   E.coli_K12_SRR3226036   E.coli_K12_SRR3226037   E.coli_K12_SRR3226038E.coli_K12_SRR3226039    E.coli_K12_SRR3226040   E.coli_K12_SRR3226041    E.coli_K12_SRR3226042<br>NC_000913.341.TC108.AC=1;AF=0.071;AN=14;BaseQRankSum=1.31;ClippingRankSum=0.00;DP=234;ExcessHet=3.0103;FS=13.222;MLEAC=1;MLEAF=0.071;MQ=59.78;MQRankSum=-5.890e-01;QD=12.00;ReadPosRankSum=-1.000e-01;SOR=3.590GT:AD:DP:GQ:PL0/1:4,5:9:57:141,0,570/0:42,0:42:99:0,114,17100/0:26,0:26:72:0,72,10800/0:33,0:33:99:0,99,1345./.:0,0:0:.:0,0,0./.:0,0:0:.:0,0,00/0:41,0:41:90:0,90,13500/0:44,0:44:99:0,105,15750/0:39,0:39:99:0,102,1530<br>NC_000913.346.TTA,TAA398.53.AC=2,1;AF=0.143,0.071;AN=14;BaseQRankSum=0.566;ClippingRankSum=0.00;DP=242;ExcessHet=4.1497;FS=0.920;MLEAC=2,1;MLEAF=0.143,0.071;MQ=59.63;MQRankSum=0.00;QD=4.43;ReadPosRankSum=1.29;SOR=0.527GT:AD:DP:GQ:PL0/2:3,0,5:8:98:173,183,296,0,113,980/1:34,9,0:43:99:166,0,941,268,976,12440/0:26,0,0:26:35:0,35,939,35,939,9390/0:33,0,0:33:99:0,99,1345,99,1345,1345./.:0,0,0:0:.:0,0,0,0,0,0./.:0,0,0:0:.:0,0,0,0,0,00/1:30,5,4:39:41:105,0,798,41,791,11070/0:44,0,0:44:99:0,105,1575,105,1575,15750/0:39,0,0:39:99:0,102,1530,102,1530,1530</p></li><li><p>基因变异位点信息<br>通过一系列的基因测序流程以及和参考基因组的比对，最终得到的VCF文件包含了这九组样本的所有基因变异位点信息。在每个样本得到的gVCF文件，其中包含非变异性调用信息和变异性调用信息，因此整个文件大小在30M左右。而真正的变异位点的片段占很少一部分，因此将九个gVCF文件中变异性调用位点信息合并到一起就小很多，只有3M左右，里面包含的全部是变异调用位点。由于样本的随机性和基因变异的不确定性，可能某位点的变异性只存在与某些样本中，因此在VCF文件中就会看到某些变异片段仅在某些样本列下存在变异信息的情况</p></li></ol><h2 id="2-3-本章小结">2.3 本章小结</h2><p>这一章主要对构建基于Hadoop的全基因组测序平台所涉及的多方面作了介绍，其中包括Hadoop框架中的HDFS原理、MapReduce框架和YARN框架的介绍；介绍了一个作业在Hadoop集群中运行和调度的全过程；接着对全基因组测序流程的设计作了一个详细的剖析，然后介绍了测序中涉及比较多的两种数据格式FASTA和FASTQ，以及对基因测序得到的中间流程的变异检测结果文件gVCF和最终的变异检测文件VCF作了简要说明。</p><p>在接下来的一章中，将会对测序平台中MapReduce程序和测序处理脚本的关键技术做一个更为详细的剖析，例如测序流程是如何与MapReduce框架结合起来的，不同的数据格式是如何进行访问和存储，以及FreeMarker模板引擎的引入和分析流程模板的编写等。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>蔡斌, 陈湘萍. Hadoop 技术内幕：深入解析Hadoop Common 和HDFS 架构设计与实现原理[M]. 机械工业出版社, 2013. <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>董西成. Hadoop技术内幕：深入解析MapReduce架构设计与实现原理[M]. 机械工业出版社, 2013 <a href="#fnref2" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>董西成. Hadoop技术内幕：深入解析YARN架构设计与实现原理[M]. 机械工业出版社, 2013. <a href="#fnref3" class="footnote-backref">↩</a></p></li><li id="fn4" class="footnote-item"><p>陈浩锋. 新一代基因组测序技术[M]. 科学出版社, 2017. <a href="#fnref4" class="footnote-backref">↩</a></p></li><li id="fn5" class="footnote-item"><p>Richard M, Leggett. Sequencing quality assessment tools to enable data-driven informatics for high throughput genomics[R]. US National Library of Medicine, 2013. 4-28. <a href="#fnref5" class="footnote-backref">↩</a></p></li><li id="fn6" class="footnote-item"><p>FastQC. The FastQC Toolkit[EB/OL]. <a href="https://www.bioinformatics.babraham.ac.uk/projects/fastqc/" target="_blank" rel="noopener">https://www.bioinformatics.babraham.ac.uk/projects/fastqc/</a>, 2018. <a href="#fnref6" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>1 绪论</title>
      <link href="/blog/11b914a5.html"/>
      <url>/blog/11b914a5.html</url>
      
        <content type="html"><![CDATA[<blockquote><p>声明：<strong>基于Hadoop的基因组测序大数据分析平台研究</strong>是本人2018年的本科毕业设计课题，根据网络资源和自己的专业知识，独立完成整个流程设计、平台搭建和单元测试等工作。<strong>本系列文章是对该项目的一个整理总结和分享记录。该目录提及的系列文章可供转载，并无需通知作者，但需要在明显地方标注文章出处</strong></p></blockquote><a id="more"></a><p><strong>项目源码GitHub托管：<a href="https://github.com/longshilin/wgs" target="_blank" rel="noopener">https://github.com/longshilin/wgs</a></strong></p><blockquote><p>开发环境介绍：通过个人便携式笔记本ThinkPad开发，内存是8GB。</p><ul><li>操作系统：Ubuntu16</li><li>开发平台：IDEA</li><li>开发时间：2018年2月~5月</li></ul></blockquote><h2 id="1-1-论文的研究背景及意义">1.1 论文的研究背景及意义</h2><p>全基因组测序的英文是 Whole Genome Sequencing，简称WGS。它是将物种细胞里从第一个DNA开始一直到最后一个DNA的完整基因组序列，通过相关仪器和技术手段检测出来并排序好得到一种文本格式的文件，最后再将其和参考基因组做比对，鉴定出基因组上任何类型的突变。对于人类来说，全基因组测序能帮助我们更好的了解我们自身的基因位点信息，检测变异基因并应用在遗传病方面等。</p><h3 id="1-1-1-全基因组测序原理及发展历程">1.1.1 全基因组测序原理及发展历程</h3><p>基因测序技术经过四十多年的发展，作为二十一世纪的朝阳行业，已是时下热门。测序技术发展简史如图1-1所示，从图中可以看出，基因测序技术的开端在20世纪70年代中期，以当时英国生物化学家弗雷德里克·桑格（Sanger）开创的双脱氧链终止法<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>为开端。这是一种常用的核酸测序技术，主要用于DNA的测序分析。正因为此次的研究成果，将双脱氧链终止法与化学降解法以及其衍生方法统称为第一代DNA测序技术。</p><p><img src="https://i.imgur.com/QufdMAd.png" alt="图1-1 测序技术发展简史"></p><p>经过不断的技术开发和改进，在二十世纪初期随着人类基因组计划的完成，第二代测序技术诞生了。主要是以Illumina的Solexa/Hiseq技术和ABI的SOLID技术为代表，能够对一个物种的转录组或基因组进行深度测序<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>。第二代测序技术也使得DNA测序进入了高通量、低成本的功能基因组时代<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>。再后来，以PacBio的SMRT和ONT的纳米孔单分子测序技术为标志的第三代测序技术<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup>。与之前的第一代和第二代测序技术相比，最大的不同点在于单分子测序技术的引入，测序过程不采用PCR扩增技术，并且可以进行超长读长，使得测序技术又有了进一步的提升。</p><p>对于人类的全基因组测序来说，人类的全基因组一共有23对染色体，包含超过30亿个碱基<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>，而目前仅有3%的碱基信息能从临床医学上给予解释，因此还有大量的测序工作值得我们去发掘。由测序服务公司提供的原始测序序列文件在经过系统地分析处理前，无法提供任何有效的信息进行疾病的分析和预测.因此要想能够解密人类基因组的奥秘，就必须建立更加高效快速的数据分析平台，并结合生物测序仪器，将生物的蛋白质物质提取并转换为文本序列格式的数据，对其进行质量剔除、序列比对和基因测序等一系列测序分析工作<sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>。</p><p>纵观我国的基因组测序行业的状况，在2017年底，我国启动“中国十万人基因组计划”<sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>，这是我国在人类全基因组测序领域的一个重大战略举措，也是目前世界最大规模的人类基因组计划，对我国的基因测序行业的发展有着深远的意义。</p><h3 id="1-1-2-hadoop分布式平台">1.1.2 Hadoop分布式平台</h3><p><a href="http://hadoop.apache.org/" target="_blank" rel="noopener">Hadoop</a> 是Apache基金会下的一个顶级项目，主要用于在商业硬件网络上进行大规模计算和数据处理，该项目发展到目前已有超过十年的时间。这个分布式计算平台是用Java编写，最初是由Doug Cutting创建的，可以通过一个简单的MapReduce编程模型<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>，跨集群地对大型数据集进行分布式存储和计算处理等，Hadoop1.x与2.x的比较如图1-2所示。</p><p><img src="https://i.imgur.com/JdQOoUO.png" alt="图1 2 Hadoop1.x与2.x的比较"></p><p>如今Hadoop项目已经发展为了一个大数据处理的应用框架体系，但其中主要应用在数据处理模块的仍然是HDFS分布式存储系统和MapReduce计算框架。在Hadoop2.0<sup class="footnote-ref"><a href="#fn9" id="fnref9">[9]</a></sup>之后，将集群的资源管理和任务调度的功能分离成了一个YARN服务模块。在其1.0版本中，原本计算、资源管理及调度是统一管理的，但由于调度任务的增多导致集群资源调度的性能瓶颈问题，为了打破原来的性能瓶颈，将任务调度和资源管理从原来的模块中分离出来。也正是因为这种改进，使得一些其他的计算框架如<a href="http://spark.apache.org/" target="_blank" rel="noopener">Spark</a> 、<a href="http://hbase.apache.org/" target="_blank" rel="noopener">HBase</a> 等能够轻松应用在Hadoop集群中。</p><h2 id="1-2-国内外研究现状">1.2 国内外研究现状</h2><h3 id="1-2-1-基因测序的研究现状">1.2.1 基因测序的研究现状</h3><p>在基因测序服务方面，目前全球有上千家厂商提供测序服务，美国是基因测序服务发展较早的国家，因此大部分的服务产商都分布在美国。目前有超过200家分布在中国。在奥巴马政府于2015年初颁布<a href="http://guba.eastmoney.com/news,000788,173474151.html" target="_blank" rel="noopener">“精准医疗计划”</a> ；第二年中国国家科技部也将精准医疗列入<a href="https://www.hottui.com/bencandy-65-197782-1.htm" target="_blank" rel="noopener">重点研发计划</a> ，精准医疗受到中美两国政府的高度重视。政府机构对于个性化精准医疗的战略部署，直接反映了基因测序在当今时代的重要性。</p><p>随着全球市场对基因测序的需求不断提升，一些大型的测序产商也在快速更新自己的测序仪产品。如Illumina测序公司在2014年推出了HiSeqX Ten测序仪系统，是第一个每年处理20,000个基因组的高通量系统，每个成本为1,000美元，从2015年，Illumina开始提供该系统的缩小版本HiSeq X Five。由于测序成本的不断降低，恰好也推动着一些医院和科研机构纷纷加入个性化医疗的研究领域中来，使得个性化精准医疗能够快速发展<sup class="footnote-ref"><a href="#fn10" id="fnref10">[10]</a></sup>。</p><h3 id="1-2-2-hadoop与基因测序的研究现状">1.2.2 Hadoop与基因测序的研究现状</h3><p>下图是每个DNA序列的测序费用，早在2007年末，测序费用已打破摩尔定律迅猛下降，而截止到2017年对于每条DNA序列的测序费用已低于0.01美元。这十年间测序技术的发展和世界各国科研的投入，使得基因测序的成本和门槛越来越低。</p><p>每段DNA序列的测序成本如图1-3所示，面对如此低廉的测序成本，使得个性化测序医疗的成本也会越来越低。普通人群也可以享受基因测序带来的红利，想通过基因测序了解自己的基因变异位点和遗传病的情况等。单随之而来的会面临大数据量的基因测序问题，因此我们必须寻求其他手段来高效并发的处理基因测序，在此可以利用Hadoop大数据批量处理的优势来并行化测序。在国外已经有科研机构或者公司进行这方面的尝试，如Broad Institute研究所，他们采用Hadoop、Spark的方式和基因测序结合起来，发挥Hadoop和Spark基于内存的实时分析能力，对基因测序和大数据结合的尝试。在GATK4.0的时候推出了<a href="https://software.broadinstitute.org/gatk/documentation/article?id=11244" target="_blank" rel="noopener">“在Spark集群上启用Spark的GATK工具”</a>等系列课题 。</p><p><img src="https://i.imgur.com/BGn56hb.jpg" alt="图1 3 每段DNA序列的测序成本"></p><h2 id="1-3-论文的主要研究内容">1.3 论文的主要研究内容</h2><p>通过全基因组测序流程分析并引入参考基因组，将测序样本进行一系列的处理操作如：</p><ol><li>原始数据质量控制</li><li>数据预处理（包括：序列比对、排序、标记重复序列以及建立索引等）</li><li>基因变异检测</li></ol><p>接着通过Hadoop大数据分析平台，并引入第三方模板引擎来构建分析处理脚本，结合MapReduce计算处理框架和分布式存储系统HDFS来并行计算和存储基因数据，从而实现对基因测序进行并行化测序，最终完成全基因组测序的全过程，获得样本准确的变异基因位点的集合。最后可以将这个变异集合用来指导一些疾病的研究和精准化医疗的应用方面等。</p><h2 id="1-4-章节安排">1.4 章节安排</h2><p>在第一章绪论中，主要介绍全基因组测序的研究背景及意义，其中包括对全基因组测序原理及发展历程的介绍，并对Hadoop分布式平台加以介绍，最后对国内外基因测序的研究现状和本论文主要的研究内容作了阐述。</p><p>第二章中，将会重点介绍基因测序所需要的Hadoop平台，包括HDFS组件和MapReduce组件的内部原理和相关特性。接着对全基因组测序所用到的相关流程模块及测序技术作整体介绍，主要包括三大模块：原始数据质控、数据预处理以及变异检测。最后对测序技术中出现的主要文件格式，如FASTA、FASTQ、gVCF和VCF这四种格式做了概述和格式说明。</p><p>第三章中，基于Hadoop平台基因测序的关键技术作了剖析和研究。其中包括测序逻辑与MapReduce的结合，不同格式数据的访问和存储，以及如何保证分析流程的完整性，最后引入FreeMarker模板引擎完成关键性的定制化脚本的搭建工作。</p><p>第四章中，基于Hadoop平台基因测序的流程处理和MapReduce计算框架的代码实现方面。首先是对Hadoop伪分布式环境和分布式环境的搭建和功能比较比较，其次实现Shell脚本执行引擎的构建，并分析Mappe阶段和Reduce阶段作业设计与分配，最后将对作业进行整体调度的实现。</p><p>第五章中，主要对第四章搭建的大数据处理平台的测试、优化及扩展。考虑到测试数据的易用性和分析平台测序流程的完整性，这里使用E.coli_K12的样本组数据。通过将该项目的样本数据引大数据分析平台，可以很好的对该平台进行测试和系统优化。最后对Hadoop分布式的基因测序效率与传统的脚本式的测序效率作比较，将系统平台进行扩展应用等。</p><p>第六章，对基于Hadoop的基因测序大数据平台研究与设计的总结，通过对基于Hadoop的基因组测序大数据分析平台的研究，实现自动并行化的样本数据的基因测序，大大改善传统串行处理的测序分析处理流程，从而有效的提升基因测序处理效率，并有望将该平台研究成果应用于实际基因精准化测序和个性化医疗领域中。</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Sanger, F. &amp; Nicklen, S. DNA sequencing with chain-terminating[P]. 74, 5463–5467 (1977). <a href="#fnref1" class="footnote-backref">↩</a></p></li><li id="fn2" class="footnote-item"><p>Struster SC.Next-generation sequencing transform today’s biology[J].Nat Methods.5(1):16-18 (2008). <a href="#fnref2" class="footnote-backref">↩</a></p></li><li id="fn3" class="footnote-item"><p>解增言,林俊华,谭军,舒坤贤. DNA测序技术的发展历史与最新进展[J]. 生物技术通报. 2010(08). <a href="#fnref3" class="footnote-backref">↩</a></p></li><li id="fn4" class="footnote-item"><p>Rusk N. Cheap third-generation sequecing[J]. Nature. 6(4): 244-245 (2011). <a href="#fnref4" class="footnote-backref">↩</a></p></li><li id="fn5" class="footnote-item"><p>J. Craig Venter, Mark D. Adams, Eugene W. Myers. The Sequence of the Human Genome[J]. Science, 2001, 291(5507): 1304-1351. <a href="#fnref5" class="footnote-backref">↩</a></p></li><li id="fn6" class="footnote-item"><p>高通量DNA测序技术及其应用进展[J]. 于聘飞,王英,葛芹玉. 南京晓庄学院学报2010-05-20 (05). <a href="#fnref6" class="footnote-backref">↩</a></p></li><li id="fn7" class="footnote-item"><p>衣春翔. 哈工大牵头启动十万人基因组计划[N]. 黑龙江日报. 2017-12-29 (003). <a href="#fnref7" class="footnote-backref">↩</a></p></li><li id="fn8" class="footnote-item"><p>Jeffrey Dean, Sanjay Ghemawat. MapReduce: Simplified Data Processing on Large Clusters[C]. America:Google, Inc., 2004. 137-149. <a href="#fnref8" class="footnote-backref">↩</a></p></li><li id="fn9" class="footnote-item"><p>Garry Turkington. Hadoop基础教程[M]. 张治起译. 人民邮电出版社 第1版, 2014. <a href="#fnref9" class="footnote-backref">↩</a></p></li><li id="fn10" class="footnote-item"><p>新一代基因组测序-通往个性化医疗[M]. 贾尼特编著,薛庆中等译. 科学出版社, 2012. <a href="#fnref10" class="footnote-backref">↩</a></p></li></ol></section>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>摘要</title>
      <link href="/blog/205eb716.html"/>
      <url>/blog/205eb716.html</url>
      
        <content type="html"><![CDATA[<p>本论文主要研究内容是将传统的全基因组测序与Hadoop框架结合的大数据测序平台研发，通过Hadoop中的HDFS分布式存储系统来提供高可靠的存储服务，结合基因测序的一系列软件工具（如：BWA、Samtools、Picard和GATK等）来进行测序流程设计，并引入第三方FreeMarker模板引擎来制定模板脚本，针对不同的样本数据生成定制化的脚本处理文件。将各个样本的处理脚本与Hadoop框架的MapReduce计算框架结合，以Map任务的方式提交到Hadoop集群的各个计算节点的Container容器中运行，从而实现基因测序的并行化测序处理流程，得到样本组中各个样本对的单体变异检测gVCF文件。再通过一个Reduce任务，将各个Map阶段的变异检测结果中包含基因变异位点的信息汇总到一个VCF文件，得到最终多个样本准确的变异集合。</p><a id="more"></a><p>通过本次毕业设计，让我学习了全基因组基因测序的全过程和Hadoop大数据框架的相关组件，并掌握许多关于脚本处理的过程及方法。</p><p>关键词  Hadoop;WGS;HDFS;MapReduce;全基因组测序;Hadoop分布式环境;变异检测</p><hr><p>Title:  Research on Hadoop-based genome sequencing big data analysis platform</p><p>Abstract:<br>The main research content of this paper is the development of the big data sequencing platform combining traditional whole genome sequencing with the Hadoop framework, providing highly reliable storage services through the HDFS distributed storage system in Hadoop, combined with a series of software tools for gene sequencing (eg, : BWA, Samtools, Picard, GATK, etc.) to design the sequencing process, and to introduce a third-party FreeMarker template engine to create a template script to generate customized script processing files for different sample data. Combining the processing scripts of each sample with the MapReduce computing framework of the Hadoop framework and submitting them to the Containers of each computing node of the Hadoop cluster as a Map task, to achieve the parallel sequencing process of gene sequencing, and to obtain each sample group The monomer variation of the sample pair detects the gVCF file. Then through a Reduce task, the information of the gene mutation sites contained in the mutation detection results of each Map stage is aggregated into a VCF file to obtain the final accurate variation set of multiple samples.<br>Through this graduation project, I learned the whole process of genome-wide gene sequencing and related components of Hadoop big data framework, and mastered many processes and methods of script processing.</p><p>Keywords:  Hadoop; WGS; HDFS; MapReduce; whole genome sequencing; Hadoop distributed environment; mutation detection</p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
          <category> Hadoop基因测序 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
            <tag> 大数据 </tag>
            
            <tag> 基因测序 </tag>
            
            <tag> 毕业设计 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>集合与线程实现</title>
      <link href="/blog/f054fe4e.html"/>
      <url>/blog/f054fe4e.html</url>
      
        <content type="html"><![CDATA[<h2 id="集合"><strong>集合</strong></h2><p>迭代器</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Iterator</span>&lt;<span class="title">E</span>&gt;</span>&#123;</span><br><span class="line"><span class="function">E <span class="title">next</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">boolean</span> <span class="title">hasNext</span><span class="params">()</span></span>;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">remove</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><p>将Java迭代器认为是位于两个元素之间。当调用next时，迭代器就越过下一个元素，并返回刚刚越过的那个元素的引用。<br>Iterator接口的remove方法将会删除上次调用next方法时返回的元素。如果在调用之前没有调用next将是不合法的。</p><h2 id="线程"><strong>线程</strong></h2><p><strong>Java多线程的三种实现</strong><br>(1) 继承Thread类，重写run函数<br>(2) 实现Runnable接口，重写run函数<br>(3) 实现Callable接口，重写call函数</p><p>利用继承类或实现接口的MyRunnable类，并重写run函数来创建并启动一个新线程：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span> Thread(<span class="keyword">new</span> MyRunnable()).start()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 迭代器 </tag>
            
            <tag> 线程 </tag>
            
            <tag> 集合 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>接口与内部类</title>
      <link href="/blog/9ab14111.html"/>
      <url>/blog/9ab14111.html</url>
      
        <content type="html"><![CDATA[<h3 id="接口"><strong>接口</strong></h3><p>重难点总结：<br>1、接口中的所有方法自动地属于public，在接口声明方法时，不必提供关键字public，但是在实现接口 编写具体的接口方法时，需要加上public修饰符。</p><p>2、在接口中可以定义常量(默认就是用public static final修饰)。接口中绝对不能含有实例域或静态方法，也不能在接口中实现方法。</p><a id="more"></a><p>3、接口看成是没有实例域的抽象方法。</p><p>4、Java程序设计语言是一种强类型语言，在调用方法的时候，编译器将会检查这个方法是否存在。</p><p>5、不能构造接口的对象，却能声明接口的变量，接口变量必须引用实现了接口的类对象。</p><p>6、接口之间通过继承关系扩展接口。</p><h3 id="克隆"><strong>克隆</strong></h3><p>对象的克隆是指创建一个新对象，且新对象的状态与原始对象的状态相同。当对克隆的新对象进行修改时，不会影响原始对象的状态。</p><h3 id="接口与回调"><strong>接口与回调</strong></h3><p>回调可以指出某个特定事件发生时应该采取的动作，需要实现接口中的方法。</p><h3 id="内部类"><strong>内部类</strong></h3><h4 id="内部类-v2"><strong>内部类</strong></h4><p>顾名思义在类中再定义一个类。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class A&#123;</span><br><span class="line">int i = 100;</span><br><span class="line">class innerClass&#123;</span><br><span class="line">int m = i;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>内部类方法可以访问该类定义所在的作用域中的数据，包括私有的数据</li><li>内部类可以对同一个包中的其他类隐藏起来</li><li>定义一个回调函数且不想编写大量代码时，使用匿名内部类比较便携</li></ul><p>值得注意的是，外围类的引用在构造器中设置。编译器修改了所有的内部类的构造器，添加一个外围类的引用参数。因此内部类的对象有一个隐式引用，它指向创建它的外部类对象。<br>所以内部类既可以访问自身的数据域，也可以访问创建它的外围类对象的数据域。</p><h4 id="局部内部类"><strong>局部内部类</strong></h4><p>一个类的方法体中定义的类。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class A&#123;</span><br><span class="line">int i = 100;</span><br><span class="line">public void innerClass()&#123;</span><br><span class="line">class B&#123;</span><br><span class="line">int m = i;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>局部类 不能用public或private访问说明符进行声明，他的作用域被限定在声明这个局部类的块中。<br>局部类有一个优势，即对外部世界可以完全隐藏起来。</p><h4 id="匿名内部类"><strong>匿名内部类</strong></h4><p>一个类在new 创建一个对象实例时，直接实现其中的接口方法</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ActionListener listener = new ActionListener()&#123;</span><br><span class="line">public void actionPerformed(ActionEvent event)&#123;...&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="静态内部类"><strong>静态内部类</strong></h4><p>使用内部类只是为了把一个类隐藏在另外一个类的内部，并不需要内部类引用外围对象。为此，可以将内部类声明为static，以便取消产生的引用。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class A&#123;</span><br><span class="line">static int i = 100;</span><br><span class="line">static class B&#123;</span><br><span class="line">int m = i;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>反射机制与代理机制</title>
      <link href="/blog/cfff76a5.html"/>
      <url>/blog/cfff76a5.html</url>
      
        <content type="html"><![CDATA[<h2 id="反射"><strong>反射</strong></h2><h3 id="获得对应的class对象-以及创建其实例">获得对应的Class对象，以及创建其实例</h3><p>三种获取对应Class对象的方法</p><a id="more"></a><p>①通过实例对象获取对应类的类对象</p><pre><code>e.getClass();</code></pre><p>②通过定义一个字符串类名，获取一个类对象</p><pre><code>String str = &quot;java.util.date&quot;;Class.forName(str);</code></pre><p>③通过类名获取类对象</p><pre><code>Employee.class</code></pre><p>通过获得的Class对象，调用其newInstance()方法创建其实例</p><pre><code>AClass.newInstance();</code></pre><h3 id="利用反射分析类的能力">利用反射分析类的能力</h3><p>分析一个类的能力从三方面入手，其域、方法和构造器。反射java.util.reflact包中有这三方面相关的获取方法，可以得知一个类实际的类能力。</p><ul><li>Field 域</li><li>Method 方法</li><li>Constructor 构造器</li></ul><h3 id="在运行时使用反射分析类">在运行时使用反射分析类</h3><p>在运行时，可以通过类属性获得某个指定的实例对象</p><h3 id="使用反射编写泛型数组代码">使用反射编写泛型数组代码</h3><h3 id="反射机制允许你调用任意方法">反射机制允许你调用任意方法</h3><p>允许调用包装在当前Method对象中的方法</p><p>比如：m1代表Employee类的getName方法<br>Method m1 = Employee.class.getMethod(“getName”,String.class);<br>String n = (String)m1.invoke(harry);</p><h3 id="总结">总结</h3><p>反射机制使得人们可以通过在运行时查看域和方法，让人们编写出更具体有通用性的程序。这种功能对于编写系统程序来说极其实用。</p><a></a>## **代理**利用代理可以在运行时创建一个实现了一组给定接口的心类，这种功能只有在编译时 无法确定需要实现哪个接口时，才有必要使用。无论何时调用代理对象的方法，调用处理器的invoke方法都会被调用。并向其传递Method对象和原始的调用参数。<p>代理的作用是：为其它对象提供一种代理以控制对这个对象的访问。在某些情况下，一 个客户不想直接引用另一个对象，而代理对象可以在客户端和目标对象之间起到中介作用。通过代理类这一中间层，能够有效控制对实际委托类对象的直接访问，也可以很好地隐藏和 保护实际对象，实施不同的控制策略，从而在设计上获得了更大的灵活性。</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 反射机制 </tag>
            
            <tag> 类对象 </tag>
            
            <tag> Class </tag>
            
            <tag> 代理 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>零碎注意点</title>
      <link href="/blog/3bf9b222.html"/>
      <url>/blog/3bf9b222.html</url>
      
        <content type="html"><![CDATA[<p>在DOS命令行下执行命令运行Java文件：<br>1、运行jar包    <code>java -jar test.jar</code><br>2、编译Java文件 <code>javac test.java</code><br>3、运行Java的字节码文件 <code>java test</code><br>4、调试Java文件 <code>jdb test.java</code><br>5、生成Java doc文件 <code>javadoc test.java</code></p><a id="more"></a><p>JPanel面板的默认布局—流布局管理器 Flow Layout Manager，面板中的元素是按照从左到右的顺序排列的。</p><p>JFrame框架的默认布局—边框布局管理器 Border Layout Manager，页面JFrame框架分上北下南 左西右东 中间 五个模块，组成了边框布局。</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> Java </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>异常与泛型</title>
      <link href="/blog/ea45fa53.html"/>
      <url>/blog/ea45fa53.html</url>
      
        <content type="html"><![CDATA[<h2 id="捕获异常"><strong>捕获异常</strong></h2><p>异常分为两种类型：未检查异常和已检查异常。</p><p><strong>已检查异常</strong>：系统已检查可能发生异常的地方，程序员编写再精细的程序也无法保证不发生异常的地方。(派生于IOException)<br><strong>未检查异常</strong>：派生于Error类或RuntimeException</p><a id="more"></a><p>对于已检查异常，编译器将会检查是否提供了处理器。(在即时编译器的编译阶段就已经检测出来，必须处理或抛出该有可能发生的已检查异常)。<br>然而，有很多常见的异常，例如，访问null引用，都属于未检查异常。编译器不会查看是否为这些错误提供了处理器。毕竟，应该精心地编写代码来避免这些错误的发生，而不是将精力放在编写异常处理器上。</p><p>|—throw在方法体中抛出异常<br>|—throws  在方法头上抛出异常<br>|—try-catch  捕获并处理异常</p><h2 id="泛型"><strong>泛型</strong></h2><p>泛型类可以看作不同类的工厂<br>一个泛型类就是具有一个或多个类型变量的类</p>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 异常 </tag>
            
            <tag> 泛型 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>类和对象的易错点</title>
      <link href="/blog/df938bb4.html"/>
      <url>/blog/df938bb4.html</url>
      
        <content type="html"><![CDATA[<ul><li><p>总结Java程序设计语言中方法参数的使用情况:</p><ul><li>一个方法不能修改一个基本数据类型的参数（即数值型和布尔型，在Java中只有基本类型不是对象）。当参数是传入一个数值或布尔值时，传入的参数和原数据基本类型参数是两份不同的拷贝，互不影响。</li></ul><a id="more"></a><ul><li>一个方法可以改变一个对象参数的状态（如传入一个数组引用改变某一个数组元素值，或传入一个对象，改变其属性值。原因在于：方法得到的是对象引用的拷贝，对象引用及其他的拷贝同时引用同一个对象。<strong>但是注意：</strong> 当传入一个字符串对象引用时，由于string是final类型的，所有方法体中的修改相当于新创建了一个字符串对象，而不会改变原字符串变量的值）</li><li>一个方法不能让对象参数引用一个新的对象，如示例代码：</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static void swap(Employee x, Employee y)&#123;</span><br><span class="line">Employee temp = x;</span><br><span class="line">x = y;</span><br><span class="line">y = temp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><pre><code>这个交换方法被swap(a,b)调用时，并不能将a对象和b对象交换，原因在于，方法结束时参数变量x和y被丢弃。原来的变量a和b仍然引用这个方法调用之前所引用的对象。</code></pre></li><li><p>总结调用构造器的具体步骤：<br>1）所有数据域被初始化为默认值（0、false或null）。<br>2）按照在类声明中出现的次序，依次执行所有域初始化语句和初始化快.(即按声明顺序执行静态块和静态语句)<br>3）如果构造器第一行调用了第二个构造器，则执行第二个构造器主体。<br>4）执行这个构造器的主体。</p></li></ul><ul><li><p>总结域与局部变量主要不同点：<br>必须明确地初始化方法中的局部变量。但是，如果没有初始化类中的域（即成员变量），将会被初始化为默认值（0、false或null）</p><p>初始化域变量时，需注意 如果在编写一个类时没有编写构造器 ，那么系统就会提供一个无参数构造器。这个构造器将会为所有的实例域设置为默认值。于是，实例域中的数值型数据设置为0、布尔型数据设置为false、所有对象变量将设置为null。<br>如果类中提供了至少一个构造器，但是没有提供无参数的构造器，则在构造对象时如果没有提供参数就会被视为不合法。因此，在构造一个类时，最好创建一个无参构造器。<br>仅当类没有提供任何构造器的时候，系统才会提供一个默认的构造器。</p></li><li><p>返回类型不是方法签名的一部分。也就是说，不能有两个名字相同、参数类型也相同却返回不同类型值的方法。</p></li><li><p>继承类代码实例说明：</p></li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class test &#123;</span><br><span class="line">public static void main(String[] args) &#123;</span><br><span class="line"></span><br><span class="line">// ****向上转型的注意点****</span><br><span class="line"></span><br><span class="line">// 向上直接转型，声明时的子类可以直接被更为抽象的父类引用</span><br><span class="line">// 声明为 什么类型，该对象就是什么类型，就算其引用的是子类的对象，该对象也只能调用声明的父类中拥有的方法体。</span><br><span class="line">// father fat = new son();</span><br><span class="line">// fat.fa();</span><br><span class="line">// fat.son(); //The method son() is undefined for the type father</span><br><span class="line"></span><br><span class="line">// ****强转的注意点****</span><br><span class="line"></span><br><span class="line">// 两个来自不同的类的对象，不存在向上转型的情况下，不可以互相强转。</span><br><span class="line">// 编译可以通过，但是会报运行时异常 java.lang.ClassCastException</span><br><span class="line">// father fat = new father();</span><br><span class="line">// son son = (son) fat; // father cannot be cast to son</span><br><span class="line"></span><br><span class="line">// 存在向上转型的情况下：</span><br><span class="line">// 原本是子类对象，后来向上转型之后，被父类引用后，再被强转为子类是可以的。可以正常运行！</span><br><span class="line">// father fat = new son();</span><br><span class="line">// son son = (son) fat;</span><br><span class="line"></span><br><span class="line">// 对于null对象强转，下面示例编译可以通过，但会报运行时空指针异常</span><br><span class="line">// ((son) null).so(); // java.lang.NullPointerException</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * 子类</span><br><span class="line"> */</span><br><span class="line">class son extends father &#123;</span><br><span class="line"></span><br><span class="line">public son() &#123;</span><br><span class="line">System.out.println(&quot;子类默认构造器！&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public son(int a) &#123;</span><br><span class="line">System.out.println(&quot;子类带参构造器！&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void test() &#123;</span><br><span class="line">System.out.println(&quot;子类test方法！&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public void so() &#123;</span><br><span class="line">System.out.println(&quot;子类独有的so方法！&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * 父类</span><br><span class="line"> */</span><br><span class="line">class father &#123;</span><br><span class="line">father() &#123;</span><br><span class="line">System.out.println(&quot;父类默认构造器！&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">father(int i) &#123;</span><br><span class="line">System.out.println(&quot;父类带参构造器！&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void test() &#123;</span><br><span class="line">System.out.println(&quot;父类test方法！&quot;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void fa() &#123;</span><br><span class="line">System.out.println(&quot;父类独有的fa方法！&quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><p>类型转换总结：</p><ul><li><strong>只能在继承层次内进行类型转换。</strong></li><li>在将超类转换成子类之前，应该使用<code>instance of</code>进行检查。</li><li>继承层次内的转换如：</li></ul><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Object obj = new Employee(&quot;Harry Hacker&quot;,350000);</span><br><span class="line">Employee e = (Employee) obj;</span><br></pre></td></tr></table></figure></li><li><p>抽象类总结:</p><ul><li>为了提高程序的清晰度，包含一个或多个抽象方法的类本身必须被声明为抽象的。</li><li>除了抽象方法外，抽象类还可以包含具体数据和具体方法。</li><li>类即使不含抽象方法，也可以将类声明为抽象类。</li><li><strong>抽象类不能被实例化</strong></li><li>需要注意，可以定义一个抽象类的对象变量，但是它只能引用非抽象子类的对象。<em>(就如同可以声明一个父类 引用子类对象一样------向上转型)</em></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Java笔试易错点</title>
      <link href="/blog/337c6120.html"/>
      <url>/blog/337c6120.html</url>
      
        <content type="html"><![CDATA[<ul><li>启动线程方法start();线程stop方法已经被弃用；守护线程在非守护线程结束后，会自动结束；等待其他线程通知方法是wait()</li><li>switch结构中没有break的话，匹配完不会跳出，会继续匹配下一个case直到整个结构结束</li><li>Java.awt.*只能导入awt这个包中的所有类，awt中的包中的类不会导入</li><li>public&gt;protected&gt;默认(包访问权限)&gt;private，因为protected除了可以被同一包访问，还可以被包外的子类所访问</li></ul><a id="more"></a><ul><li>在Java中继承是通过extends关键字来描述的，而且只允许继承自一个直接父类。</li><li>向上转型(子类对象被父类引用) 直接指向；向下转型(父类对象被子类引用)，需要强转。</li><li>公式-n=<sub>n+1可推出</sub>n=-n-1，所以~10=-11再加5结果为-6</li><li>getAttribute()接受从request域中传过来的参数，getParameter()接受从页面传过来的参数</li><li>只要记住，不论怎样，必定先执行静态代码，子由父生，所以父类必先执行</li><li>一个类若有抽象方法，其本身也必须声明为抽象类；接口中的方法默认就是public abstract</li><li>abstract不能与final并列修饰同一个类</li><li>接口中的变量声明为public static final默认形式</li><li>java7，字符串常量池从方法区移到堆中。java8 整个常量池从方法区中移除。方法区使用元空间（MetaSpace）实现</li><li>jvm堆分为：新生代（一般是一个Eden区，两个Survivor区），老年代（old区）。常量池属于 PermGen（方法区）</li><li>事实上只有在我们没有显示声明任何构造方法时java才会为我们提供一个默认的无参构造函数。</li><li>super()必须是子类的第一条语句</li><li>java是解释型语言，在运行时才进行翻译指令  Java不同于一般的编译语言和直译语言。它首先将源代码编译成字节码，然后依赖各种不同平台上的虚拟机来解释执行字节码，<br>从而实现了“一次编写，到处运行”的跨平台特性， 所以说java是一种解释型的语言</li><li>三个基本元素：封装，继承，多态。</li><li>五个基本原则：单一职责原则，开放封闭原则，里氏替换原则，依赖倒置原则，接口隔离原则</li><li>switch语句后的控制表达式只能是short、char、int、long整数类型和枚举类型，不能是float，double和boolean类型。String类型是java7开始支持。</li><li>MVC概念：<br>A. 模型通常代表应用程序中的数据以及用于操纵数据的业务逻辑；<br>B. 视图是其对应的模型的可视化呈现，视图 将模型渲染成适合于交互的形式（通常为用户界面元素）；<br>C. 控制器是用户与系统之间的纽带，它接受用户输入，并指示模型和视图基于用户输入执行操作（处理数据、展示数据）；</li><li>类方法是指类中被static修饰的方法，无this指针。类方法是可以调用其他类的static方法的。可以在类方法中生成实例对象再调用实例方法</li><li>ANSI码 对于windows系统来说的话就是指当前的系统编码</li></ul>]]></content>
      
      
      <categories>
          
          <category> 语言 </category>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> java </tag>
            
            <tag> 笔试 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
